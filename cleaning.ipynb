{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO hierarchial:\n",
    "- Try dimensionality reduction before clustering\n",
    "- Analyze sub clusters and look at their performance \n",
    "- Change metric when calculating clusters (currently ward) \n",
    "- Check with professor that euclidean makes sense because tf-idf is normalized   \n",
    "- Scale up to multiple search terms. Figure out metrics to evaluate between. (silhouette etc.)\n",
    "- Visualization with tsne or umap rather than mds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from nltk) (1.11.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from sklearn) (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from scikit-learn->sklearn) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from scikit-learn->sklearn) (1.3.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from scikit-learn->sklearn) (0.14.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from gensim) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from gensim) (1.8.4)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.19.1)\n",
      "Requirement already satisfied: boto3 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.10.5)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.7)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2018.8.13)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.5 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (1.13.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from botocore<1.14.0,>=1.13.5->boto3->smart-open>=1.8.1->gensim) (2.7.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from botocore<1.14.0,>=1.13.5->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (2.7.3)\n",
      "Requirement already satisfied: six in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (39.0.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (2.4)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from networkx) (4.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install sklearn\n",
    "!pip install gensim\n",
    "!pip install matplotlib\n",
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gimli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "import sklearn\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, ward, fcluster\n",
    "import networkx as nx\n",
    "import collections\n",
    "import math\n",
    "import operator\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fileids from the reuters corpus\n",
    "fileids = reuters.fileids()\n",
    "\n",
    "# Initialize empty lists to store categories and raw text\n",
    "categories = []\n",
    "text = []\n",
    "\n",
    "# Loop through each file id and collect each files categories and raw text\n",
    "for file in fileids:\n",
    "    categories.append(reuters.categories(file))\n",
    "    text.append(reuters.raw(file))\n",
    "\n",
    "# Combine lists into pandas dataframe. reutersDf is the final dataframe. \n",
    "og = pd.DataFrame({'ids':fileids, 'categories':categories, 'text':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = og.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing text\n",
    "df.text = df.text.str.replace('\\n', ' ')\n",
    "df.text = df.text.str.replace('&lt;', '<')\n",
    "df.text = df.text.str.replace(\"&amp;\", \"&\")\n",
    "\n",
    "# down case all\n",
    "df.text = df.text.str.lower()\n",
    "\n",
    "# remove symbols\n",
    "df.text = df.text.str.replace('<', ' ')\n",
    "df.text = df.text.str.replace('>', ' ')\n",
    "df.text = df.text.str.replace('-', ' ')\n",
    "\n",
    "# delete content specific \"stop words\"\n",
    "delete_words = ['qtr', 'pct', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'bil', 'mln',\n",
    "               'quarter', 'percent', 'million', 'billion', 'january', 'february', 'march', 'april', 'may', 'june', 'july', \n",
    "                'august', 'september', 'october', 'november', 'december', 'decembers', 'janurary', 'said', 'year', 'month',\n",
    "               'shr', 'cts']\n",
    "for w in delete_words:\n",
    "    df.text = df.text.str.replace(' ' + w + ' ', ' ')\n",
    "    df.text = df.text.str.replace(' ' + w + '.', '.')\n",
    "    \n",
    "# remove punctuation\n",
    "df.text = df.text.apply(lambda row: row.translate(str.maketrans('','',string.punctuation)))\n",
    "\n",
    "# collapse words to acronyms so recognized as one concept/token (and currently they are mixed)\n",
    "df.text = df.text.str.replace('united states', 'us')\n",
    "df.text = df.text.str.replace('new zealand', 'nz')\n",
    "df.text = df.text.str.replace('hong kong', 'hk')\n",
    "df.text = df.text.str.replace('united kingdom', 'uk')\n",
    "df.text = df.text.str.replace('dlrs', 'dollars')\n",
    "\n",
    "# remove all numbers\n",
    "    # originally removing number words\n",
    "df.text = df.text.apply(lambda row: re.sub('\\d*', '', row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retail = df[df.categories.map(set(['ship']).issubset)] # subset to start with \n",
    "df_retail = df_retail.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### done by tfidfvectorizer\n",
    "# tokenize words\n",
    "#df['tokens'] = df.text.apply(lambda row: nltk.word_tokenize(row))\n",
    "# remove stopwords\n",
    "#df.tokens = df.tokens.apply(lambda row: [w for w in row if not w in stopwords.words('english')])\n",
    "\n",
    "# stemming? Maybe for clutering, but not for finding common words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf. stop word removal. word tokenizer. \n",
    "tfidf = TfidfVectorizer(stop_words = 'english', analyzer = 'word')\n",
    "m = tfidf.fit_transform(df_retail['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effect 0.08203928799307676\n",
      "long 0.0640065924205518\n",
      "say 0.06253499501881706\n",
      "altogether 0.10969198057978341\n",
      "stopped 0.08665974212844965\n",
      "affected 0.07527861176516835\n",
      "movements 0.14783225076419668\n",
      "container 0.07527861176516835\n",
      "walk 0.10223744265960574\n",
      "shift 0.0969483638334321\n",
      "start 0.07834992661801123\n",
      "time 0.05706998037461946\n",
      "short 0.08665974212844965\n",
      "work 0.05706998037461946\n",
      "turn 0.0969483638334321\n",
      "taking 0.07834992661801123\n",
      "appear 0.10223744265960574\n",
      "unions 0.07264768090577554\n",
      "various 0.08010221882595321\n",
      "laws 0.0969483638334321\n",
      "compensation 0.08420474708708078\n",
      "workers 0.05871751359437815\n",
      "states 0.06325607381847434\n",
      "changes 0.08949382591325443\n",
      "protest 0.0673586020796019\n",
      "council 0.08665974212844965\n",
      "labour 0.061841133326547774\n",
      "trades 0.09284583557230454\n",
      "called 0.06478905742976887\n",
      "action 0.11111604076435447\n",
      "industrial 0.07264768090577554\n",
      "kembla 0.10223744265960574\n",
      "newcastle 0.0969483638334321\n",
      "sydney 0.08949382591325443\n",
      "handling 0.1535004183338062\n",
      "cargo 0.10330009163359853\n",
      "began 0.061172508635747025\n",
      "disruption 0.18569167114460908\n",
      "today 0.09081631041026185\n",
      "commission 0.07264768090577554\n",
      "arbitration 0.09284583557230454\n",
      "hearing 0.09284583557230454\n",
      "went 0.08420474708708078\n",
      "vessels 0.048103482461999574\n",
      "nearly 0.08203928799307676\n",
      "port 0.13219673084456732\n",
      "movement 0.07834992661801123\n",
      "prevented 0.0969483638334321\n",
      "claim 0.08665974212844965\n",
      "pay 0.11310410900074826\n",
      "ago 0.07264768090577554\n",
      "week 0.10102491414424604\n",
      "imposed 0.08949382591325443\n",
      "sources 0.10657071785595432\n",
      "shipping 0.10855335066754508\n",
      "dispute 0.1250699900376341\n",
      "arate 0.0969483638334321\n",
      "disrupted 0.1733194842568993\n",
      "containers 0.08665974212844965\n",
      "carrying 0.07146113034072947\n",
      "ships 0.038711875374698355\n",
      "flag 0.05930121826037665\n",
      "lifted 0.08949382591325443\n",
      "yesterday 0.0528626931253086\n",
      "australia 0.08949382591325443\n",
      "western 0.06929567124672545\n",
      "victoria 0.09284583557230454\n",
      "wales 0.08949382591325443\n",
      "south 0.05328535892797716\n",
      "new 0.04459090276323988\n",
      "crews 0.08665974212844965\n",
      "tug 0.08665974212844965\n",
      "hit 0.05871751359437815\n",
      "ports 0.2708050394031783\n",
      "nsw 0.4642291778615227\n",
      "ends 0.08665974212844965\n",
      "ban 0.2785375067169136\n",
      "ship 0.045408155205130925\n",
      "foreign 0.09819503316039292\n",
      "australian 0.08665974212844965\n"
     ]
    }
   ],
   "source": [
    "# get words and scores in document 0\n",
    "feature_names = tfidf.get_feature_names() # words \n",
    "feature_index = m[0,:].nonzero()[1] # non zero words for first document\n",
    "tfidf_scores = zip(feature_index, [m[0, x] for x in feature_index]) # scores and words\n",
    "\n",
    "for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
    "    print(w, s)\n",
    "    \n",
    "#tfidf.vocabulary_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dist = 1 - cosine_similarity(m)\n",
    "# the diagonals are numbers very close to, but not quite zero (very small negative number)\n",
    "# problematic for some functions that don't expect negative numbers\n",
    "for i in range(len(dist)):\n",
    "    dist[i][i] = 0\n",
    "    \n",
    "    \n",
    "dist_e = euclidean_distances(m)  ## I think its ok to use euclidean because tf-idf normalizes\n",
    "# euclidean can be innaccurate if documents are different lengths such that vectors are different lengths \n",
    "# I would prefer to use euclidean because then more sensicl to calculate centroids\n",
    "# ask professor?? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dimensionality Reduction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 0.95) # keep 95% of variance \n",
    "pcam = pca.fit_transform(m.toarray())\n",
    "\n",
    "dist_e_pca = euclidean_distances(pcam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchial Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:4: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:7: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# hieararchial clusters with children \n",
    "linkage_matrix = linkage(dist, method = 'ward') # try different methods\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n",
    "linkage_matrix_e = linkage(dist_e, method = 'ward') # try different methods\n",
    "\n",
    "# dimensionality reduction\n",
    "linkage_matrix_e_pca = linkage(dist_e_pca, method = 'ward') \n",
    "\n",
    "\n",
    "# plot dendogram\n",
    "#fig, ax = plt.subplots(figsize=(15, 20))\n",
    "#ax = dendrogram(linkage_matrix, orientation=\"right\", labels = df_retail.ids.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#score = dict()\n",
    "#for k in range(2 , min(math.floor(len(df_retail) / 3), 10)): # want min(minimum avg 3 documents per cluster OR 10% docs)\n",
    "#    f = fcluster(linkage_matrix, k, criterion = 'maxclust')\n",
    "#    score[k] = sklearn.metrics.silhouette_score(m, f, metric = 'cosine')\n",
    "    \n",
    "# cluster number with max score\n",
    "#k = np.argmax(score) + 2\n",
    "#k = max(score.items(), key=operator.itemgetter(1))[0] + 2\n",
    "#f = fcluster(linkage_matrix, k, criterion = 'maxclust')\n",
    "\n",
    "## this seemingly doesn't calculate silhouette score right...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/21638130/tutorial-for-scipy-cluster-hierarchy\n",
    "# idea for scree plot to determine number of clusters in fcluster using 'maxclust' in criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_merge(df, f):\n",
    "    # merge in with original data via pandas\n",
    "    frameh = pd.DataFrame(df.index, index = [f], columns = ['index_retail'])\n",
    "    frameh = pd.merge(frameh, df, right_index = True, left_on = 'index_retail')\n",
    "    frameh['cluster'] = frameh.index.str[0]\n",
    "    frameh = frameh.reset_index()\n",
    "    return frameh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_label(frameh, m):\n",
    "    # most common words in clusters (based on tf-idf not just frequency)\n",
    "    centroid = dict()\n",
    "    labels = []\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        print()\n",
    "        # most common words\n",
    "        cluster1 = list(frameh[frameh.cluster == c].index.unique())\n",
    "        # find documents cluster\n",
    "        m1 = m[cluster1,:]\n",
    "        # take mean vector among all documents\n",
    "        m1 = m1.mean(axis = 0)\n",
    "        # record mean vector: centroids of each sub cluster\n",
    "        centroid[c] = m1\n",
    "\n",
    "        # max values in mean vector \n",
    "        lst = []\n",
    "\n",
    "        for i in np.argsort(np.asarray(m1)[0])[::-1][:6]:\n",
    "            lst.append(feature_names[i])\n",
    "\n",
    "        labels.append(lst)\n",
    "        \n",
    "    return labels, centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_individ(frameh):\n",
    "    sil_a = dict()\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        sil_a[c] = dict()\n",
    "        docs_i = list(frameh[frameh.cluster == c].index.unique())\n",
    "        for i in docs_i:\n",
    "            lst = []\n",
    "            for j in docs_i: \n",
    "                if i != j:\n",
    "                    lst.append(np.linalg.norm(m[i].toarray()-m[j].toarray()))\n",
    "            sil_a[c][i] = np.mean(lst)\n",
    "\n",
    "    sil_b = dict()\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        sil_b[c] = dict()\n",
    "        docs_in = list(frameh[frameh.cluster == c].index.unique())\n",
    "        docs_out = list(frameh[frameh.cluster != c].index.unique())\n",
    "        for i in docs_in:\n",
    "            lst = []\n",
    "            for j in docs_out: \n",
    "                lst.append(np.linalg.norm(m[i].toarray()-m[j].toarray()))\n",
    "            sil_b[c][i] = np.mean(lst)\n",
    "            \n",
    "    return sil_a, sil_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_take_avg(sil):\n",
    "    avg = []\n",
    "    for v in sil.values():\n",
    "        avg.append(list(v.values()))\n",
    "    avg = [item for sublist in avg for item in sublist]\n",
    "    avg = [0 if math.isnan(i) else i for i in avg]\n",
    "    avg = np.mean(avg)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_avg(frameh):\n",
    "    sil_a, sil_b = silhouette_individ(frameh)\n",
    "    avga = silhouette_take_avg(sil_a)\n",
    "    avgb = silhouette_take_avg(sil_b)\n",
    "\n",
    "    return (avgb - avga) / max(avgb, avga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distortion is only within cluster but is good because can measure rate of change  \n",
    "silhouette captures within and without cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# distortion - sum of squared errors between points and its centroid \n",
    "# barely varies with different cluster numbers\n",
    "distortion = dict()\n",
    "silhouette = dict()\n",
    "for k in range(2 , min(math.floor(len(df_retail) / 3), 10)): # want min(minimum avg 3 documents per cluster OR 10% docs)\n",
    "    f = fcluster(linkage_matrix_e_pca, k, criterion = 'maxclust') ## using euclidean distance \n",
    "    frameh = frame_merge(df_retail, f)\n",
    "    labels, centroid = centroid_label(frameh, pcam)\n",
    "    \n",
    "    # calculate silhouette \n",
    "    silhouette[k] = silhouette_avg(frameh)\n",
    "\n",
    "    # calculate distortion\n",
    "    sumd = 0\n",
    "    for i in list(frameh.index.unique()):\n",
    "        c = int(frameh[frameh.index == i].cluster)\n",
    "        sumd += np.linalg.norm(pcam[i]-centroid[c])\n",
    "        \n",
    "    # take average \n",
    "    distortion[k] = sumd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: -0.26041233296375427,\n",
       " 3: -0.11544948297539724,\n",
       " 4: 0.03983722650916594,\n",
       " 5: 0.052067828833999484,\n",
       " 6: 0.061802507554136625,\n",
       " 7: 0.07400898489086133,\n",
       " 8: 0.08135952983322932,\n",
       " 9: 0.08753833734427226}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette # mehhhh just increases as number of clusters increases\n",
    "# maybe use for evaluation between topics rather than for selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative rate of change \n",
    "roc = []\n",
    "for k,v in distortion.items(): \n",
    "    if k+1 in distortion:\n",
    "        roc.append(abs(distortion[k+1] - distortion[k]) / distortion[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.007566058815660023\n"
     ]
    }
   ],
   "source": [
    "# find k \n",
    "for i in range(len(roc)):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    print(i)\n",
    "    print(abs(roc[i] - roc[i-1]))\n",
    "    if abs(roc[i] - roc[i-1]) >  .005: # threshold in change of roc -- this threshold might be too tight? \n",
    "                                        # tried roc threshold, but magnitude changes for different topics \n",
    "        k = i+1 # number of clusters\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot distortion\n",
    "distortion = sorted(distortion.items()) # sorted by key, return a list of tuples\n",
    "x, y = zip(*distortion) # unpack a list of pairs into two tuples\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use this number of clusters to form final clusters\n",
    "f = fcluster(linkage_matrix, k, criterion = 'maxclust')\n",
    "frameh = frame_merge(df_retail, f)\n",
    "labels, centroid = centroid_label(frameh, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find Documents at Each Level of Hierarchy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in linkage matrix, indicate the aggregated node for each node pair\n",
    "links = pd.DataFrame(linkage_matrix_e) # using euclidean \n",
    "links.columns = ['source1', 'source2', 'd', 'n']\n",
    "\n",
    "links['target'] = 0\n",
    "n = 24 \n",
    "for i, row in links.iterrows():\n",
    "    n += 1\n",
    "    links.at[i,'target'] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten irregular nested lists\n",
    "def flatten(l):\n",
    "    for el in l:\n",
    "        if isinstance(el, collections.Iterable) and not isinstance(el, (str, bytes)):\n",
    "            yield from flatten(el)\n",
    "        else:\n",
    "            yield el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_docs(merge, source):\n",
    "    merge = pd.merge(merge, merge[['target', 'docs']], left_on = source, right_on = 'target',  how = 'left')\n",
    "    merge.docs_x = np.where(merge.docs_x.isnull(), '', merge.docs_x)\n",
    "    merge.docs_y = np.where(merge.docs_y.isnull(), '', merge.docs_y)\n",
    "    merge['docs'] = merge[['docs_x', 'docs_y']].values.tolist()\n",
    "    merge = merge.drop(columns = ['docs_x', 'docs_y'])\n",
    "    #merge.docs = merge.docs.apply(np.ravel)\n",
    "    merge = merge.rename(columns = {'target_x':'target'})\n",
    "    merge.docs = list(merge.docs.apply(lambda row: flatten(row)))\n",
    "    merge.docs = merge.docs.apply(lambda row: [i for i in row if i != ''])\n",
    "\n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-29-36f3cc872e88>\", line 19, in <module>\n",
      "    merge = merge_docs(merge, 'source2')\n",
      "  File \"<ipython-input-28-39ced3f4dd0d>\", line 10, in merge_docs\n",
      "    merge.docs = merge.docs.apply(lambda row: [i for i in row if i != ''])\n",
      "  File \"c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\core\\series.py\", line 3194, in apply\n",
      "    mapped = lib.map_infer(values, f, convert=convert_dtype)\n",
      "  File \"pandas/_libs/src\\inference.pyx\", line 1472, in pandas._libs.lib.map_infer\n",
      "  File \"<ipython-input-28-39ced3f4dd0d>\", line 10, in <lambda>\n",
      "    merge.docs = merge.docs.apply(lambda row: [i for i in row if i != ''])\n",
      "  File \"<ipython-input-28-39ced3f4dd0d>\", line 10, in <listcomp>\n",
      "    merge.docs = merge.docs.apply(lambda row: [i for i in row if i != ''])\n",
      "  File \"<ipython-input-27-080fc6ac86a6>\", line 5, in flatten\n",
      "    yield from flatten(el)\n",
      "  File \"<ipython-input-27-080fc6ac86a6>\", line 4, in flatten\n",
      "    if isinstance(el, collections.Iterable) and not isinstance(el, (str, bytes)):\n",
      "  File \"c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\abc.py\", line 139, in __instancecheck__\n",
      "    return _abc_instancecheck(cls, instance)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\inspect.py\", line 1495, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\inspect.py\", line 1453, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"c:\\users\\gimli\\appdata\\local\\programs\\python\\python37-32\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# initial merge between frame ids and source1/source2\n",
    "merge = pd.merge(links, frameh[['ids']], left_on = 'source1', right_index = True, how = 'left')\n",
    "merge = pd.merge(merge, frameh[['ids']], left_on = 'source2', right_index = True, how = 'left')\n",
    "# create single docs list column \n",
    "merge = merge.rename(columns = {'ids_x':'docs1', 'ids_y':'docs2'})\n",
    "merge.docs2 = np.where(merge.docs2.isnull(), '', merge.docs2)\n",
    "merge.docs1 = np.where(merge.docs1.isnull(), '', merge.docs1)\n",
    "merge['docs']= merge[['docs1', 'docs2']].values.tolist()\n",
    "merge = merge.drop(columns = ['docs1', 'docs2'])\n",
    "# flattern docs list column\n",
    "merge.docs = merge.docs.apply(lambda row: [i for i in row if i != ''])\n",
    "merge['len'] = merge.docs.apply(lambda row: len(set(row)))\n",
    "\n",
    "merge1 = merge.copy()\n",
    "\n",
    "# loop until have one id per document at node (n)\n",
    "while int(merge[merge.target == merge.target.max()].len) != int(merge[merge.target == merge.target.max()].n): \n",
    "    merge = merge_docs(merge, 'source1')\n",
    "    merge = merge_docs(merge, 'source2')\n",
    "    merge['len'] = merge.docs.apply(lambda row: len(set(row)))\n",
    "    \n",
    "merge.docs = merge.docs.apply(lambda row: set(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph reflecting hierarchy\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(links.source1)\n",
    "G.add_nodes_from(links.source2)\n",
    "G.add_nodes_from(links.target)\n",
    "\n",
    "subset = links[['source1', 'target']]\n",
    "G.add_edges_from([tuple(x) for x in subset.values])\n",
    "subset = links[['source2', 'target']]\n",
    "G.add_edges_from([tuple(x) for x in subset.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add docs to each target node so know which docs exist at each target \n",
    "for i in list(merge.target.unique()):\n",
    "    G.nodes[i]['docs'] = merge[merge.target == i].docs.values[0]\n",
    "    \n",
    "# add docs to origianl nodes as well \n",
    "ogdocs = pd.merge(links, frameh[['ids']], left_on = 'source1', right_index = True, how = 'left')\n",
    "ogdocs = pd.merge(ogdocs, frameh[['ids']], left_on = 'source2', right_index = True, how = 'left')\n",
    "for i in range(25):\n",
    "    if len(ogdocs[ogdocs.source1 == i].ids_x) != 0:\n",
    "        G.nodes[i]['docs'] = ogdocs[ogdocs.source1 == i].ids_x.values[0]\n",
    "    if len(ogdocs[ogdocs.source2 == i].ids_y) != 0:\n",
    "        G.nodes[i]['docs'] = ogdocs[ogdocs.source2 == i].ids_y.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# labels and centroids for every sub cluster \n",
    "labelsh = []\n",
    "centroidh = []\n",
    "for i in list(range(links.target.max())):\n",
    "    # most common words\n",
    "    if i < 25:\n",
    "        cluster1 = list(frameh[frameh.ids == list(G.nodes[i].values())[0]].index.unique())\n",
    "    else:\n",
    "        cluster1 = list(frameh[frameh.ids.isin(list(list(G.nodes[i].values())[0]))].index.unique())\n",
    "    # find documents cluster\n",
    "    m1 = m[cluster1,:]\n",
    "    # take mean vector among all documents\n",
    "    m1 = m1.mean(axis = 0)\n",
    "    # record mean vector: centroids of each sub cluster\n",
    "    centroidh.append(m1)\n",
    "    \n",
    "    # max values in mean vector: labels\n",
    "    lst = []\n",
    "    for i in np.argsort(np.asarray(m1)[0])[::-1][:3]:\n",
    "        lst.append(feature_names[i])\n",
    "        \n",
    "    labelsh.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find top level clusters and mark as such - determined by fcluster above \n",
    "nx.set_node_attributes(G, 0, 'cluster')\n",
    "for c in list(frameh.cluster.unique()):\n",
    "    try:\n",
    "        c_ids = set(frameh[frameh.cluster == c].ids.unique())\n",
    "        node = [x for x,y in G.nodes(data=True) if y['docs']==c_ids][0]\n",
    "    except:\n",
    "        c_ids = frameh[frameh.cluster == c].ids.unique()\n",
    "        node = [x for x,y in G.nodes(data=True) if y['docs']==c_ids][0]\n",
    "    G.nodes[node]['cluster'] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels for top nodes\n",
    "topnodes = []\n",
    "for c in list(frameh.cluster.unique()):\n",
    "    topnodes.append( [x for x,y in G.nodes(data=True) if y['cluster']==c][0])\n",
    "[labelsh[int(i)] for i in topnodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroids for the top leve clusters\n",
    "[centroidh[int(i)] for i in topnodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization based on distances: x and y coordinates  \n",
    "\n",
    "###### should use tsne or umap in reality \n",
    "\n",
    "MDS()\n",
    "\n",
    "# convert two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "pos = mds.fit_transform(dist_e_pca)  # shape (n_components, n_samples) \n",
    "# trying pca dimensionality reduced \n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_vis = pd.DataFrame(dict(x = xs, y = ys, cluster = f))\n",
    "df_vis.cluster = df_vis.cluster - 1 # want clusters to start at 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(17, 9)) \n",
    "\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3'}\n",
    "\n",
    "groups = df_vis.groupby('cluster')\n",
    "\n",
    "for name, group in groups:\n",
    "    print(name)\n",
    "    ax.scatter(group.x, group.y, c  = cluster_colors[name], label = labels[name])\n",
    "    ax.legend()\n",
    "    \n",
    "### everything is far apart! curse of dimensionality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans \n",
    "# need to research and mess with paramters \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 3\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(m)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge in with original data via pandas\n",
    "frame = pd.DataFrame(df_retail.index, index = [clusters], columns = ['index_retail'])\n",
    "frame = pd.merge(frame, df_retail, right_index = True, left_on = 'index_retail')\n",
    "frame['cluster'] = frame.index.str[0]\n",
    "frame = frame.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "order_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frame.cluster.value_counts() # number of documents per cluster \n",
    "\n",
    "## http://brandonrose.org/clustering : more info on finding most common words in cluster, visualizing, hierarchial \n",
    "# also try taking average of tf-idf per cluster and using that way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# most common words in clusters \n",
    "\n",
    "labels = []\n",
    "for i in range(3):\n",
    "    # most common words\n",
    "    cluster1 = list(frame[frame.cluster == i].index.unique())\n",
    "    # find documents cluster\n",
    "    m1 = m[cluster1,:]\n",
    "    # take mean vector among all documents\n",
    "    m1 = m1.mean(axis = 0)\n",
    "    # max values in mean vector \n",
    "    lst = []\n",
    "\n",
    "    for i in np.argsort(np.asarray(m1)[0])[::-1][:3]:\n",
    "        lst.append(feature_names[i])\n",
    "        \n",
    "    labels.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization based on distances: x and y coordinates  \n",
    "\n",
    "###### should use tsne or umap in reality \n",
    "\n",
    "MDS()\n",
    "\n",
    "# convert two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_vis = pd.DataFrame(dict(x = xs, y = ys, cluster = f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = dict()\n",
    "for i in range(len(labels)):\n",
    "    cluster_labels[i] = labels[i]\n",
    "    \n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(17, 9)) \n",
    "\n",
    "groups = df_vis.groupby('cluster')\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.scatter(group.x, group.y, c  = cluster_colors[name], label = cluster_labels[name])\n",
    "    ax.legend()\n",
    "    \n",
    "### everything is far apart!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svd\n",
    "# try following lingo algorithm as per paper\n",
    "u,s,v = np.linalg.svd(m.todense().T, full_matrices = False) # full_matrices make dimensions work. Min(M,N) (why necessary??)\n",
    "s = np.diag(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.matmul(s,v.T)\n",
    "np.matmul(u, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique list of topics (categories)\n",
    "topics = list(df.categories)\n",
    "topics = [item for sublist in topics for item in sublist]\n",
    "topics = set(topics)\n",
    "\n",
    "\n",
    "df[df.categories.map(set(['earn']).issubset)]\n",
    "\n",
    "df['len'] = df.text.apply(lambda row: len(row))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
