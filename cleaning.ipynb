{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing NLTK; Create TF-IDF Vector Space Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#!pip install sklearn\n",
    "#!pip install gensim\n",
    "#!pip install matplotlib\n",
    "#!pip install networkx\n",
    "#!pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gimli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fileids from the reuters corpus\n",
    "fileids = reuters.fileids()\n",
    "\n",
    "# Initialize empty lists to store categories and raw text\n",
    "categories = []\n",
    "text = []\n",
    "\n",
    "# Loop through each file id and collect each files categories and raw text\n",
    "for file in fileids:\n",
    "    categories.append(reuters.categories(file))\n",
    "    text.append(reuters.raw(file))\n",
    "\n",
    "# Combine lists into pandas dataframe. reutersDf is the final dataframe. \n",
    "og = pd.DataFrame({'ids':fileids, 'categories':categories, 'text':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = og.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing text\n",
    "df.text = df.text.str.replace('\\n', ' ')\n",
    "df.text = df.text.str.replace('&lt;', '<')\n",
    "df.text = df.text.str.replace(\"&amp;\", \"&\")\n",
    "\n",
    "# down case all\n",
    "df.text = df.text.str.lower()\n",
    "\n",
    "# remove symbols\n",
    "df.text = df.text.str.replace('<', ' ')\n",
    "df.text = df.text.str.replace('>', ' ')\n",
    "df.text = df.text.str.replace('-', ' ')\n",
    "\n",
    "# delete content specific \"stop words\"\n",
    "delete_words = ['qtr', 'pct', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'bil', 'mln',\n",
    "               'quarter', 'percent', 'million', 'billion', 'january', 'february', 'march', 'april', 'may', 'june', 'july', \n",
    "                'august', 'september', 'october', 'november', 'december', 'janurary', 'said', 'year', 'month',\n",
    "               'shr', 'cts', 'january\\'s', 'february\\'s', 'march\\'s', 'april\\'s', 'may\\'s', 'june\\'s', 'july\\'s', \n",
    "                'august\\'s', 'september\\'s', 'october\\'s', 'november\\'s', 'december\\'s',\n",
    "               'feet', 'ounces', 'ounce', 'foot', 'ton', 'tons', 'tonnes']\n",
    "for w in delete_words:\n",
    "    df.text = df.text.str.replace(' ' + w + ' ', ' ')\n",
    "    df.text = df.text.str.replace(' ' + w + '\\\\.', '.')\n",
    "\n",
    "# remove punctuation\n",
    "df.text = df.text.apply(lambda row: row.translate(str.maketrans('','',string.punctuation)))\n",
    "\n",
    "# collapse words to acronyms so recognized as one concept/token (and currently they are mixed)\n",
    "df.text = df.text.str.replace('united states', 'us')\n",
    "df.text = df.text.str.replace('new zealand', 'nz')\n",
    "df.text = df.text.str.replace('hong kong', 'hk')\n",
    "df.text = df.text.str.replace('united kingdom', 'uk')\n",
    "df.text = df.text.str.replace('dlrs', 'dollars')\n",
    "\n",
    "# remove all numbers\n",
    "    # originally removing number words\n",
    "df.text = df.text.apply(lambda row: re.sub('\\d*', '', row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('reuters_processed') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
