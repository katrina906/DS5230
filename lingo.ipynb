{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lingo Algorithm\n",
    "- Create TF-IDF Matrix\n",
    "- SVD TF-IDF: choose k via elbow in ROC of retained variance \n",
    "- Find cluster labels via strongest relationships between words and concepts in reduced V matrix \n",
    "- Assign documents to clusters based on document-label relationship strength\n",
    "- Combine clusters with overlapping labels\n",
    "- Unstem label\n",
    "- Sort clusters\n",
    "- Calculate statistics for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gimli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, ward, fcluster\n",
    "import networkx as nx\n",
    "import collections\n",
    "import math\n",
    "import operator\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from kneed import KneeLocator\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf Matrix\n",
    "- Stemming \n",
    "- Tokenize\n",
    "- Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_tokenizer(str_input):\n",
    "    \n",
    "    # initialize Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # tokenize input words and stem\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(df):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # stem stop words before removing\n",
    "    stop_stem = [stemming_tokenizer(t) for t in stopwords.words('english')]\n",
    "    stop_stem = [item for sublist in stop_stem for item in sublist]\n",
    "    \n",
    "    # tfidf: stop word removal, tokenize and stem\n",
    "    # set max number of features else memory issues\n",
    "    tfidf = TfidfVectorizer(stop_words = stop_stem, tokenizer = stemming_tokenizer, max_features = 5000)\n",
    "    # fit tfidf to corpus\n",
    "    m = tfidf.fit_transform(df['text'])\n",
    "    \n",
    "    # record words in corpus with index\n",
    "    feature_names = tfidf.get_feature_names() \n",
    "\n",
    "    return m, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Search from TF-IDF Matrix  \n",
    "Else will often present the search query as a good label for a cluster. Also do not want to include as a clustering factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_search(tfidf, feature_names, search):\n",
    "    try: # sometimes search already removed (ex stop word)\n",
    "        # find index of search in feature_names\n",
    "        search_index = feature_names.index(search)\n",
    "        \n",
    "        # delete from feature_names and tfidf matrix \n",
    "        cols = list(range(0,len(feature_names)))\n",
    "        del cols[cols.index(search_index)]\n",
    "        tfidf = tfidf[:,cols]\n",
    "        del feature_names[search_index]\n",
    "        \n",
    "    # if error, should be a value error because search not in feature_names\n",
    "    except ValueError: \n",
    "        pass\n",
    "    except:\n",
    "        raise 'unknown error'\n",
    "    \n",
    "    return tfidf, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD \n",
    "SVD decomposition of tf-idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_calculate(tfidf):\n",
    "    U, S, Vt = np.linalg.svd(tfidf.todense(), full_matrices = False)\n",
    "    V = Vt.T\n",
    "    return U, S, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find K: Number of Factors to Keep in SVD\n",
    "Calculate retained varaiance at each possible value of k   \n",
    "Calculate rate of change  of retained varaince    \n",
    "    \n",
    "k is also the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_var_calculate(S): # input is S matrix from SVD (singular values)\n",
    "\n",
    "    # calculate retained variance for values of k (sum up to that value of k)\n",
    "    k_var_lst = []\n",
    "    k_var = 0\n",
    "    for i in S:\n",
    "        k_var += i**2\n",
    "        k_var_lst.append(k_var)\n",
    "        \n",
    "    # rate of change of variance\n",
    "    roc = []\n",
    "    for k in range(len(k_var_lst)):\n",
    "        if k+1 < len(k_var_lst):\n",
    "            roc.append(abs(k_var_lst[k+1] - k_var_lst[k]) / k_var_lst[k])\n",
    "    \n",
    "    return roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find elbow in variance ROC curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_knee(roc):\n",
    "    # knee in variance ROC \n",
    "    kn = KneeLocator(range(len(roc)), roc, curve='convex', direction='decreasing')\n",
    "    if kn.knee == None: # sometimes there is no knee, just take the max k in that case\n",
    "        return len(roc)\n",
    "    k = kn.knee + 1 # index starts at 0 -- if knee = 1, then is roc spot 2. Between k = 2 and k = 3 --> k = 2 \n",
    "    \n",
    "    # need at least 2 clusters. Sometimes returns knee such k = 1 \n",
    "    if k < 2:\n",
    "        return 2\n",
    "    \n",
    "    return k "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Dimensionality of V matrix based on K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_V(V, k):\n",
    "    # zero out non-selected k's\n",
    "    V = V[:,:k]     \n",
    "    return V "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Cluster Labels: Top 3 words in each concept (V column)\n",
    "Record label-concept score for sorting clusters   \n",
    "Finding labels that \"describe\" each concept bests: has highest label/word-concept score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_labels(V, feature_names): \n",
    "    # need to record labels in two different formats: \n",
    "        # dictionary where key is 1, 2, or 3: best words for each concept, second best words etc.\n",
    "            # used to assign documents to clusters \n",
    "        # dictionary where key is the concept and values are the corresponding labels\n",
    "            # used for reporting cluster labels\n",
    "            # also record label-concept score to sort final clusters and to find labels of combined clusters in a later step\n",
    "    \n",
    "    # 1. key = top 1, 2, 3\n",
    "    max_w3 = dict()\n",
    "    max_w3_score = dict()\n",
    "    # find 3 words: loop 3 times\n",
    "    for i in range(1,4):\n",
    "        max_w3[i] = []\n",
    "        max_w3_score[i] = []\n",
    "        # loop through columns of V (rows of V.T) and find ith strongest word (largest word-concept strength)\n",
    "        for r in range(len(V.T)):\n",
    "            max_w3[i].append(np.array(V.T[r])[0].argsort()[-i:][::-1][i-1])\n",
    "    \n",
    "    # 2. key = concept\n",
    "    max_w_concept = dict()\n",
    "    max_w_concept_score = dict()\n",
    "    max_score = dict()\n",
    "    # loop through columns of V (rows of V.T)\n",
    "    for r in range(len(V.T)):\n",
    "        # find top 3 scored words in each concept/cluster \n",
    "        if r in max_w_concept:\n",
    "            max_w_concept[r].append(np.array(V.T[r])[0].argsort()[-3:][::-1])\n",
    "            # record word-concept score for each word in label \n",
    "            max_w_concept_score[r].append((np.sort(np.array(V.T[r])[0])[-3:][::-1]))\n",
    "        else:\n",
    "            max_w_concept[r] = [np.array(V.T[r])[0].argsort()[-3:][::-1]]\n",
    "            # record word-concept score for each word in label \n",
    "            max_w_concept_score[r] = [np.sort(np.array(V.T[r])[0])[-3:][::-1]]\n",
    "            \n",
    "        # overall max score for the concept: used to sort final clusters\n",
    "        max_score[r] = np.max(max_w_concept_score[r])\n",
    "\n",
    "    # find corresponding words corresponding to the index numbers recorded in max_w_concept\n",
    "    # these are the labels\n",
    "    labels = dict()\n",
    "    # loop through each concept/cluster\n",
    "    for k,v in max_w_concept.items():\n",
    "        # record corresponding words in feature_names as labels\n",
    "        labels[k] = []\n",
    "        for w in v[0]:\n",
    "            labels[k].append(feature_names[w])\n",
    "            \n",
    "    return labels, max_w3, max_w_concept, max_w_concept_score, max_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Documents to Clusters: Label-Document Score\n",
    "If document-label score above a threshold for _any_ of the three words in the label for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_docs(feature_names, max_w, m):\n",
    "    # term-concept label matrix \n",
    "    # term-term matrix is identity because currently no phrases. \n",
    "\n",
    "    # tf-idf vectors for each of the 1st, 2nd, and 3rd words in each cluster \n",
    "    Q1 = np.identity(len(feature_names))[:,max_w[1]]\n",
    "    Q2 = np.identity(len(feature_names))[:,max_w[2]]\n",
    "    Q3 = np.identity(len(feature_names))[:,max_w[3]]\n",
    "\n",
    "    # find label-document strength: tf-idf vectors for each label word by tf-idf matricies for documents\n",
    "        # cij = strength of membership of jth document to ith concept \n",
    "    C1 = np.matmul(Q1.T, m.T.toarray())\n",
    "    C2 = np.matmul(Q2.T, m.T.toarray())\n",
    "    C3 = np.matmul(Q3.T, m.T.toarray())\n",
    "    \n",
    "    # choose documents for each cluster with strength > 0.1 \n",
    "    # any of the top 3 words in the cluster \n",
    "    docs = dict()\n",
    "    for r in range(len(C1)):\n",
    "        docs[r] = []\n",
    "        for c in range(len(C1[r])): \n",
    "            if C1[r][c] > 0.1 or C2[r][c] > 0.1 or C3[r][c] > 0.1:\n",
    "                docs[r].append(c)\n",
    "\n",
    "    # documents can be in multiple clusters \n",
    "    # documents can be in no clusters \n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Clusters with Overlapping Labels    \n",
    "If any of the 3 label words are the same:\n",
    "- Combine documents into 1 cluster\n",
    "- Choose the 3 highest ranking labels between the two clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: combine dictionary d (labels, label scores) based on combined clusters\n",
    "def combine_cluster_dictionaries(d, combo_clusters):\n",
    "    del_keys = []\n",
    "    # for each cluster to be combined, add values to master cluster dictionary and drop absorbed cluster \n",
    "    for k,v in combo_clusters.items():\n",
    "        if len(combo_clusters[k]) > 0:\n",
    "            for i in v:\n",
    "                d[k] += d[i]\n",
    "                del_keys.append(i)\n",
    "    # delete absorbed clusters\n",
    "    for i in del_keys:\n",
    "        del d[i]\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_clusters(labels, max_w_concept_score, max_w_concept,  max_score, docs, feature_names):\n",
    "    \n",
    "    # find clusters that contain any of the same label words\n",
    "    combo_clusters = dict() # dictionary of cluster# : [clusters to be combined with key cluster]\n",
    "    used_clusters = []\n",
    "    # loop through labels. For each label, check all following labels for overlap.\n",
    "    for k in labels.keys():\n",
    "        combo_clusters[k] = list()\n",
    "        for k2 in labels.keys():\n",
    "            # check cluster hasn't already been marked for combination\n",
    "            if k < k2 and k2 not in used_clusters: \n",
    "                # if overlapping label words, record k and k2 are clusters to be combined\n",
    "                intersect = set(labels[k]).intersection(set(labels[k2]))\n",
    "                if len(intersect) != 0 and k2 not in used_clusters: \n",
    "                    combo_clusters[k].append(k2)\n",
    "                    used_clusters.append(k2)\n",
    "    \n",
    "    # if no clusters need to be combined, return original labels etc.\n",
    "    if len(combo_clusters) == 0:\n",
    "        return docs, labels, max_w_concept_score, max_w_concept\n",
    "    \n",
    "    # for dictionaries with values of scores and label indices, combine clusters and delete absorbed clusters\n",
    "    max_w3_score = combine_cluster_dictionaries(max_w_concept_score, combo_clusters)\n",
    "    max_w3 = combine_cluster_dictionaries(max_w_concept, combo_clusters)\n",
    "    max_score = combine_cluster_dictionaries(max_score, combo_clusters)\n",
    "    \n",
    "    # flatten lists\n",
    "    for k in max_w_concept_score.keys():\n",
    "        max_w_concept_score[k] = [item for sublist in max_w_concept_score[k] for item in sublist]\n",
    "    for k in max_w_concept.keys():\n",
    "        max_w_concept[k] = [item for sublist in max_w_concept[k] for item in sublist]\n",
    "        \n",
    "        \n",
    "    # generate labels of combined clusters: dictionaries of label indicies now longer than 3 after combination \n",
    "    # indices in max_w_concept with the max 3 scores in combined max_w_concept_score per cluster\n",
    "    combined_labels = dict()\n",
    "    count = 0\n",
    "    # loop through label lists\n",
    "    for k in max_w_concept_score.keys():\n",
    "        # take words in label list with the highest concept-label scores (max_w_concept_score)\n",
    "        # take top 5 because there could be duplicates from combination: narrow to 3 later\n",
    "        label_keys = np.array(max_w_concept[k])[list(np.array(max_w_concept_score[k]).argsort()[-5:][::-1])] \n",
    "        # find indicies in feature_arrays, unique values only -> drop duplicates\n",
    "        # sorted such that most important label (highest score) is first (np.unique naturally sorts)\n",
    "        indexes = list(np.unique(np.array(feature_names)[label_keys], return_index = True)[1])\n",
    "        # record corresponding words to indexes in sorted importance order as new labels of combined clusters\n",
    "        combined_labels[count] = [np.array(feature_names)[label_keys][index] for index in sorted(indexes)]        \n",
    "        count += 1\n",
    "        \n",
    "    # limit labels to first 3 words (most important)\n",
    "    for k,v in combined_labels.items():\n",
    "        combined_labels[k] = v[:3]\n",
    "\n",
    "        \n",
    "    # combine documents based on combined clusters \n",
    "    docs = combine_cluster_dictionaries(docs, combo_clusters)\n",
    "    \n",
    "    # reset keys to logical values (numerically ascending)\n",
    "    count = 0\n",
    "    docs_copy = docs.copy()\n",
    "    for k in docs_copy.keys():\n",
    "        docs[count] = docs.pop(k)\n",
    "        count += 1\n",
    "\n",
    "    count = 0\n",
    "    max_score_copy = max_score.copy()\n",
    "    for k in max_score_copy.keys():\n",
    "        max_score[count] = max_score.pop(k)\n",
    "        count += 1\n",
    "        \n",
    "    return docs, combined_labels, max_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un-Stem Labels\n",
    "Find the most frequent word corresponding to each stem out of the documents returned for a search   \n",
    "Replace stems in labels with that word    \n",
    "Else stems are not always human readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_unstem_create(df, labels, search):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # generate fresh tfidf without stemming\n",
    "    tfidf = TfidfVectorizer(stop_words = stopwords.words('english'))\n",
    "    tfidf_norm = tfidf.fit_transform(df['text'])\n",
    "    words = tfidf.get_feature_names()\n",
    "\n",
    "    # record stemmed versions of all words in a df\n",
    "    aux = pd.DataFrame(words, columns =['word'] )\n",
    "    aux['word_stemmed'] = aux['word'].apply(lambda x : stemmer.stem(x))\n",
    "    \n",
    "    # count the frequency of all words \n",
    "    vec = sklearn.feature_extraction.text.CountVectorizer().fit(df['text'])\n",
    "    bag_of_words = vec.transform(df['text'])\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = pd.DataFrame(words_freq)\n",
    "    words_freq.columns = ['word', 'num']\n",
    "    \n",
    "    # merge frequency of words with stemming. \n",
    "    # sort such that first instance of a stem corresponds to the most frequent variation of that stemmed word\n",
    "    aux = pd.merge(aux, words_freq, on = 'word', how = 'left')\n",
    "    aux = aux.sort_values(['word_stemmed', 'num'], ascending = False)\n",
    "            \n",
    "    # remove search term from auxilliary if it exists\n",
    "    try:\n",
    "        aux = aux[aux.word != search]\n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "    # loop through labels and grab the most frequent un-stemmed word for each stem encountered\n",
    "    labels_unstem = collections.OrderedDict()\n",
    "    # loop through labels\n",
    "    for i in labels.keys():\n",
    "        labels_unstem[i] = []\n",
    "        # loop through words in a each label\n",
    "        for j in labels[i]:\n",
    "            if len(aux[aux.word_stemmed == j]) == 0:\n",
    "                labels_unstem[i].append(j)\n",
    "                continue\n",
    "            # else take most frequent unstemmed word\n",
    "            labels_unstem[i].append(aux[aux.word_stemmed == j].word.values[0])\n",
    "                \n",
    "    return labels_unstem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Clusters in DF \n",
    "Add list of assigned clusters to DF with document IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cluster_df(df, docs):\n",
    "    # create dataframe that indicates which documents belong to which cluster and labels. List of clusters. \n",
    "    framesvd = df\n",
    "    framesvd['cluster'] = ''\n",
    "    for k,v in docs.items():\n",
    "        for d in v: \n",
    "            framesvd.cluster = np.where(framesvd.index == d, framesvd.cluster + str(k) + ',', framesvd.cluster)\n",
    "\n",
    "    # create listof clusters\n",
    "    framesvd.cluster = framesvd.cluster.str.split(',')\n",
    "    # drop blanks\n",
    "    framesvd.cluster = framesvd.cluster.apply(lambda row: [i for i in row if i != ''])\n",
    "    # drop duplicates\n",
    "    framesvd.cluster = framesvd.cluster.apply(set)\n",
    "    framesvd.cluster = framesvd.cluster.apply(list)\n",
    "    # convert into integers\n",
    "    framesvd.cluster = framesvd.cluster.apply(lambda row: [int(i) for i in row])\n",
    "    \n",
    "    return framesvd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort Clusters\n",
    "Max label-concept score * size of cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sort(labels, max_score, frame):\n",
    "    # multiply max_score by number of documents in that cluster: prefer large and well separated clusters \n",
    "    for k,v in max_score.items():\n",
    "        max_score[k] = max_score[k] * len(frame[frame.cluster.map(set([k]).issubset)])\n",
    "    \n",
    "    # sort max score\n",
    "    max_score = sorted(max_score.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    \n",
    "    # add values in sorted order to labels: ordered dictionary\n",
    "    labels_sorted = collections.OrderedDict()\n",
    "    for i in max_score:\n",
    "        labels_sorted[i[0]] = labels[i[0]]\n",
    "        \n",
    "    return labels_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Metrics\n",
    "Not used to calculate clusters - just for evaluation and comparison with Hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Euclidean Distance: TFIDF__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_calculate(tfidf):\n",
    "    dist = euclidean_distances(tfidf)  \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find Cluster Centroids__    \n",
    "Mean TFIDF vector among documents in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_centroids(frameh, tfidf, k):\n",
    "    # most common words in clusters (based on tf-idf not just frequency)\n",
    "    centroid = dict()\n",
    "    # loop through clusters\n",
    "    for c in range(k):\n",
    "        # identify documents in cluster \n",
    "        cluster1 = list(frameh[frameh.cluster.map(set([c]).issubset)].index.unique())\n",
    "        # mean vector of tf-idf vectors of documents in cluster\n",
    "        tfidf1 = tfidf[cluster1,:]\n",
    "        tfidf1 = tfidf1.mean(axis = 0)\n",
    "        centroid[c] = tfidf1\n",
    "        \n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate Silhouette Score__  \n",
    "Return overall average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate individual silhouette for each point \n",
    "def silhouette_individ(frameh, dist, k):\n",
    "    \n",
    "    # average distance to points in own cluster\n",
    "    sil_a = dict()\n",
    "    # loop through clusters\n",
    "    for c in range(k):\n",
    "        sil_a[c] = dict()\n",
    "        # all documents in cluster c \n",
    "        docs_i = list(frameh[frameh.cluster.map(set([c]).issubset)].index.unique())\n",
    "        # if one document in cluster, then avg distance = 0\n",
    "        if len(docs_i) == 1:\n",
    "            sil_a[c][docs_i[0]] = 0\n",
    "        else:\n",
    "            # loop through points in each cluster and take mean distance from other points in documents\n",
    "            for i in docs_i:\n",
    "                docs_i.remove(i)\n",
    "                sil_a[c][i] = np.nanmean(dist[i, docs_i].tolist())\n",
    "                \n",
    "    # minimum average distance to points in other clusters \n",
    "    # for each point, create list of average distances to points in each other cluster. Take minimum of list\n",
    "    sil_b = dict()\n",
    "    # loop through clusters\n",
    "    for c in range(k):\n",
    "        sil_b[c] = dict()\n",
    "        # all documents in cluster c\n",
    "        docs_in = list(frameh[frameh.cluster.map(set([c]).issubset)].index.unique())\n",
    "        # loop through points in each cluster -> loop through other clusters -> loop through documents in that cluster \n",
    "        for i in docs_in:\n",
    "            lst = []\n",
    "            for c2 in range(k):\n",
    "                if c2 != c:\n",
    "                    docs_out = list(frameh[frameh.cluster.map(set([c2]).issubset)].index.unique())\n",
    "                    if i in docs_out: # if target document is in cluster (can be in multiple), remove \n",
    "                        docs_out.remove(i)\n",
    "                    # loop through documents in other cluster and find average distance\n",
    "                    lst.append(np.nanmean(dist[i,docs_out].tolist()))\n",
    "                \n",
    "            # take minimum of average distance to other clusters\n",
    "            sil_b[c][i] = np.min(lst)\n",
    "            \n",
    "    return sil_a, sil_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_avg_calculate(frameh, dist, k):\n",
    "    # silhouette components from each individual point\n",
    "    sil_a, sil_b = silhouette_individ(frameh, dist, k)\n",
    "    \n",
    "    # take average among points in same cluster to get cluster-level silhouette\n",
    "    sil_scores = dict()\n",
    "    for k,v in sil_a.items():\n",
    "        lst = []\n",
    "        for i in range(len(v.values())):\n",
    "            max_ab = max(list(sil_b[k].values())[i], list(sil_a[k].values())[i])\n",
    "            min_ab = min(list(sil_b[k].values())[i], list(sil_a[k].values())[i])\n",
    "            # calculate silhouette score \n",
    "            lst.append(1 - min_ab/max_ab)\n",
    "        sil_scores[k] = np.nanmean(lst) \n",
    "       \n",
    "    # take average among all clusters to get overall silhouette\n",
    "    return np.nanmean(list(sil_scores.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate Distortion__   \n",
    "Calculate total and average distortion of a clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortion_calculate(tfidf, centroid, frameh):\n",
    "    sumd = 0\n",
    "    countpts = 0\n",
    "    # sum together distortion for each cluster \n",
    "    for i in list(frameh.index.unique()):\n",
    "        for c in frameh[frameh.index == i].cluster.tolist()[0]:\n",
    "            sumd += np.linalg.norm(tfidf[i]-centroid[c])**2\n",
    "            countpts +=1 \n",
    "        \n",
    "    return sumd, sumd/countpts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # read in processed data\n",
    "    df = pd.read_pickle('reuters_processed')\n",
    "    \n",
    "    # set of topics/search terms\n",
    "    topics = list(df.categories)\n",
    "    topics = [item for sublist in topics for item in sublist]\n",
    "    topics = list(set(topics))\n",
    "\n",
    "    # initialize data structures to store stats for each search query/topic's clustering\n",
    "    df_final = pd.DataFrame()\n",
    "    labels_dict = dict()\n",
    "    k_dict = dict()\n",
    "    silhouette_dict = dict()\n",
    "    distortion_dict = dict()\n",
    "    max_score_dict = dict()\n",
    "    percent_zero_dict = dict()\n",
    "    percent_mult_dict = dict()\n",
    "    cluster1 = []\n",
    "\n",
    "    # loop through all search queries \n",
    "    for search in topics:\n",
    "        gc.collect()\n",
    "        \n",
    "        # subset of dataframe for search\n",
    "        df_subset = df[df.categories.map(set([search]).issubset)] \n",
    "        df_subset = df_subset.reset_index()\n",
    "        \n",
    "        # if less than 5 documents in search, enforce 1 cluster\n",
    "        if len(df_subset) < 5:\n",
    "            cluster1.append(search)\n",
    "            continue\n",
    "            \n",
    "        # skip two search terms that return a large number of serach tera\n",
    "        if search == 'earn' or search == 'acq': # too big to deal with\n",
    "            continue\n",
    "        \n",
    "        print(search)\n",
    "        \n",
    "        # TF-IDF matrix\n",
    "        tfidf, feature_names = tf_idf(df_subset)\n",
    "                \n",
    "        # remove search from tf-idf matrix so is not a clustering factor or a label \n",
    "        tfidf, feature_names = remove_search(tfidf, feature_names, search)\n",
    "\n",
    "        # SVD \n",
    "        U, S, V = svd_calculate(tfidf)\n",
    "\n",
    "        # Find K via elbow in ROC of retained variance and reduce dimensionality\n",
    "        roc = roc_var_calculate(S)\n",
    "        k = find_knee(roc) \n",
    "        V = reduce_V(V, k)\n",
    "\n",
    "        # Find cluster labels: max concept-word scores \n",
    "        labels, max_w3, max_w_concept, max_w_concept_score, max_score = find_labels(V, feature_names)\n",
    "        \n",
    "        # Assign documents to clusters based on labels\n",
    "        docs = find_docs(feature_names, max_w3, tfidf)\n",
    "                \n",
    "        # delete cluster if no documents in it (no documents pass threshold)\n",
    "        for k,v in docs.copy().items():\n",
    "            if len(docs[k]) == 0:\n",
    "                del docs[k]\n",
    "                del labels[k]\n",
    "                del max_score[k]\n",
    "                del max_w_concept_score[k]\n",
    "                del max_w_concept[k]\n",
    "                \n",
    "        # combine clusters with overlapping labels\n",
    "        docs, labels, max_score = combine_clusters(labels, max_w_concept_score, max_w_concept, max_score, docs, feature_names)\n",
    "\n",
    "        # mark clusters in dataframe\n",
    "        frame = cluster_df(df_subset, docs)\n",
    "\n",
    "        # sort clusters\n",
    "        labels = label_sort(labels, max_score, frame)\n",
    "        \n",
    "        # unstem labels\n",
    "        labels = labels_unstem_create(df_subset, labels, search)        \n",
    "        \n",
    "        # if process returned only one cluster (after merging), skip the rest of the calculations \n",
    "        if len(labels) == 1:\n",
    "            cluster1.append(search)\n",
    "            continue\n",
    "            \n",
    "        # percent of documents with no cluster\n",
    "        frame['len']= frame.cluster.apply(lambda row: len(row))\n",
    "        percent_zero_dict[search] = len(frame[frame.len == 0]) / len(frame)\n",
    "        \n",
    "        # percent of documents in multiple clusters\n",
    "        percent_mult_dict[search] = len(frame[frame.len > 1]) / len(frame)\n",
    "        \n",
    "        # calculate metrics for comparison\n",
    "        dist = dist_calculate(tfidf)\n",
    "        centroid = find_centroids(frame, tfidf, len(labels))\n",
    "        silhouette = silhouette_avg_calculate(frame, dist, len(labels))\n",
    "        distortion, distortion_avg = distortion_calculate(tfidf, centroid, frame)\n",
    "        \n",
    "        # record statistics for this serach \n",
    "        labels_dict[search] = labels\n",
    "        k_dict[search] = len(labels) # don't use k because combined clusters after that step\n",
    "        silhouette_dict[search] = silhouette\n",
    "        distortion_dict[search] = distortion_avg\n",
    "        max_score_dict[search] = max_score\n",
    "\n",
    "        frame['search'] = search\n",
    "        df_final = df_final.append(frame)\n",
    "        \n",
    "    return df_final, labels_dict, percent_zero_dict, percent_mult_dict, k_dict, distortion_dict, silhouette_dict, cluster1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soy-oil\n",
      "oilseed\n",
      "ship\n",
      "rubber\n",
      "instal-debt\n",
      "retail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gimli\\Anaconda3\\lib\\site-packages\\kneed\\knee_locator.py:188: UserWarning: No knee/elbow found\n",
      "  warnings.warn(\"No knee/elbow found\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groundnut\n",
      "pet-chem\n",
      "soybean\n",
      "coconut\n",
      "housing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gimli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: RuntimeWarning: Mean of empty slice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade\n",
      "heat\n",
      "naphtha\n",
      "grain\n",
      "coconut-oil\n",
      "silver\n",
      "lead\n",
      "corn\n",
      "meal-feed\n",
      "gas\n",
      "oat\n",
      "wpi\n",
      "rice\n",
      "dmk\n",
      "jobs\n",
      "yen\n",
      "reserves\n",
      "interest\n",
      "fuel\n",
      "coffee\n",
      "propane\n",
      "money-fx\n",
      "lumber\n",
      "carcass\n",
      "orange\n",
      "alum\n",
      "crude\n",
      "tea\n",
      "sorghum\n",
      "barley\n",
      "income\n",
      "nickel\n",
      "jet\n",
      "gnp\n",
      "strategic-metal\n",
      "money-supply\n",
      "lei\n",
      "wheat\n",
      "bop\n",
      "dlr\n",
      "rape-oil\n",
      "sugar\n",
      "copper\n",
      "ipi\n",
      "sun-oil\n",
      "tin\n",
      "gold\n",
      "soy-meal\n",
      "veg-oil\n",
      "rapeseed\n",
      "livestock\n",
      "palm-oil\n",
      "l-cattle\n",
      "cocoa\n",
      "platinum\n",
      "potato\n",
      "iron-steel\n",
      "cpi\n",
      "hog\n",
      "cotton\n",
      "nat-gas\n",
      "zinc\n",
      "sunseed\n"
     ]
    }
   ],
   "source": [
    "df_final, labels_dict, percent_zero_dict, percent_mult_dict, k_dict, distortion_dict, silhouette_dict, cluster1 = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data locally\n",
    "with open('lingo', \"wb\") as f:\n",
    "    pickle.dump(df_final, f)\n",
    "    pickle.dump(labels_dict, f)\n",
    "    pickle.dump(k_dict, f)\n",
    "    pickle.dump(distortion_dict, f)\n",
    "    pickle.dump(silhouette_dict, f)\n",
    "    pickle.dump(percent_zero_dict, f)\n",
    "    pickle.dump(percent_mult_dict, f)\n",
    "    pickle.dump(cluster1, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
