{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO SVD:\n",
    "- Try to get phrases rather than just words - perhaps beyond scope\n",
    "    - Can specify bigrams in tokenizer, but then returns \"australian\" \"australian prime\" \"prime\" - need some cleaning\n",
    "    - Also need to force both words to be nouns. else \"fed says\" \n",
    "- Eliminate labels with low edit distance (also hierarchial?) \n",
    "- reintroduce drop clusters with 1 document? how frequent is this?\n",
    "- is there any way to visualize these clusters?? \n",
    "- once do a full run, look at statistic distributions and look at outliers \n",
    "\n",
    "TO DO in general:\n",
    "- Visualization with tsne or umap rather than mds -- tsne works. can't get umap to install.    \n",
    "- If time figure out proximity matrix and do correlation analysis\n",
    "\n",
    "SVD generally results in more clusters because restricting more in hierarchial in where we are searching for the knee. But this is a good thing -- hierarchial can get more granular with hierarhcy, not possible with SVD   \n",
    "   \n",
    "Super garbage silhouette score, but not surprising: points are in multiple clusters so comparing with points also in your own cluster.    \n",
    "In general, looking at how separated clusters are is not helpful for svd because can belong to multiple clusters   \n",
    "    \n",
    "Am currently not lemmatizing like in hiearchial. Made labels look bad. Don't know why.   \n",
    "Maybe want to enforce some edit distance requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gimli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "import sklearn\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, ward, fcluster\n",
    "import networkx as nx\n",
    "import collections\n",
    "import math\n",
    "import operator\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from kneed import KneeLocator\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_tokenizer(str_input):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(df):\n",
    "    \n",
    "    #stemmer = PorterStemmer()\n",
    "    \n",
    "    #stop_lem = [stemming_tokenizer(t) for t in stopwords.words('english')]\n",
    "    #stop_lem = [item for sublist in stop_lem for item in sublist]\n",
    "    \n",
    "    # tfidf. stop word removal. word tokenizer. \n",
    "    #tfidf = TfidfVectorizer(stop_words = stop_lem, tokenizer = stemming_tokenizer)\n",
    "    tfidf = TfidfVectorizer(stop_words = stopwords.words('english'), max_features = 5000)\n",
    "\n",
    "    m = tfidf.fit_transform(df['text'])\n",
    "    \n",
    "    feature_names = tfidf.get_feature_names() # words \n",
    "\n",
    "    return m, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Search from TF-IDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_search(tfidf, feature_names, search):\n",
    "    try: # sometimes search already removed (stop word)\n",
    "        # remove search from the tfidf matrix: do not want as a label or clustering factor\n",
    "        search_index = feature_names.index(search)\n",
    "        cols = list(range(0,len(feature_names)))\n",
    "        del cols[cols.index(search_index)]\n",
    "        tfidf = tfidf[:,cols]\n",
    "        del feature_names[search_index]\n",
    "    except ValueError: \n",
    "        pass\n",
    "    except:\n",
    "        raise 'unknown error'\n",
    "    \n",
    "    return tfidf, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_calculate(m):\n",
    "    U, S, Vt = np.linalg.svd(m.todense(), full_matrices = False) # full_matrices make dimensions work\n",
    "    V = Vt.T\n",
    "    return U, S, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance for various values of k -> rate of change \n",
    "# preference is to choose some percent of variance, but 80% retains too many topics \n",
    "def roc_var_calculate(S):\n",
    "    # calculate variance for various values of k\n",
    "    k_var_lst = []\n",
    "    k_var = 0\n",
    "    for i in S:\n",
    "        k_var += i**2\n",
    "        k_var_lst.append(k_var)\n",
    "            \n",
    "    # rate of change of variance\n",
    "    roc = []\n",
    "    for k in range(len(k_var_lst)):\n",
    "        if k+1 < len(k_var_lst):\n",
    "            roc.append(abs(k_var_lst[k+1] - k_var_lst[k]) / k_var_lst[k])\n",
    "    \n",
    "    return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Elbow in Variance ROC \n",
    "def find_knee(roc):\n",
    "    # knee in variance ROC \n",
    "    kn = KneeLocator(range(len(roc)), roc, curve='convex', direction='decreasing')\n",
    "    if kn.knee == None: # sometimes there is no knee, just take the max k in that case\n",
    "        return len(roc)\n",
    "    k = kn.knee + 1 # index starts at 0 -- if knee = 1, then is roc spot 2. Between k = 2 and k = 3 --> k = 2 \n",
    "    #https://raghavan.usc.edu//papers/kneedle-simplex11.pdf\n",
    "    \n",
    "    # need at least 2 clusters\n",
    "    if k < 2:\n",
    "        return 2\n",
    "    \n",
    "    return k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots()\n",
    "#ax.plot(roc)\n",
    "#ax.axvline(k-1, color = 'black')\n",
    "#print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_V(V, k):\n",
    "    # zero out non-selected k's\n",
    "    #S[k:] = 0\n",
    "    V = V[:,:k] \n",
    "    # V is term to concept. Because no phrases yet, no need to multiple by a term matrix. Currently T = identity \n",
    "    \n",
    "    return V "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Cluster Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_labels(V, feature_names): # top 3 words/vectors \n",
    "    # find maximum 3 term vectors for each column (concept) in V\n",
    "    # find top 3 with key = top 1, 2, 3\n",
    "    max_w = dict()\n",
    "    max_w_score = dict()\n",
    "    for i in range(1,4):\n",
    "        max_w[i] = []\n",
    "        max_w_score[i] = []\n",
    "        for r in range(len(V.T)):\n",
    "            max_w[i].append(np.array(V.T[r])[0].argsort()[-i:][::-1][i-1])\n",
    "            max_w_score[i].append(np.sort(np.array(V.T[r])[0])[-i:][::-1][i-1])\n",
    "    \n",
    "    # find top 3 with key = concept\n",
    "    max_w3 = dict()\n",
    "    max_w3_score = dict()\n",
    "    max_score = dict()\n",
    "    for r in range(len(V.T)):\n",
    "        if r in max_w3:\n",
    "            max_w3[r].append(np.array(V.T[r])[0].argsort()[-3:][::-1])\n",
    "            max_w3_score[r].append((np.sort(np.array(V.T[r])[0])[-3:][::-1]))\n",
    "        else:\n",
    "            max_w3[r] = [np.array(V.T[r])[0].argsort()[-3:][::-1]]\n",
    "            max_w3_score[r] = [np.sort(np.array(V.T[r])[0])[-3:][::-1]]\n",
    "        max_score[r] = np.max(max_w3_score[r])\n",
    "\n",
    "    # find corresponding words -> labels \n",
    "    labels = dict()\n",
    "    for k,v in max_w3.items():\n",
    "        labels[k] = []\n",
    "        for w in v[0]:\n",
    "            labels[k].append(feature_names[w])\n",
    "            \n",
    "    return labels, max_w, max_w3, max_w3_score, max_score\n",
    "    \n",
    "# label overlap only if use phrases (Z^TZ)\n",
    "# TO DO: what about phrases? then could just use max "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Documents in Clusters Based on Labels \n",
    "Can include any of the top 3 words that describes each cluster/concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_docs(feature_names, max_w, m):\n",
    "    # term-concept label matrix \n",
    "    # term-term matrix is identity because currently no phrases. \n",
    "\n",
    "    # compute for each of the top 3 words \n",
    "    # assign to cluster if any of those words exist in the document \n",
    "    Q1 = np.identity(len(feature_names))[:,max_w[1]]\n",
    "    Q2 = np.identity(len(feature_names))[:,max_w[2]]\n",
    "    Q3 = np.identity(len(feature_names))[:,max_w[3]]\n",
    "\n",
    "    # cij = strength of membership of jth document to ith concept \n",
    "    C1 = np.matmul(Q1.T, m.T.toarray())\n",
    "    C2 = np.matmul(Q2.T, m.T.toarray())\n",
    "    C3 = np.matmul(Q3.T, m.T.toarray())\n",
    "    \n",
    "    # choose documents for each cluster with strength > 0 aka exists in document\n",
    "    # any of the top 3 words in the cluster \n",
    "    docs = dict()\n",
    "    for r in range(len(C1)):\n",
    "        docs[r] = []\n",
    "        for c in range(len(C1[r])): \n",
    "            if C1[r][c] > 0 or C2[r][c] > 0 or C3[r][c] > 0: # threshold \n",
    "                docs[r].append(c)\n",
    "\n",
    "    # documents can be in multiple clusters \n",
    "    # documents can be in no clusters \n",
    "\n",
    "    # drop clusters with only 1 document in it \n",
    "    #del_lst = []\n",
    "    #for k,v in docs.items():\n",
    "    #    if len(v) == 1:\n",
    "    #        del_lst.append(k)\n",
    "    #for i in del_lst:\n",
    "    #    del docs[i]\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Clusters with Overlapping Labels    \n",
    "If any of the 3 label words are the same:\n",
    "- Combine documents into 1 cluster\n",
    "- Choose the 3 highest ranking labels between the two clusters \n",
    "\n",
    "Allowing multiple clusters to combine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine dictionaries based on combined clusters\n",
    "def combine_dictionary_old(d, combo_clusters):\n",
    "    del_keys = []\n",
    "    for i in combo_clusters:\n",
    "        d[i[0]] += d[i[1]]\n",
    "        del_keys.append(i[1])\n",
    "    # delete absorbed clusters\n",
    "    for i in del_keys:\n",
    "        del d[i]\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine dictionaries based on combined clusters\n",
    "def combine_dictionary(d, combo_clusters):\n",
    "    del_keys = []\n",
    "    for k,v in combo_clusters.items():\n",
    "        if len(combo_clusters[k]) > 0:\n",
    "            for i in v:\n",
    "                d[k] += d[i]\n",
    "                del_keys.append(i)\n",
    "    # delete absorbed clusters\n",
    "    for i in del_keys:\n",
    "        del d[i]\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_clusters(labels, max_w3_score, max_w3,  max_score, docs, feature_names):\n",
    "    \n",
    "    # find clusters that contain any of the same label words\n",
    "    # only allow one combination per cluster. sometimes multiple clusters with the same word\n",
    "        # in the future, could do more combinations based on cleaned until convergence if deemed necessary\n",
    "    combo_clusters = dict()\n",
    "    used_clusters = []\n",
    "    for k in labels.keys():\n",
    "        combo_clusters[k] = list()\n",
    "        for k2 in labels.keys():\n",
    "            if k < k2 and k2 not in used_clusters: \n",
    "                intersect = set(labels[k]).intersection(set(labels[k2]))\n",
    "                if len(intersect) != 0 and k2 not in used_clusters:\n",
    "                    combo_clusters[k].append(k2)\n",
    "                    used_clusters.append(k2)\n",
    "    if len(combo_clusters) == 0:\n",
    "        return docs, labels, max_w3_score, max_w3\n",
    "    \n",
    "    # combine scores and label indicies based on combined clusters \n",
    "    max_w3_score = combine_dictionary(max_w3_score, combo_clusters)\n",
    "    max_w3 = combine_dictionary(max_w3, combo_clusters)\n",
    "    max_score = combine_dictionary(max_score, combo_clusters)\n",
    "    \n",
    "    # flatten lists\n",
    "    for k in max_w3_score.keys():\n",
    "        max_w3_score[k] = [item for sublist in max_w3_score[k] for item in sublist]\n",
    "    for k in max_w3.keys():\n",
    "        max_w3[k] = [item for sublist in max_w3[k] for item in sublist]\n",
    "        \n",
    "    # generate combined labels \n",
    "    # indices in max_w3 with the max 3 scores in combined max_w3_score per cluster\n",
    "    combined_labels = dict()\n",
    "    count = 0\n",
    "    for k in max_w3_score.keys():\n",
    "        # take first 5 because then will drop repeats. limit to 3 next\n",
    "        label_keys = np.array(max_w3[k])[list(np.array(max_w3_score[k]).argsort()[-5:][::-1])] \n",
    "        # find indicies in feature_arrays, unique values only\n",
    "        # get indexes first to preserve order. np.unique naturally sorts. Want most important label first. \n",
    "        indexes = list(np.unique(np.array(feature_names)[label_keys], return_index = True)[1])\n",
    "        combined_labels[count] = [np.array(feature_names)[label_keys][index] for index in sorted(indexes)]        \n",
    "        count += 1\n",
    "        \n",
    "    # limit labels to first 3\n",
    "    for k,v in combined_labels.items():\n",
    "        combined_labels[k] = v[:3]\n",
    "\n",
    "    # combine documents based on combined clusters \n",
    "    docs = combine_dictionary(docs, combo_clusters)\n",
    "    # reset keys to logical values\n",
    "    count = 0\n",
    "    docs_copy = docs.copy()\n",
    "    for k in docs_copy.keys():\n",
    "        docs[count] = docs.pop(k)\n",
    "        count += 1\n",
    "        \n",
    "    # reset keys to logical values\n",
    "    count = 0\n",
    "    max_score_copy = max_score.copy()\n",
    "    for k in max_score_copy.keys():\n",
    "        max_score[count] = max_score.pop(k)\n",
    "        count += 1\n",
    "        \n",
    "    return docs, combined_labels, max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_clusters_old(labels, max_w3_score, max_w3, docs, feature_names):\n",
    "    \n",
    "    #combo_clusters = []\n",
    "    #for k in labels.keys():\n",
    "    #    for k2 in labels.keys():\n",
    "    #        if k > k2:\n",
    "    #            intersect = set(labels[k]).intersection(set(labels[k2]))\n",
    "    #            if len(intersect) != 0:\n",
    "    #                combo_clusters.append([k, k2])\n",
    "    \n",
    "    # find clusters that contain any of the same label words\n",
    "    # only allow one combination per cluster. sometimes multiple clusters with the same word\n",
    "        # in the future, could do more combinations based on cleaned until convergence if deemed necessary\n",
    "    combo_clusters = []\n",
    "    used_clusters = []\n",
    "    for k in labels.keys():\n",
    "        for k2 in labels.keys():\n",
    "            if k > k2 and k not in used_clusters: \n",
    "                intersect = set(labels[k]).intersection(set(labels[k2]))\n",
    "                if len(intersect) != 0 and k2 not in used_clusters:\n",
    "                    combo_clusters.append([k, k2])\n",
    "                    used_clusters.append(k)\n",
    "                    used_clusters.append(k2)\n",
    "    if len(combo_clusters) == 0:\n",
    "        return docs, labels, max_w3_score, max_w3\n",
    "                    \n",
    "    # combine scores and label indicies based on combined clusters \n",
    "    max_w3_score = combine_dictionary_old(max_w3_score, combo_clusters)\n",
    "    max_w3 = combine_dictionary_old(max_w3, combo_clusters)\n",
    "    \n",
    "    # flatten lists\n",
    "    for k in max_w3_score.keys():\n",
    "        max_w3_score[k] = [item for sublist in max_w3_score[k] for item in sublist]\n",
    "    for k in max_w3.keys():\n",
    "        max_w3[k] = [item for sublist in max_w3[k] for item in sublist]\n",
    "        \n",
    "    # generate combined labels \n",
    "    # indices in max_w3 with the max 3 scores in combined max_w3_score per cluster\n",
    "    combined_labels = dict()\n",
    "    count = 0\n",
    "    for k in max_w3_score.keys():\n",
    "        # take first 5 because then will drop repeats. limit to 3 next\n",
    "        label_keys = np.array(max_w3[k])[list(np.array(max_w3_score[k]).argsort()[-5:][::-1])] \n",
    "        # find indicies in feature_arrays, unique values only\n",
    "        # get indexes first to preserve order. np.unique naturally sorts. Want most important label first. \n",
    "        indexes = list(np.unique(np.array(feature_names)[label_keys], return_index = True)[1])\n",
    "        combined_labels[count] = [np.array(feature_names)[label_keys][index] for index in sorted(indexes)]        \n",
    "        count += 1\n",
    "        \n",
    "    # limit labels to first 3\n",
    "    for k,v in combined_labels.items():\n",
    "        combined_labels[k] = v[:3]\n",
    "\n",
    "    # combine documents based on combined clusters \n",
    "    docs = combine_dictionary_old(docs, combo_clusters)\n",
    "    # reset keys to logical values\n",
    "    count = 0\n",
    "    docs_copy = docs.copy()\n",
    "    for k in docs_copy.keys():\n",
    "        docs[count] = docs.pop(k)\n",
    "        count += 1\n",
    "        \n",
    "\n",
    "        \n",
    "    return docs, combined_labels, max_w3_score, max_w3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un-Lemmatize Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_unlem(df, labels):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # tfidf without lemitization\n",
    "    tfidf = TfidfVectorizer(stop_words = stopwords.words('english'))\n",
    "    m_norm = tfidf.fit_transform(df['text'])\n",
    "    words = tfidf.get_feature_names()\n",
    "\n",
    "    # dataframe that records words and their lemitized versions\n",
    "    aux = pd.DataFrame(words, columns =['word'] )\n",
    "    aux['word_stemmed'] = aux['word'].apply(lambda x : stemmer.stem(x))\n",
    "\n",
    "    # loop through returned labels and grab the first instance of the un-lemmatized word (just any version will do)\n",
    "    labels_unlem = dict()\n",
    "    for i in labels.keys():\n",
    "        labels_unlem[i] = []\n",
    "        for j in labels[i]:\n",
    "            labels_unlem[i].append(aux[aux.word_stemmed == j].word.values[0])\n",
    "            \n",
    "    return labels_unlem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Clusters in DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cluster_df(df, docs):\n",
    "    # create dataframe that indicates which documents belong to which cluster and labels. List of clusters. \n",
    "    framesvd = df\n",
    "    #framesvd['label'] = ''\n",
    "    framesvd['cluster'] = ''\n",
    "    for k,v in docs.items():\n",
    "        for d in v: \n",
    "            framesvd.cluster = np.where(framesvd.index == d, framesvd.cluster + str(k) + ',', framesvd.cluster)\n",
    "            #framesvd.label = np.where(framesvd.index == d, str(labels[k]), framesvd.label)\n",
    "                    # this only gets one of th clusters in the list. not all of them. \n",
    "\n",
    "    # create list\n",
    "    framesvd.cluster = framesvd.cluster.str.split(',')\n",
    "    # drop blanks\n",
    "    framesvd.cluster = framesvd.cluster.apply(lambda row: [i for i in row if i != ''])\n",
    "    # drop duplicates\n",
    "    framesvd.cluster = framesvd.cluster.apply(set)\n",
    "    framesvd.cluster = framesvd.cluster.apply(list)\n",
    "    # integers\n",
    "    framesvd.cluster = framesvd.cluster.apply(lambda row: [int(i) for i in row])\n",
    "    \n",
    "    return framesvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_calculate(m):\n",
    "    dist = euclidean_distances(m)  \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sort(labels, max_score):\n",
    "    max_score = sorted(max_score.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    \n",
    "    labels_sorted = dict()\n",
    "    for i in max_score:\n",
    "        labels_sorted[i[0]] = labels[i[0]]\n",
    "        \n",
    "    return labels_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find Cluster Centroids__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_centroids(frameh, m, k):\n",
    "    # most common words in clusters (based on tf-idf not just frequency)\n",
    "    centroid = dict()\n",
    "    for c in range(k):\n",
    "        ## centroid ## \n",
    "        cluster1 = list(frameh[frameh.cluster.map(set([c]).issubset)].index.unique())\n",
    "        # find documents cluster\n",
    "        m1 = m[cluster1,:]\n",
    "        # take mean vector among all documents\n",
    "        m1 = m1.mean(axis = 0)\n",
    "        # record mean vector: centroids of each sub cluster\n",
    "        centroid[c] = m1\n",
    "        \n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate Silhouette__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_individ(frameh, dist, k):\n",
    "    # average distance to points in your cluster\n",
    "    sil_a = dict()\n",
    "    for c in range(k):\n",
    "        sil_a[c] = dict()\n",
    "        docs_i = list(frameh[frameh.cluster.map(set([c]).issubset)].index.unique())\n",
    "        if len(docs_i) == 1:\n",
    "            sil_a[c][docs_i[0]] = 0\n",
    "        else:\n",
    "            for i in docs_i.copy():\n",
    "                docs_i.remove(i)\n",
    "                sil_a[c][i] = np.nanmean(dist[i,docs_i].tolist())\n",
    "\n",
    "    # minimum average distance to points in other clusters \n",
    "    sil_b = dict()\n",
    "    for c in range(k):\n",
    "        sil_b[c] = dict()\n",
    "        docs_in = list(frameh[frameh.cluster.map(set([c]).issubset)].index.unique())\n",
    "        for i in docs_in:\n",
    "            # loop through other clusters and find average distance \n",
    "            lst = []\n",
    "            for c2 in range(k):\n",
    "                if c2 != c:\n",
    "                    docs_out = list(frameh[frameh.cluster.map(set([c2]).issubset)].index.unique())\n",
    "                    if i in docs_out: # can be in multiple clusters\n",
    "                        docs_out.remove(i)\n",
    "                    lst.append(np.nanmean(dist[i,docs_out].tolist()))\n",
    "                \n",
    "            # take minimum of average distance to other clusters\n",
    "            sil_b[c][i] = np.min(lst)\n",
    "            \n",
    "    return sil_a, sil_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_avg(frameh, dist, k):\n",
    "    sil_a, sil_b = silhouette_individ(frameh, dist, k)\n",
    "    \n",
    "    # find silhouette score of each point in each cluster and take average -> cluster score\n",
    "    sil_scores = dict()\n",
    "    for k,v in sil_a.items():\n",
    "        lst = []\n",
    "        for i in range(len(v.values())):\n",
    "            max_ab = max(list(sil_b[k].values())[i], list(sil_a[k].values())[i])\n",
    "            min_ab = min(list(sil_b[k].values())[i], list(sil_a[k].values())[i])\n",
    "            lst.append(1 - min_ab/max_ab)\n",
    "        sil_scores[k] = np.nanmean(lst) # ignore nans: ex point in all of the clusters, so no b to calculate \n",
    "    # return overall average\n",
    "    return np.nanmean(list(sil_scores.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate Distortion__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortion_calculate(m, centroid, frameh):\n",
    "    sumd = 0\n",
    "    countpts = 0\n",
    "    for i in list(frameh.index.unique()):\n",
    "        for c in frameh[frameh.index == i].cluster.tolist()[0]:\n",
    "            sumd += np.linalg.norm(m[i]-centroid[c])\n",
    "            countpts +=1 \n",
    "        \n",
    "    return sumd, sumd/countpts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # read in data\n",
    "    df = pd.read_pickle('reuters_processed')\n",
    "    \n",
    "    # set of topics\n",
    "    topics = list(df.categories)\n",
    "    topics = [item for sublist in topics for item in sublist]\n",
    "    topics = list(set(topics))\n",
    "\n",
    "    percent_zero_dict = dict()\n",
    "    df_final = pd.DataFrame()\n",
    "    labels_dict = dict()\n",
    "    k_dict = dict()\n",
    "    silhouette_dict = dict()\n",
    "    distortion_dict = dict()\n",
    "    max_score_dict = dict()\n",
    "    cluster1 = []\n",
    "\n",
    "    for search in topics:\n",
    "        gc.collect()\n",
    "        \n",
    "        df_subset = df[df.categories.map(set([search]).issubset)] \n",
    "        df_subset = df_subset.reset_index()\n",
    "        \n",
    "        if len(df_subset) < 5:\n",
    "            cluster1.append(search)\n",
    "            continue\n",
    "            \n",
    "        if search == 'earn' or search == 'acq': # too big to deal with\n",
    "            continue\n",
    "        \n",
    "        print(search)\n",
    "        \n",
    "        # TF-IDF matrix\n",
    "        tfidf, feature_names = tf_idf(df_subset)\n",
    "                \n",
    "        # remove search from tf-idf matrix\n",
    "        tfidf, feature_names = remove_search(tfidf, feature_names, search)\n",
    "\n",
    "        # SVD \n",
    "        U, S, V = svd_calculate(tfidf)\n",
    "        # Find K and reduce dimensionality\n",
    "        roc = roc_var_calculate(S)\n",
    "        k = find_knee(roc) ## alternative is to find_knee(S) directly: size of singular values drops off\n",
    "                            ## but this often results in even more clusters. not a good thing.\n",
    "        V = reduce_V(V, k)\n",
    "\n",
    "        # Find cluster labels\n",
    "        labels, max_w, max_w3, max_w3_score, max_score = find_labels(V, feature_names)\n",
    "        #return labels\n",
    "        # Assign documents to clusters based on labels\n",
    "        docs = find_docs(feature_names, max_w, tfidf)\n",
    "        # combine clusters with overlapping labels\n",
    "        docs, labels, max_score = combine_clusters(labels, max_w3_score, max_w3, max_score, docs, feature_names)\n",
    "\n",
    "        # unlemmatize labels\n",
    "        #labels = labels_unlem(df_subset, labels)\n",
    "        \n",
    "        # sort clusters\n",
    "        labels = label_sort(labels, max_score)\n",
    "        \n",
    "        # mark clusters in dataframe\n",
    "        frame = cluster_df(df_subset, docs)\n",
    "        # percent of documents with no cluster\n",
    "        frame['len']= frame.cluster.apply(lambda row: len(row))\n",
    "        percent_zero_dict[search] = len(frame[frame.len == 0]) / len(frame)\n",
    "\n",
    "        if len(labels) == 1:\n",
    "            cluster1.append(search)\n",
    "            continue\n",
    "        \n",
    "        # calculate metrics\n",
    "        dist = dist_calculate(tfidf)\n",
    "        centroid = find_centroids(frame, tfidf, len(labels))\n",
    "        silhouette = silhouette_avg(frame, dist, len(labels))\n",
    "        distortion, distortion_avg = distortion_calculate(tfidf, centroid, frame)\n",
    "        \n",
    "        labels_dict[search] = labels\n",
    "        k_dict[search] = len(labels) # don't use k because combined clusters after that step\n",
    "        silhouette_dict[search] = silhouette\n",
    "        distortion_dict[search] = distortion_avg\n",
    "        max_score_dict[search] = max_score\n",
    "\n",
    "        frame['search'] = search\n",
    "        df_final = df_final.append(frame)\n",
    "        \n",
    "    return df_final, labels_dict, percent_zero_dict, k_dict, distortion_dict, silhouette_dict, cluster1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final, labels_dict, percent_zero_dict, k_dict, distortion_dict, silhouette_dict, cluster1 = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lingo', \"wb\") as f:\n",
    "    pickle.dump(df_final, f)\n",
    "    pickle.dump(labels_dict, f)\n",
    "    pickle.dump(k_dict, f)\n",
    "    pickle.dump(distortion_dict, f)\n",
    "    pickle.dump(silhouette_dict, f)\n",
    "    pickle.dump(percent_zero_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(percent_zero_dict.values())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(distortion_dict.values())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(silhouette_dict.values())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "silhouette_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
