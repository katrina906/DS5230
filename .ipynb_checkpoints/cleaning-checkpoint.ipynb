{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Preprocess Reuters Corpus\n",
    "- Import data\n",
    "- Remove symbols\n",
    "- Lower case all\n",
    "- Remove punctuation\n",
    "- Remove content specific stop words: financial, dates, units\n",
    "- Collapse countries to acronyms so single token\n",
    "- Remove numbers: financial so many numbers, often surface as labels if not removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#!pip install sklearn\n",
    "#!pip install gensim\n",
    "#!pip install matplotlib\n",
    "#!pip install networkx\n",
    "#!pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gimli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fileids from the reuters corpus\n",
    "fileids = reuters.fileids()\n",
    "\n",
    "# Initialize empty lists to store categories and raw text\n",
    "categories = []\n",
    "text = []\n",
    "\n",
    "# Loop through each file id and collect each file's categories and raw text\n",
    "for file in fileids:\n",
    "    categories.append(reuters.categories(file))\n",
    "    text.append(reuters.raw(file))\n",
    "\n",
    "# Combine lists into pandas dataframe\n",
    "df = pd.DataFrame({'ids':fileids, 'categories':categories, 'text':text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove line breaks, clean symbols\n",
    "df.text = df.text.str.replace('\\n', ' ')\n",
    "df.text = df.text.str.replace('&lt;', '<')\n",
    "df.text = df.text.str.replace(\"&amp;\", \"&\")\n",
    "\n",
    "# down case all\n",
    "df.text = df.text.str.lower()\n",
    "\n",
    "# remove some symbols\n",
    "df.text = df.text.str.replace('<', ' ')\n",
    "df.text = df.text.str.replace('>', ' ')\n",
    "\n",
    "# remove punctuation\n",
    "df.text = df.text.apply(lambda row: row.translate(str.maketrans('','', string.punctuation)))\n",
    "\n",
    "# delete content specific \"stop words\"\n",
    "delete_words = ['qtr', 'pct', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'bil', 'mln',\n",
    "               'quarter', 'percent', 'million', 'billion', 'january', 'february', 'march', 'april', 'may', 'june', 'july', \n",
    "                'august', 'september', 'october', 'november', 'december', 'janurary', 'said', 'year', 'month', 'v', 'iv', 'vi',\n",
    "               'shr', 'cts', 'januarys', 'februarys', 'marchs', 'aprils', 'mays', 'junes', 'julys', 'thous', \n",
    "                'augusts', 'septembers', 'octobers', 'novembers', 'decembers', 'previous', 'prev', 'also', 'say', 'says',\n",
    "               'feet', 'ounces', 'ounce', 'foot', 'ton', 'tons', 'tonnes', 'vs', 'nil', 'pound', 'thou', 'tonne', 'week', 'wk']\n",
    "for w in delete_words:\n",
    "    df.text = df.text.str.replace(' ' + w + ' ', ' ') # word with spaces on either side\n",
    "    df.text = df.text.str.replace(' ' + w + '\\\\.', '.') # word followed by a period\n",
    "\n",
    "# collapse countries to acronyms so recognized as one concept/token \n",
    "df.text = df.text.str.replace('united states', 'us')\n",
    "df.text = df.text.str.replace('new zealand', 'nz')\n",
    "df.text = df.text.str.replace('hong kong', 'hk')\n",
    "df.text = df.text.str.replace('united kingdom', 'uk')\n",
    "\n",
    "# dollars is sometimes written as dlr and sometimes as dollars. Make uniform. \n",
    "df.text = df.text.str.replace('dlrs', 'dollars')\n",
    "\n",
    "# remove all numbers that start a word or have a number before it \n",
    "df.text = df.text.apply(lambda row: re.sub('\\d*', '', row))\n",
    "\n",
    "# final pass on delete words in case any are now surfaced from other deletions (such as numbers)\n",
    "for w in delete_words:\n",
    "    df.text = df.text.str.replace(' ' + w + ' ', ' ')\n",
    "    df.text = df.text.str.replace(' ' + w + '\\\\.', '.')\n",
    "# removing numbers resulted in floating 'th's\n",
    "for w in ['th']:\n",
    "    df.text = df.text.str.replace(' ' + w + ' ', ' ')\n",
    "    df.text = df.text.str.replace(' ' + w + '\\\\.', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle processed data and save locally\n",
    "df.to_pickle('reuters_processed') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
