{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO in general:\n",
    "- Visualization with tsne or umap rather than mds -- tsne works. can't get umap to install.    \n",
    "- If time figure out proximity matrix and do correlation analysis\n",
    "- once do a full run, look at statistic distributions and look at outliers \n",
    "\n",
    "Linkage matrix: ward  \n",
    "because in general clusters are not well separated even once PCA reduced. single, complete, average linkage tend to create one big cluster and then a bunch of clusters with one or two points. Ward gives us more well sized clusters. \n",
    "\n",
    "High run time, but argue ok because small subset of documents clustering within   \n",
    "    \n",
    "Not sure silhouette is right still. sklearn gives something else (lower)    \n",
    "   \n",
    "Have subcluster process available. Give an example, but not doing in depth analysis of them.    \n",
    "    \n",
    "Usually would not choose a universal heuristic to find clusters. But in this case of a search engine, need a universal method so can also scale to other saerch terms. Knee method was the most universal, least subjective method I could think of.    \n",
    "Usually would look for knee in distortion, but distortion smooth in the range we are looking often. Get much better results by looking at roc in that range.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gimli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "import sklearn\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, ward, fcluster\n",
    "import networkx as nx\n",
    "import collections\n",
    "import math\n",
    "import operator\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from kneed import KneeLocator\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.spatial.distance\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchial Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_tokenizer(str_input):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(df):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    stop_lem = [stemming_tokenizer(t) for t in stopwords.words('english')]\n",
    "    stop_lem = [item for sublist in stop_lem for item in sublist]\n",
    "    \n",
    "    # tfidf. stop word removal. word tokenizer. \n",
    "    tfidf = TfidfVectorizer(stop_words = stop_lem, tokenizer = stemming_tokenizer, max_features = 5000)\n",
    "    m = tfidf.fit_transform(df['text'])\n",
    "    \n",
    "    feature_names = tfidf.get_feature_names() # words \n",
    "\n",
    "    return m, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Search from TF-IDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_search(tfidf, feature_names, search):\n",
    "    try: # sometimes search already removed (stop word)\n",
    "        # remove search from the tfidf matrix: do not want as a label or clustering factor\n",
    "        search_index = feature_names.index(search)\n",
    "        cols = list(range(0,len(feature_names)))\n",
    "        del cols[cols.index(search_index)]\n",
    "        tfidf = tfidf[:,cols]\n",
    "        del feature_names[search_index]\n",
    "    except ValueError: \n",
    "        pass\n",
    "    except:\n",
    "        raise 'unknown error'\n",
    "    \n",
    "    return tfidf, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_calculate(m, reduce):\n",
    "    dist = euclidean_distances(m)  ## I think its ok to use euclidean because tf-idf normalizes\n",
    "    if reduce:\n",
    "        flat_dist = scipy.spatial.distance.pdist(m, 'euclidean') # needed for linkage function\n",
    "    else:\n",
    "        flat_dist = scipy.spatial.distance.pdist(m.toarray(), 'euclidean') # needed for linkage function\n",
    "\n",
    "    # euclidean can be innaccurate if documents are different lengths such that vectors are different lengths \n",
    "    # I would prefer to use euclidean because then more sensicl to calculate centroids\n",
    "    # ask professor?? \n",
    "    return dist, flat_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_reduce(m):\n",
    "    pca = PCA(n_components = 0.8) # keep % of variance \n",
    "    pcam = pca.fit_transform(m.toarray())\n",
    "\n",
    "    return pcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linkage Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def linkage_calculate(dist):\n",
    "    linkage_matrix = linkage(dist, method = 'ward') \n",
    "    return linkage_matrix\n",
    "    \n",
    "# plot dendogram\n",
    "#fig, ax = plt.subplots(figsize=(15, 20))\n",
    "#ax = dendrogram(linkage_matrix, orientation=\"right\", labels = df_retail.ids.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose K and Create Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_merge(df, f):\n",
    "    # merge in with original data via pandas\n",
    "    frameh = pd.DataFrame(df.index, index = [f], columns = ['index_search'])\n",
    "    frameh = pd.merge(frameh, df, right_index = True, left_on = 'index_search')\n",
    "    frameh['cluster'] = frameh.index.str[0]\n",
    "    frameh = frameh.reset_index()\n",
    "    return frameh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find Cluster Centroids and Cluster Labels__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_label(frameh, m_pca, m, feature_names, search):\n",
    "    # most common words in clusters (based on tf-idf not just frequency)\n",
    "    centroid = dict()\n",
    "    labels = dict()\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        ## centroid ## \n",
    "        cluster1 = list(frameh[frameh.cluster == c].index.unique())\n",
    "        # find documents cluster\n",
    "        m1_pca = m_pca[cluster1,:]\n",
    "        # take mean vector among all documents\n",
    "        m1_pca = m1_pca.mean(axis = 0)\n",
    "        # record mean vector: centroids of each sub cluster\n",
    "        centroid[c] = m1_pca\n",
    "\n",
    "        ## labels ##\n",
    "        # redo mean vector with non-reduced tfidf matrix \n",
    "        m1 = m[cluster1,:]\n",
    "        # take mean vector among all documents\n",
    "        m1 = m1.mean(axis = 0)\n",
    "        \n",
    "        # max values in mean vector \n",
    "        lst = []\n",
    "\n",
    "        for i in np.argsort(np.asarray(m1)[0])[::-1][:4]:  # 3 words as label - take an extra in case one is the search term\n",
    "            if feature_names[i] == search: # don't record as label if it is the search\n",
    "                continue\n",
    "            lst.append(feature_names[i])\n",
    "            \n",
    "        labels[c] = lst[:3] # 3 word labels\n",
    "        \n",
    "    return labels, centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate Silhouette__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_individ(frameh, dist):\n",
    "    # average distance to points in your cluster\n",
    "    sil_a = dict()\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        sil_a[c] = dict()\n",
    "        docs_i = list(frameh[frameh.cluster == c].index.unique())\n",
    "        if len(docs_i) == 1:\n",
    "            sil_a[c][docs_i[0]] = 0\n",
    "        else:\n",
    "            for i in docs_i:\n",
    "                docs_i.remove(i)\n",
    "                sil_a[c][i] = np.nanmean(dist[i,docs_i].tolist())\n",
    "\n",
    "    # minimum average distance to points in other clusters \n",
    "    sil_b = dict()\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        sil_b[c] = dict()\n",
    "        docs_in = list(frameh[frameh.cluster == c].index.unique())\n",
    "        for i in docs_in:\n",
    "            # loop through other clusters and find average distance \n",
    "            lst = []\n",
    "            for c2 in list(frameh.cluster.unique()):\n",
    "                if c2 != c:\n",
    "                    docs_out = list(frameh[frameh.cluster == c2].index.unique())\n",
    "                    if i in docs_out: # can be in multiple clustesr\n",
    "                        docs_out.remove(i)\n",
    "                    lst.append(np.nanmean(dist[i,docs_out].tolist()))\n",
    "                \n",
    "            # take minimum of average distance to other clusters\n",
    "            sil_b[c][i] = np.min(lst)\n",
    "            \n",
    "    return sil_a, sil_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_avg_calculate(frameh, dist):\n",
    "    sil_a, sil_b = silhouette_individ(frameh, dist)\n",
    "    \n",
    "    # find silhouette score of each point in each cluster and take average -> cluster score\n",
    "    sil_scores = dict()\n",
    "    for k,v in sil_a.items():\n",
    "        lst = []\n",
    "        for i in range(len(v.values())):\n",
    "            max_ab = max(list(sil_b[k].values())[i], list(sil_a[k].values())[i])\n",
    "            min_ab = min(list(sil_b[k].values())[i], list(sil_a[k].values())[i])\n",
    "            lst.append(1 - min_ab/max_ab)\n",
    "        sil_scores[k] = np.nanmean(lst) # ignore nans: ex point in all of the clusters, so no b to calculate \n",
    "        \n",
    "    # return clusters and overall average\n",
    "    return sil_scores, np.nanmean(list(sil_scores.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Clustering__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clusters(k, linkage_matrix, m_pca, m, df, feature_names, search):\n",
    "    f = fcluster(linkage_matrix, k, criterion = 'maxclust')\n",
    "    frameh = frame_merge(df, f)\n",
    "    labels, centroid = centroid_label(frameh, m_pca, m, feature_names, search)\n",
    "\n",
    "    return frameh, labels, centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate Distortion__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortion_calculate(m, centroid, frameh):\n",
    "    sumd = 0\n",
    "    for i in list(frameh.index.unique()):\n",
    "        c = int(frameh[frameh.index == i].cluster)\n",
    "        sumd += (np.linalg.norm(m[i]-centroid[c]))**2\n",
    "        \n",
    "    return sumd, sumd / len(frameh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate Distortion, Silhouette at various k values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def distortion_silhouette(linkage_matrix, m_pca, m, dist, df, feature_names, search):\n",
    "    # distortion - sum of squared errors between points and its centroid \n",
    "    # barely varies with different cluster numbers\n",
    "    distortion = dict()\n",
    "    silhouette = dict()\n",
    "\n",
    "    for k in range(2, min(math.ceil(len(df) / 3), 10)+1): \n",
    "        # max # clusters: 1/3 of documents as long as get on average 10 docs per. Else limit to 1/2 of documents. \n",
    "        # min # clusters: 2 \n",
    "        frameh, labels, centroid = find_clusters(k, linkage_matrix, m_pca, m, df, feature_names, search)\n",
    "\n",
    "        # calculate silhouette \n",
    "        x, silhouette[k] = silhouette_avg_calculate(frameh, dist)\n",
    "\n",
    "        # calculate distortion\n",
    "        sumd, sumd_avg = distortion_calculate(m_pca, centroid, frameh)\n",
    "        # take average \n",
    "        distortion[k] = sumd\n",
    "        \n",
    "    return distortion, silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc = distortion_roc(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retail\n"
     ]
    }
   ],
   "source": [
    "d = main(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortion_roc(distortion):\n",
    "    # relative rate of change \n",
    "    roc = []\n",
    "    for k,v in distortion.items(): \n",
    "        if k+1 in distortion:\n",
    "            roc.append(abs(distortion[k+1] - distortion[k]) / distortion[k])\n",
    "            \n",
    "    return roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find K Based on Distortion ROC Elbow__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k(roc):\n",
    "    if len(roc) == 1:\n",
    "        return 2 # k = 2 to k = 3 --> k = 2 \n",
    "    \n",
    "    # find k using knee method \n",
    "    kn = KneeLocator(range(len(roc)), roc, curve='convex', direction='decreasing')\n",
    "    if kn.knee == None: # sometimes there is no knee, just take the max k in that case\n",
    "        return len(roc) + 1 # starts at 2. If length = 1, then that's k = 2 to k = 3 --> k = 2 \n",
    "    k = kn.knee + 2 # index started at 0 and k starts at 2 \n",
    "                    # if choose index 1 roc that is the second value and going from k = 3 to k = 4 --> k = 3 \n",
    "    \n",
    "    return k "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot distortion\n",
    "#fig, ax = plt.subplots()\n",
    "\n",
    "#distortion = sorted(distortion.items()) # sorted by key, return a list of tuples\n",
    "#x, y = zip(*distortion) # unpack a list of pairs into two tuples\n",
    "#ax.plot(x,y)\n",
    "#ax.axvline(k, color = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-72929a38ac8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'black'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'k' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe468b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(roc)\n",
    "ax.axvline(k-1, color = 'black')\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 14.453905895626464),\n",
       " (3, 13.206722330088137),\n",
       " (4, 12.009577075331766),\n",
       " (5, 10.873993958161034),\n",
       " (6, 9.77460343122789),\n",
       " (7, 8.696556908556573),\n",
       " (8, 7.679039395421721),\n",
       " (9, 6.763675950365539)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn.knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gimli\\Anaconda3\\lib\\site-packages\\kneed\\knee_locator.py:188: UserWarning: No knee/elbow found\n",
      "  warnings.warn(\"No knee/elbow found\")\n"
     ]
    }
   ],
   "source": [
    "kn = KneeLocator(range(len(d)), list(d.values()), curve='convex', direction='decreasing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gimli\\Anaconda3\\lib\\site-packages\\kneed\\knee_locator.py:134: RuntimeWarning: No local maxima found in the difference curve\n",
      "The line is probably not polynomial, try plotting\n",
      "the difference curve with plt.plot(knee.x_difference, knee.y_difference)\n",
      "Also check that you aren't mistakenly setting the curve argument\n",
      "  RuntimeWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_k(roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Un-Lemmatize Labels__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_unlem_create(df, labels, t):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # tfidf without lemitization\n",
    "    tfidf = TfidfVectorizer(stop_words = stopwords.words('english'))\n",
    "    m_norm = tfidf.fit_transform(df['text'])\n",
    "    words = tfidf.get_feature_names()\n",
    "\n",
    "    # dataframe that records words and their lemitized versions\n",
    "    aux = pd.DataFrame(words, columns =['word'] )\n",
    "    aux['word_stemmed'] = aux['word'].apply(lambda x : stemmer.stem(x))\n",
    "    \n",
    "    # count the number of words in the corpus \n",
    "    vec = sklearn.feature_extraction.text.CountVectorizer().fit(df['text'])\n",
    "    bag_of_words = vec.transform(df['text'])\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = pd.DataFrame(words_freq)\n",
    "    words_freq.columns = ['word', 'num']\n",
    "    \n",
    "    # merge with aux and sort such that when take first value, will be most frequent word in corpus\n",
    "    aux = pd.merge(aux, words_freq, on = 'word', how = 'left')\n",
    "    aux = aux.sort_values(['word_stemmed', 'num'], ascending = False)\n",
    "    \n",
    "    if t == 'flat':\n",
    "        # loop through returned labels and grab the first instance of the un-lemmatized word (just any version will do)\n",
    "        labels_unlem = dict()\n",
    "        for i in labels.keys():\n",
    "            labels_unlem[i] = []\n",
    "            for j in labels[i]:\n",
    "                if len(aux[aux.word_stemmed == j]) == 0:\n",
    "                    labels_unlem[i].append(j)\n",
    "                    continue\n",
    "                labels_unlem[i].append(aux[aux.word_stemmed == j].word.values[0])\n",
    "            \n",
    "    if t == 'sub': \n",
    "        labels_unlem = dict()\n",
    "        for i in labels.keys():\n",
    "            labels_unlem[i] = []\n",
    "            ct = 0\n",
    "            for j in labels[i]:\n",
    "                labels_unlem[i].append([])\n",
    "                for k in j:\n",
    "                    if len(aux[aux.word_stemmed == k]) == 0:\n",
    "                        labels_unlem[i][ct].append(k)\n",
    "                        continue\n",
    "                    labels_unlem[i][ct].append(aux[aux.word_stemmed == k].word.values[0])\n",
    "                ct += 1\n",
    "            \n",
    "    return labels_unlem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sort Labels based on Silhouette Scores of Clusters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sort(labels, max_score):\n",
    "    max_score = sorted(max_score.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    \n",
    "    labels_sorted = dict()\n",
    "    for i in max_score:\n",
    "        labels_sorted[i[0]] = labels[i[0]]\n",
    "        \n",
    "    return labels_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchy: Sub-Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linkage Matrix: Understand Node Linkages__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkage_df(linkage_matrix, frameh):\n",
    "    # in linkage matrix, indicate the aggregated node for each node pair\n",
    "    links = pd.DataFrame(linkage_matrix) # using euclidean \n",
    "    links.columns = ['source1', 'source2', 'd', 'n']\n",
    "\n",
    "    links['target'] = 0\n",
    "    n = len(frameh)-1\n",
    "    for i, row in links.iterrows():\n",
    "        n += 1\n",
    "        links.at[i,'target'] = n\n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten irregular nested lists\n",
    "def flatten(l):\n",
    "    for el in l:\n",
    "        if isinstance(el, collections.Iterable) and not isinstance(el, (str, bytes)):\n",
    "            yield from flatten(el)\n",
    "        else:\n",
    "            yield el"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find Documents at Various Sub-Clusters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_docs(merge, source):\n",
    "    merge = pd.merge(merge, merge[['target', 'docs']], left_on = source, right_on = 'target',  how = 'left')\n",
    "    merge.docs_x = np.where(merge.docs_x.isnull(), '', merge.docs_x)\n",
    "    merge.docs_y = np.where(merge.docs_y.isnull(), '', merge.docs_y)\n",
    "    merge['docs'] = merge[['docs_x', 'docs_y']].values.tolist()\n",
    "    merge = merge.drop(columns = ['docs_x', 'docs_y'])\n",
    "    #merge.docs = merge.docs.apply(np.ravel)\n",
    "    merge = merge.rename(columns = {'target_x':'target'})\n",
    "    merge.docs = list(merge.docs.apply(lambda row: flatten(row)))\n",
    "    merge.docs = merge.docs.apply(lambda row: [i for i in row if i != ''])\n",
    "\n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def assign_docs(frameh, links):\n",
    "    # initial merge between frame ids and source1/source2\n",
    "    merge = pd.merge(links, frameh[['ids']], left_on = 'source1', right_index = True, how = 'left')\n",
    "    merge = pd.merge(merge, frameh[['ids']], left_on = 'source2', right_index = True, how = 'left')\n",
    "    # create single docs list column \n",
    "    merge = merge.rename(columns = {'ids_x':'docs1', 'ids_y':'docs2'})\n",
    "    merge.docs2 = np.where(merge.docs2.isnull(), '', merge.docs2)\n",
    "    merge.docs1 = np.where(merge.docs1.isnull(), '', merge.docs1)\n",
    "    merge['docs']= merge[['docs1', 'docs2']].values.tolist()\n",
    "    merge = merge.drop(columns = ['docs1', 'docs2'])\n",
    "    # flattern docs list column\n",
    "    merge.docs = merge.docs.apply(lambda row: [i for i in row if i != ''])\n",
    "    merge['len'] = merge.docs.apply(lambda row: len(set(row)))\n",
    "\n",
    "    # loop until have one id per document at node (n)\n",
    "    while int(merge[merge.target == merge.target.max()].len) != int(merge[merge.target == merge.target.max()].n): \n",
    "        print(merge[merge.target == merge.target.max()].len)\n",
    "        merge = merge_docs(merge, 'source1')\n",
    "        merge = merge_docs(merge, 'source2')\n",
    "        merge['len'] = merge.docs.apply(lambda row: len(set(row)))\n",
    "        \n",
    "        merge = merge.drop(columns = ['target_y'])\n",
    "\n",
    "    merge.docs = merge.docs.apply(lambda row: set(row))\n",
    "    \n",
    "    return merge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Assign Graph Attributes: Docs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def attributes(G, merge, frameh, links):\n",
    "    # add docs to each target node so know which docs exist at each target \n",
    "    for i in list(merge.target.unique()):\n",
    "        G.nodes[i]['docs'] = merge[merge.target == i].docs.values[0]\n",
    "\n",
    "    # add docs to origianl nodes as well \n",
    "    ogdocs = pd.merge(links, frameh[['ids']], left_on = 'source1', right_index = True, how = 'left')\n",
    "    ogdocs = pd.merge(ogdocs, frameh[['ids']], left_on = 'source2', right_index = True, how = 'left')\n",
    "    for i in range(len(frameh)):\n",
    "        if len(ogdocs[ogdocs.source1 == i].ids_x) != 0:\n",
    "            G.nodes[i]['docs'] = ogdocs[ogdocs.source1 == i].ids_x.values[0]\n",
    "        if len(ogdocs[ogdocs.source2 == i].ids_y) != 0:\n",
    "            G.nodes[i]['docs'] = ogdocs[ogdocs.source2 == i].ids_y.values[0]\n",
    "            \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mark Top Level Clusters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_cluster(G, frameh):\n",
    "    # find top level clusters and mark as such - determined by fcluster above \n",
    "    nx.set_node_attributes(G, 0, 'cluster')\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        try:\n",
    "            c_ids = set(frameh[frameh.cluster == c].ids.unique())\n",
    "            node = [x for x,y in G.nodes(data=True) if y['docs']==c_ids][0] # equals ALL of these ids \n",
    "        except:\n",
    "            c_ids = frameh[frameh.cluster == c].ids.unique()\n",
    "            node = [x for x,y in G.nodes(data=True) if y['docs']==c_ids][0]\n",
    "        G.nodes[node]['cluster'] = c\n",
    "        \n",
    "    return G "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Labels for all Sub Clusters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_labels(links, m, frameh):\n",
    "    labels_sub = []\n",
    "    for i in list(range(links.target.max())):\n",
    "        if i < len(frameh):\n",
    "            cluster1 = list(frameh[frameh.ids == list(G.nodes[i].values())[0]].index.unique())\n",
    "        else:\n",
    "            cluster1 = list(frameh[frameh.ids.isin(list(list(G.nodes[i].values())[0]))].index.unique())\n",
    "        # find documents cluster\n",
    "        m1 = m[cluster1,:]\n",
    "        # take mean vector among all documents\n",
    "        m1 = m1.mean(axis = 0)\n",
    "\n",
    "        # max values in mean vector: labels\n",
    "        lst = []\n",
    "        for i in np.argsort(np.asarray(m1)[0])[::-1][:3]:\n",
    "            lst.append(feature_names[i])\n",
    "\n",
    "        labels_sub.append(lst)\n",
    "    \n",
    "    return labels_sub        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create Hierarchy Graph__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(links, merge, frameh):\n",
    "    # add nodes\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(links.source1)\n",
    "    G.add_nodes_from(links.source2)\n",
    "    G.add_nodes_from(links.target)\n",
    "\n",
    "    # add edges\n",
    "    subset = links[['source1', 'target']]\n",
    "    G.add_edges_from([tuple(x) for x in subset.values])\n",
    "    subset = links[['source2', 'target']]\n",
    "    G.add_edges_from([tuple(x) for x in subset.values])\n",
    "    \n",
    "    # add attributes\n",
    "    G = attributes(G, merge, frameh, links)\n",
    "    \n",
    "    # mark top clusters: attributes\n",
    "    G = top_cluster(G, frameh)\n",
    "    \n",
    "    return G "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hierarchy(linkage_matrix, frameh):\n",
    "    \n",
    "    links = linkage_df(linkage_matrix, frameh)\n",
    "    merge = assign_docs(frameh, links)\n",
    "    G = create_graph(links, merge, frameh)\n",
    "                     \n",
    "    return G, links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Labels for all Sub Clusters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_labels(links, m, frameh, G, feature_names):\n",
    "    labels_sub = []\n",
    "    for i in list(range(links.target.max())):\n",
    "        if i < len(frameh):\n",
    "            cluster1 = list(frameh[frameh.ids == list(G.nodes[i].values())[0]].index.unique())\n",
    "        else:\n",
    "            cluster1 = list(frameh[frameh.ids.isin(list(list(G.nodes[i].values())[0]))].index.unique())\n",
    "        # find documents cluster\n",
    "        m1 = m[cluster1,:]\n",
    "        # take mean vector among all documents\n",
    "        m1 = m1.mean(axis = 0)\n",
    "\n",
    "        # max values in mean vector: labels\n",
    "        lst = []\n",
    "        for i in np.argsort(np.asarray(m1)[0])[::-1][:3]:\n",
    "            lst.append(feature_names[i])\n",
    "\n",
    "        labels_sub.append(lst)\n",
    "    \n",
    "    return labels_sub        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels for subclusters 1 layer down from top flat clusters\n",
    "def labels_layerdown(G, frameh, labels_sub):\n",
    "    # identify topnodes \n",
    "    topnodes = []\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        topnodes.append([x for x,y in G.nodes(data=True) if y['cluster']==c][0])\n",
    "        \n",
    "    # find subcluster labels: 1 level down from topnodes \n",
    "    labels1 = dict()\n",
    "    for i in range(len(topnodes)):\n",
    "        sub = list(G.predecessors(topnodes[i]))\n",
    "        labels1[i] = [labels_sub[int(j)] for j in sub]\n",
    "        \n",
    "    return labels1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Main Hiearchy Function: Get Labels of all sub-clusters 1 down from top__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hierarchy_main(linkage_matrix, frameh, tfidf_unreduced, df_subset, feature_names):\n",
    "    G, links = create_hierarchy(linkage_matrix, frameh)\n",
    "    labels_sub = sub_labels(links, tfidf_unreduced, frameh, G, feature_names)\n",
    "    labels1 = labels_layerdown(G, frameh, labels_sub)\n",
    "    labels1 = labels_unlem_create(df_subset, labels1, t = 'sub')\n",
    "    \n",
    "    return labels1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dendogram with manually drawn in cut line for tin  \n",
    "#fig, ax = plt.subplots(figsize=(15, 20))\n",
    "#ax = dendrogram(linkage_matrix, orientation=\"right\", labels = frameh.ids.unique())\n",
    "#plt.axvline(1.69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retail\n"
     ]
    }
   ],
   "source": [
    "d = main(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(reduce, hierarchy = False, hsearch = False):\n",
    "    \n",
    "    # read in data\n",
    "    df = pd.read_pickle('reuters_processed')\n",
    "    \n",
    "    # set of topics\n",
    "    topics = list(df.categories)\n",
    "    topics = [item for sublist in topics for item in sublist]\n",
    "    topics = list(set(topics))\n",
    "    \n",
    "    distortion_dict = dict()\n",
    "    silhouette_dict = dict()\n",
    "    k_dict = dict()\n",
    "    labels_dict = dict()\n",
    "    dist_dict = dict()\n",
    "    cluster1 = list()\n",
    "    \n",
    "    df_final = pd.DataFrame()\n",
    "    \n",
    "    if hierarchy: \n",
    "        topics = [hsearch]\n",
    "    \n",
    "    for search in topics:\n",
    "        \n",
    "        if search == 'earn' or search == 'acq': # too big to deal with\n",
    "            continue\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "        df_subset = df[df.categories.map(set([search]).issubset)] \n",
    "        df_subset = df_subset.reset_index()\n",
    "        \n",
    "        if len(df_subset) < 5:\n",
    "            cluster1.append(search)\n",
    "            continue\n",
    "            \n",
    "        print(search)\n",
    "        \n",
    "        # TF-IDF matrix\n",
    "        tfidf, feature_names = tf_idf(df_subset)\n",
    "\n",
    "        # remove search from tf-idf matrix\n",
    "        tfidf, feature_names = remove_search(tfidf, feature_names, search)\n",
    "\n",
    "        # PCA dimensionality reduction\n",
    "        if reduce:\n",
    "            tfidf_unreduced = tfidf.copy()\n",
    "            tfidf = pca_reduce(tfidf)\n",
    "                    \n",
    "        # distances \n",
    "        dist, dist_flat = dist_calculate(tfidf, reduce)\n",
    "                \n",
    "        # linkage matrix\n",
    "        linkage_matrix = linkage_calculate(dist_flat)\n",
    "        \n",
    "        # find K \n",
    "        if reduce: \n",
    "            #return linkage_matrix, tfidf, tfidf_unreduced, dist, df_subset, feature_names, search\n",
    "            if len(df_subset) < 7: # else searching for a k is useless. only reasonable number is 2. \n",
    "                k = 2\n",
    "            else:\n",
    "                # use non-reduced tfidf to find labels. reduced for everything else. \n",
    "                distortion_lst, silhouette_lst = distortion_silhouette(linkage_matrix, tfidf, tfidf_unreduced, dist, df_subset,\n",
    "                                                                       feature_names, search)\n",
    "                \n",
    "                roc = distortion_roc(distortion_lst)\n",
    "                k = find_k(roc)\n",
    "            \n",
    "            # final flat clusters\n",
    "            frameh, labels, centroid = find_clusters(k, linkage_matrix, tfidf, tfidf_unreduced, \n",
    "                                                     df_subset, feature_names, search)\n",
    "            \n",
    "            if frameh.cluster.nunique() == 1:   \n",
    "                cluster1.append(search)\n",
    "                continue\n",
    "                \n",
    "            distortion, distortion_avg = distortion_calculate(tfidf, centroid, frameh)\n",
    "            silhouette_cluster, silhouette_avg = silhouette_avg_calculate(frameh, dist)\n",
    "            \n",
    "        else:\n",
    "            # pass tfidf in for both reduced and unreduced arguments\n",
    "            distortion_lst, silhouette_lst = distortion_silhouette(linkage_matrix, tfidf, tfidf, dist, df_subset, \n",
    "                                                                   feature_names, search)\n",
    "            roc = distortion_roc(distortion_lst)\n",
    "            k = find_k(roc)\n",
    "\n",
    "            # final flat clusters\n",
    "            frameh, labels, centroid = find_clusters(k, linkage_matrix, tfidf, tfidf, \n",
    "                                                     df_subset, feature_names, search)\n",
    "            if frameh.cluster.nunique() == 1:   \n",
    "                cluster1.append(search)\n",
    "                continue\n",
    "                \n",
    "            distortion, distortion_avg = distortion_calculate(tfidf, centroid, frameh)\n",
    "            silhouette_cluster, silhouette_avg = silhouette_avg_calculate(frameh, dist)\n",
    "            \n",
    "        # un-lemmatize labels\n",
    "        labels = labels_unlem_create(df_subset, labels, t = 'flat')\n",
    "        \n",
    "        # sort labels based on cluster silhouette score\n",
    "        labels = label_sort(labels, silhouette_cluster)\n",
    "\n",
    "        # dictionary record \n",
    "        distortion_dict[search] = distortion_avg\n",
    "        silhouette_dict[search] = silhouette_avg\n",
    "        k_dict[search] = k\n",
    "        labels_dict[search] = labels\n",
    "        dist_dict[search] = dist\n",
    "        \n",
    "        frameh['search'] = search\n",
    "        df_final = df_final.append(frameh)\n",
    "        \n",
    "        if hierarchy: \n",
    "            labels1 = hierarchy_main(linkage_matrix, frameh, tfidf_unreduced, df_subset, feature_names)\n",
    "            return labels1\n",
    "\n",
    "    return distortion_dict, silhouette_dict, k_dict, labels_dict, df_final, dist_dict, cluster1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#labels1 = main(reduce = True, hierarchy = True, hsearch = 'tin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wheat\n",
      "barley\n",
      "naphtha\n",
      "crude\n",
      "alum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gimli\\Anaconda3\\lib\\site-packages\\kneed\\knee_locator.py:134: RuntimeWarning: No local maxima found in the difference curve\n",
      "The line is probably not polynomial, try plotting\n",
      "the difference curve with plt.plot(knee.x_difference, knee.y_difference)\n",
      "Also check that you aren't mistakenly setting the curve argument\n",
      "  RuntimeWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money-supply\n",
      "bop\n",
      "housing\n",
      "gnp\n",
      "propane\n",
      "palm-oil\n",
      "ipi\n",
      "sorghum\n",
      "coconut-oil\n",
      "tea\n",
      "retail\n",
      "jobs\n",
      "sugar\n",
      "l-cattle\n",
      "rice\n",
      "soy-meal\n",
      "oat\n",
      "trade\n",
      "lei\n",
      "veg-oil\n",
      "livestock\n",
      "dlr\n",
      "instal-debt\n",
      "platinum\n",
      "pet-chem\n",
      "ship\n",
      "income\n",
      "potato\n",
      "groundnut\n",
      "strategic-metal\n",
      "yen\n",
      "cpi\n",
      "cocoa\n",
      "zinc\n",
      "sun-oil\n",
      "reserves\n",
      "interest\n",
      "rapeseed\n",
      "oilseed\n",
      "hog\n",
      "money-fx\n",
      "orange\n",
      "corn\n",
      "rape-oil\n",
      "cotton\n",
      "coconut\n",
      "heat\n",
      "jet\n",
      "wpi\n",
      "lumber\n",
      "copper\n",
      "nickel\n",
      "fuel\n",
      "coffee\n",
      "tin\n",
      "soy-oil\n",
      "gold\n",
      "gas\n",
      "rubber\n",
      "meal-feed\n",
      "nat-gas\n",
      "silver\n",
      "carcass\n",
      "grain\n",
      "sunseed\n",
      "soybean\n",
      "iron-steel\n",
      "lead\n",
      "dmk\n"
     ]
    }
   ],
   "source": [
    "distortion_dict, silhouette_dict, k_dict, labels_dict, df_final, dist_dict, cluster1 = main(reduce = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hierarchial', \"wb\") as f:\n",
    "    pickle.dump(df_final, f)\n",
    "    pickle.dump(labels_dict, f)\n",
    "    pickle.dump(k_dict, f)\n",
    "    pickle.dump(distortion_dict, f)\n",
    "    pickle.dump(silhouette_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alum': 9,\n",
       " 'barley': 5,\n",
       " 'bop': 3,\n",
       " 'carcass': 4,\n",
       " 'cocoa': 3,\n",
       " 'coconut': 2,\n",
       " 'coconut-oil': 2,\n",
       " 'coffee': 3,\n",
       " 'copper': 3,\n",
       " 'corn': 4,\n",
       " 'cotton': 3,\n",
       " 'cpi': 3,\n",
       " 'crude': 6,\n",
       " 'dlr': 6,\n",
       " 'dmk': 4,\n",
       " 'fuel': 3,\n",
       " 'gas': 3,\n",
       " 'gnp': 5,\n",
       " 'gold': 5,\n",
       " 'grain': 4,\n",
       " 'groundnut': 2,\n",
       " 'heat': 6,\n",
       " 'hog': 7,\n",
       " 'housing': 3,\n",
       " 'income': 5,\n",
       " 'instal-debt': 2,\n",
       " 'interest': 4,\n",
       " 'ipi': 9,\n",
       " 'iron-steel': 3,\n",
       " 'jet': 2,\n",
       " 'jobs': 5,\n",
       " 'l-cattle': 2,\n",
       " 'lead': 4,\n",
       " 'lei': 4,\n",
       " 'livestock': 4,\n",
       " 'lumber': 3,\n",
       " 'meal-feed': 3,\n",
       " 'money-fx': 4,\n",
       " 'money-supply': 5,\n",
       " 'naphtha': 2,\n",
       " 'nat-gas': 3,\n",
       " 'nickel': 2,\n",
       " 'oat': 4,\n",
       " 'oilseed': 3,\n",
       " 'orange': 4,\n",
       " 'palm-oil': 6,\n",
       " 'pet-chem': 3,\n",
       " 'platinum': 3,\n",
       " 'potato': 2,\n",
       " 'propane': 2,\n",
       " 'rape-oil': 2,\n",
       " 'rapeseed': 5,\n",
       " 'reserves': 3,\n",
       " 'retail': 8,\n",
       " 'rice': 8,\n",
       " 'rubber': 6,\n",
       " 'ship': 5,\n",
       " 'silver': 6,\n",
       " 'sorghum': 5,\n",
       " 'soy-meal': 4,\n",
       " 'soy-oil': 8,\n",
       " 'soybean': 6,\n",
       " 'strategic-metal': 3,\n",
       " 'sugar': 6,\n",
       " 'sun-oil': 2,\n",
       " 'sunseed': 5,\n",
       " 'tea': 4,\n",
       " 'tin': 3,\n",
       " 'trade': 5,\n",
       " 'veg-oil': 4,\n",
       " 'wheat': 6,\n",
       " 'wpi': 3,\n",
       " 'yen': 3,\n",
       " 'zinc': 5}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alum': {1: ['credit', 'azpurua', 'financing'],\n",
       "  2: ['prices', 'lb', 'cents'],\n",
       "  3: ['alcan', 'close', 'smelter'],\n",
       "  4: ['british', 'vessel', 'orinoco'],\n",
       "  5: ['suralco', 'billiton', 'bauxite'],\n",
       "  6: ['reynolds', 'restart', 'metal'],\n",
       "  7: ['ipai', 'ave', 'daily'],\n",
       "  8: ['lme', 'contract', 'grade'],\n",
       "  9: ['aluminium', 'plant', 'smelter']},\n",
       " 'barley': {1: ['ecus', 'licences', 'ec'],\n",
       "  2: ['bonus', 'ccc', 'accepted'],\n",
       "  3: ['wheat', 'uk', 'export'],\n",
       "  4: ['reserve', 'v', 'iv'],\n",
       "  5: ['crops', 'usda', 'acres']},\n",
       " 'bop': {1: ['dollars', 'deficit', 'surplus'],\n",
       "  2: ['lire', 'italy', 'showed'],\n",
       "  3: ['surplus', 'current', 'deficit']},\n",
       " 'carcass': {1: ['plant', 'union', 'iowa'],\n",
       "  2: ['japan', 'beef', 'imports'],\n",
       "  3: ['ec', 'meat', 'us'],\n",
       "  4: ['meat', 'pork', 'poultry']},\n",
       " 'cocoa': {1: ['buffer', 'stock', 'delegates'],\n",
       "  2: ['season', 'purchases', 'board'],\n",
       "  3: ['official', 'icco', 'stock']},\n",
       " 'coconut': {1: ['products', 'exports', 'ec'],\n",
       "  2: ['agency', 'military', 'administrator']},\n",
       " 'coconut-oil': {1: ['saturated', 'oil', 'tropical'],\n",
       "  2: ['oil', 'trade', 'ec']},\n",
       " 'coffee': {1: ['quotas', 'meeting', 'ico'],\n",
       "  2: ['sao', 'mm', 'rainfall'],\n",
       "  3: ['coffee', 'export', 'bags']},\n",
       " 'copper': {1: ['cents', 'price', 'magma'],\n",
       "  2: ['noranda', 'fire', 'mine'],\n",
       "  3: ['production', 'dollars', 'mine']},\n",
       " 'corn': {1: ['inspections', 'thous', 'bushels'],\n",
       "  2: ['sold', 'unknown', 'report'],\n",
       "  3: ['ecus', 'rebate', 'maize'],\n",
       "  4: ['us', 'grain', 'imports']},\n",
       " 'cotton': {1: ['bales', 'stocks', 'last'],\n",
       "  2: ['adjusted', 'location', 'world'],\n",
       "  3: ['dollars', 'us', 'planted']},\n",
       " 'cpi': {1: ['consumer', 'index', 'prices'],\n",
       "  2: ['cost', 'living', 'fell'],\n",
       "  3: ['inflation', 'prices', 'rise']},\n",
       " 'crude': {1: ['postings', 'raised', 'wti'],\n",
       "  2: ['stocks', 'distillate', 'gasoline'],\n",
       "  3: ['attack', 'iranian', 'gulf'],\n",
       "  4: ['oil', 'dollars', 'gas'],\n",
       "  5: ['ecuador', 'pipeline', 'exports'],\n",
       "  6: ['opec', 'saudi', 'prices']},\n",
       " 'dlr': {1: ['opened', 'tokyo', 'yen'],\n",
       "  2: ['bank', 'dealers', 'dollar'],\n",
       "  3: ['set', 'liradollar', 'turkish'],\n",
       "  4: ['fed', 'johnson', 'dollar'],\n",
       "  5: ['dollar', 'us', 'rates'],\n",
       "  6: ['miyazawa', 'sumita', 'ministry']},\n",
       " 'dmk': {1: ['set', 'rate', 'liradollar'],\n",
       "  2: ['bundesbank', 'dollar', 'fixed'],\n",
       "  3: ['rate', 'bank', 'would'],\n",
       "  4: ['warrants', 'weightings', 'crown']},\n",
       " 'fuel': {1: ['sulphur', 'dollars', 'prices'],\n",
       "  2: ['budget', 'tax', 'government'],\n",
       "  3: ['oil', 'barrels', 'stocks']},\n",
       " 'gas': {1: ['prices', 'octane', 'petrol'],\n",
       "  2: ['bpd', 'demand', 'eia'],\n",
       "  3: ['stocks', 'distillate', 'bbls']},\n",
       " 'gnp': {1: ['gdp', 'canada', 'fourth'],\n",
       "  2: ['german', 'west', 'institute'],\n",
       "  3: ['japan', 'official', 'octoberdecember'],\n",
       "  4: ['oecd', 'growth', 'domestic'],\n",
       "  5: ['growth', 'bank', 'government']},\n",
       " 'gold': {1: ['coins', 'issue', 'mint'],\n",
       "  2: ['warrants', 'francs', 'issue'],\n",
       "  3: ['price', 'dollars', 'market'],\n",
       "  4: ['workers', 'mine', 'south'],\n",
       "  5: ['mine', 'per', 'reserves']},\n",
       " 'grain': {1: ['licences', 'ecus', 'rebate'],\n",
       "  2: ['corn', 'export', 'sold'],\n",
       "  3: ['credit', 'ccc', 'bonus'],\n",
       "  4: ['wheat', 'us', 'corn']},\n",
       " 'groundnut': {1: ['us', 'ldp', 'export'], 2: ['ascs', 'oilseed', 'price']},\n",
       " 'heat': {1: ['rise', 'rose', 'department'],\n",
       "  2: ['dollars', 'sulphur', 'posted'],\n",
       "  3: ['futures', 'crude', 'traders'],\n",
       "  4: ['exchange', 'apex', 'trading'],\n",
       "  5: ['sun', 'barge', 'price'],\n",
       "  6: ['exxon', 'price', 'barge']},\n",
       " 'hog': {1: ['ago', 'guesstimated', 'slaughter'],\n",
       "  2: ['nppc', 'pork', 'program'],\n",
       "  3: ['senate', 'pork', 'trade'],\n",
       "  4: ['danish', 'canadian', 'pig'],\n",
       "  5: ['futures', 'contracts', 'stg'],\n",
       "  6: ['dekalb', 'market', 'weight'],\n",
       "  7: ['actual', 'plant', 'ufcwu']},\n",
       " 'housing': {1: ['sales', 'homes', 'dollars'],\n",
       "  2: ['units', 'completions', 'fell'],\n",
       "  3: ['starts', 'units', 'permits']},\n",
       " 'income': {1: ['uk', 'underlying', 'official'],\n",
       "  2: ['spending', 'personal', 'income'],\n",
       "  3: ['dollars', 'income', 'department'],\n",
       "  4: ['earnings', 'real', 'labor'],\n",
       "  5: ['growth', 'bank', 'market']},\n",
       " 'instal-debt': {1: ['dollars', 'credit', 'consumer'],\n",
       "  2: ['advanced', 'stg', 'months']},\n",
       " 'interest': {1: ['fed', 'reserves', 'repurchase'],\n",
       "  2: ['stg', 'band', 'bills'],\n",
       "  3: ['days', 'shortterm', 'maturity'],\n",
       "  4: ['rate', 'bank', 'cut']},\n",
       " 'ipi': {1: ['french', 'insee', 'fell'],\n",
       "  2: ['index', 'production', 'industrial'],\n",
       "  3: ['uk', 'manufacturing', 'official'],\n",
       "  4: ['normal', 'cbi', 'survey'],\n",
       "  5: ['canada', 'prices', 'canadian'],\n",
       "  6: ['endjanuary', 'months', 'brazilian'],\n",
       "  7: ['planned', 'soviet', 'talyzin'],\n",
       "  8: ['us', 'gain', 'fed'],\n",
       "  9: ['industrial', 'production', 'output']},\n",
       " 'iron-steel': {1: ['prices', 'increase', 'plate'],\n",
       "  2: ['ec', 'ministers', 'quota'],\n",
       "  3: ['steel', 'us', 'exports']},\n",
       " 'jet': {1: ['contract', 'defense', 'dlr'],\n",
       "  2: ['petroleum', 'barrels', 'oil']},\n",
       " 'jobs': {1: ['benefits', 'end', 'applications'],\n",
       "  2: ['growth', 'forecast', 'oecd'],\n",
       "  3: ['japan', 'record', 'unemployment'],\n",
       "  4: ['unemployment', 'workforce', 'number'],\n",
       "  5: ['unemployment', 'adjusted', 'uk']},\n",
       " 'l-cattle': {1: ['us', 'states', 'eradicated'],\n",
       "  2: ['cattle', 'market', 'prices']},\n",
       " 'lead': {1: ['trail', 'locals', 'smelter'],\n",
       "  2: ['cent', 'asarco', 'price'],\n",
       "  3: ['cominco', 'metal', 'per'],\n",
       "  4: ['zinc', 'mine', 'gold']},\n",
       " 'lei': {1: ['canada', 'official', 'gain'],\n",
       "  2: ['koreas', 'south', 'figures'],\n",
       "  3: ['index', 'growth', 'rise'],\n",
       "  4: ['revised', 'us', 'indicators']},\n",
       " 'livestock': {1: ['ago', 'guesstimated', 'slaughter'],\n",
       "  2: ['bonus', 'ccc', 'cattle'],\n",
       "  3: ['plant', 'union', 'iowa'],\n",
       "  4: ['us', 'beef', 'meat']},\n",
       " 'lumber': {1: ['credit', 'guarantees', 'plywood'],\n",
       "  2: ['charge', 'canada', 'paying'],\n",
       "  3: ['trading', 'forest', 'canadian']},\n",
       " 'meal-feed': {1: ['guarantees', 'credit', 'sales'],\n",
       "  2: ['sales', 'season', 'fourweek'],\n",
       "  3: ['soybean', 'imports', 'exports']},\n",
       " 'money-fx': {1: ['stg', 'money', 'bank'],\n",
       "  2: ['fed', 'repurchase', 'reserves'],\n",
       "  3: ['dollar', 'dealers', 'yen'],\n",
       "  4: ['rates', 'dollar', 'bank']},\n",
       " 'money-supply': {1: ['feb', 'zero', 'discountwindow'],\n",
       "  2: ['bank', 'money', 'rise'],\n",
       "  3: ['loans', 'business', 'dollars'],\n",
       "  4: ['canadian', 'dollars', 'canada'],\n",
       "  5: ['dollars', 'us', 'fed']},\n",
       " 'naphtha': {1: ['pakistan', 'kellogg', 'refinery'],\n",
       "  2: ['raises', 'prices', 'oil']},\n",
       " 'nat-gas': {1: ['reserves', 'barrels', 'oil'],\n",
       "  2: ['gas', 'oil', 'natural'],\n",
       "  3: ['prices', 'rise', 'butane']},\n",
       " 'nickel': {1: ['platinum', 'found', 'technigen'],\n",
       "  2: ['grade', 'dollars', 'sumitomo']},\n",
       " 'oat': {1: ['v', 'iv', 'level'],\n",
       "  2: ['crop', 'winter', 'saskatchewan'],\n",
       "  3: ['total', 'stocks', 'exports'],\n",
       "  4: ['farmers', 'dollars', 'corn']},\n",
       " 'oilseed': {1: ['canadian', 'rapeseed', 'crushers'],\n",
       "  2: ['bushels', 'inspections', 'thous'],\n",
       "  3: ['soybean', 'us', 'grain']},\n",
       " 'orange': {1: ['boxes', 'crop', 'yield'],\n",
       "  2: ['versus', 'gallons', 'movement'],\n",
       "  3: ['japanese', 'agriculture', 'japan'],\n",
       "  4: ['duties', 'juice', 'us']},\n",
       " 'palm-oil': {1: ['ringgit', 'duty', 'per'],\n",
       "  2: ['trade', 'oil', 'palm'],\n",
       "  3: ['oil', 'exports', 'palm'],\n",
       "  4: ['arabia', 'saudi', 'olein'],\n",
       "  5: ['tomorrow', 'pakistan', 'tender'],\n",
       "  6: ['cargoes', 'tender', 'olein']},\n",
       " 'pet-chem': {1: ['increase', 'dow', 'prices'],\n",
       "  2: ['monsanto', 'polyphenyl', 'rhonepoulenc'],\n",
       "  3: ['plant', 'dollars', 'production']},\n",
       " 'platinum': {1: ['found', 'drill', 'technigen'],\n",
       "  2: ['silver', 'coins', 'assay'],\n",
       "  3: ['matthey', 'mining', 'prices']},\n",
       " 'potato': {1: ['acres', 'cwts', 'estimated'],\n",
       "  2: ['futures', 'stg', 'traded']},\n",
       " 'propane': {1: ['dollars', 'price', 'lpg'],\n",
       "  2: ['delivery', 'exchange', 'energy']},\n",
       " 'rape-oil': {1: ['cargoes', 'dollars', 'oil'], 2: ['oil', 'rise', 'export']},\n",
       " 'rapeseed': {1: ['japan', 'canadian', 'undisclosed'],\n",
       "  2: ['crushers', 'canadian', 'japanese'],\n",
       "  3: ['wheat', 'load', 'cereals'],\n",
       "  4: ['oil', 'rose', 'imports'],\n",
       "  5: ['crops', 'sowings', 'rise']},\n",
       " 'reserves': {1: ['marks', 'german', 'bundesbank'],\n",
       "  2: ['francs', 'french', 'official'],\n",
       "  3: ['dollars', 'reserves', 'foreign']},\n",
       " 'retail': {1: ['sales', 'analysts', 'penney'],\n",
       "  2: ['sales', 'uk', 'provisional'],\n",
       "  3: ['sales', 'canada', 'rose'],\n",
       "  4: ['turnover', 'german', 'office'],\n",
       "  5: ['orders', 'durable', 'rose'],\n",
       "  6: ['us', 'rate', 'dollars'],\n",
       "  7: ['prices', 'turkish', 'consumption'],\n",
       "  8: ['shipments', 'appliance', 'home']},\n",
       " 'rice': {1: ['exports', 'private', 'thailand'],\n",
       "  2: ['guarantees', 'credit', 'dollars'],\n",
       "  3: ['japan', 'us', 'beef'],\n",
       "  4: ['weather', 'winter', 'areas'],\n",
       "  5: ['bank', 'indonesias', 'sugar'],\n",
       "  6: ['paddy', 'drought', 'crop'],\n",
       "  7: ['dollars', 'us', 'price'],\n",
       "  8: ['exports', 'imports', 'stocks']},\n",
       " 'rubber': {1: ['exchange', 'trade', 'session'],\n",
       "  2: ['cents', 'duty', 'per'],\n",
       "  3: ['disease', 'clones', 'trees'],\n",
       "  4: ['production', 'exports', 'thai'],\n",
       "  5: ['price', 'adjustment', 'consumers'],\n",
       "  6: ['new', 'pact', 'agreement']},\n",
       " 'ship': {1: ['us', 'gulf', 'iran'],\n",
       "  2: ['seamen', 'strike', 'brazils'],\n",
       "  3: ['portland', 'loading', 'grain'],\n",
       "  4: ['port', 'union', 'workers'],\n",
       "  5: ['vessels', 'dollars', 'lines']},\n",
       " 'silver': {1: ['montagu', 'improving', 'price'],\n",
       "  2: ['limits', 'futures', 'trading'],\n",
       "  3: ['joe', 'st', 'troy'],\n",
       "  4: ['coins', 'mint', 'ecu'],\n",
       "  5: ['hecla', 'ore', 'bp'],\n",
       "  6: ['mine', 'gold', 'us']},\n",
       " 'sorghum': {1: ['harvest', 'area', 'rains'],\n",
       "  2: ['ec', 'maize', 'imports'],\n",
       "  3: ['reserve', 'v', 'iv'],\n",
       "  4: ['acres', 'payments', 'usda'],\n",
       "  5: ['total', 'compounding', 'exports']},\n",
       " 'soy-meal': {1: ['nspa', 'bushels', 'members'],\n",
       "  2: ['imports', 'stocks', 'last'],\n",
       "  3: ['sales', 'exports', 'season'],\n",
       "  4: ['futures', 'pellets', 'soymeal']},\n",
       " 'soy-oil': {1: ['sales', 'season', 'buyers'],\n",
       "  2: ['tender', 'shipment', 'soyoil'],\n",
       "  3: ['stocks', 'imports', 'start'],\n",
       "  4: ['last', 'reported', 'stocks'],\n",
       "  5: ['total', 'x', 'registrations'],\n",
       "  6: ['oil', 'palm', 'rise'],\n",
       "  7: ['adjusted', 'grain', 'argentine'],\n",
       "  8: ['tax', 'would', 'ec']},\n",
       " 'soybean': {1: ['bushels', 'inspections', 'thous'],\n",
       "  2: ['taiwan', 'committee', 'imports'],\n",
       "  3: ['harvest', 'rains', 'hectares'],\n",
       "  4: ['ec', 'tax', 'us'],\n",
       "  5: ['shipment', 'ussr', 'purchases'],\n",
       "  6: ['loan', 'corn', 'usda']},\n",
       " 'strategic-metal': {1: ['smelter', 'locals', 'cominco'],\n",
       "  2: ['imports', 'treasury', 'oxide'],\n",
       "  3: ['prices', 'stockpile', 'management']},\n",
       " 'sugar': {1: ['offered', 'intervention', 'ec'],\n",
       "  2: ['bd', 'uk', 'rebate'],\n",
       "  3: ['ecus', 'traders', 'rebate'],\n",
       "  4: ['white', 'cargoes', 'tender'],\n",
       "  5: ['beet', 'plantings', 'hectares'],\n",
       "  6: ['imports', 'production', 'us']},\n",
       " 'sun-oil': {1: ['total', 'next', 'registrations'],\n",
       "  2: ['oil', 'tender', 'egypt']},\n",
       " 'sunseed': {1: ['season', 'sown', 'area'],\n",
       "  2: ['harvest', 'planted', 'total'],\n",
       "  3: ['registrations', 'total', 'wheat'],\n",
       "  4: ['australs', 'staley', 'price'],\n",
       "  5: ['oil', 'imports', 'sowings']},\n",
       " 'tea': {1: ['countertrade', 'trade', 'india'],\n",
       "  2: ['pakistan', 'imports', 'kenya'],\n",
       "  3: ['production', 'exports', 'rise'],\n",
       "  4: ['vietnam', 'damage', 'people']},\n",
       " 'tin': {1: ['strike', 'miners', 'president'],\n",
       "  2: ['itc', 'extension', 'council'],\n",
       "  3: ['exports', 'us', 'atpc']},\n",
       " 'trade': {1: ['ec', 'us', 'diplomats'],\n",
       "  2: ['us', 'japan', 'japanese'],\n",
       "  3: ['deficit', 'us', 'exports'],\n",
       "  4: ['dollars', 'surplus', 'deficit'],\n",
       "  5: ['taiwan', 'korea', 'south']},\n",
       " 'veg-oil': {1: ['ec', 'tax', 'proposed'],\n",
       "  2: ['palm', 'olein', 'pakistan'],\n",
       "  3: ['sales', 'credit', 'guarantees'],\n",
       "  4: ['oil', 'palm', 'exports']},\n",
       " 'wheat': {1: ['inspections', 'bushels', 'thous'],\n",
       "  2: ['ecus', 'licences', 'free'],\n",
       "  3: ['credit', 'bonus', 'ccc'],\n",
       "  4: ['last', 'estimated', 'forecast'],\n",
       "  5: ['soviet', 'lyng', 'offer'],\n",
       "  6: ['us', 'export', 'grain']},\n",
       " 'wpi': {1: ['wholesale', 'prices', 'index'],\n",
       "  2: ['prices', 'german', 'producer'],\n",
       "  3: ['rise', 'prices', 'producer']},\n",
       " 'yen': {1: ['dollar', 'bank', 'japan'],\n",
       "  2: ['g', 'accord', 'ishihara'],\n",
       "  3: ['miyazawa', 'paris', 'nations']},\n",
       " 'zinc': {1: ['trail', 'cominco', 'locals'],\n",
       "  2: ['cents', 'grade', 'price'],\n",
       "  3: ['hecla', 'venture', 'bp'],\n",
       "  4: ['grant', 'india', 'stg'],\n",
       "  5: ['metal', 'gold', 'production']}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = 'income'\n",
    "# TSNE\n",
    "embed = TSNE(n_components=2).fit_transform(dist_dict[search], 'precomputed')\n",
    "xs, ys = embed[:, 0], embed[:, 1]\n",
    "\n",
    "# DataFrame to Plot \n",
    "clusters = df_final[df_final.search == search].cluster.tolist()\n",
    "df_vis = pd.DataFrame(dict(x = xs, y = ys, cluster = clusters))\n",
    "df_vis.cluster = df_vis.cluster  # want clusters to start at 0 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) \n",
    "\n",
    "groups = df_vis.groupby('cluster')\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.scatter(group.x, group.y, label = labels_dict[search][name])\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>74.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.517065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.129192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.177927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.450319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.531072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.623683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.706323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "count  74.000000\n",
       "mean    0.517065\n",
       "std     0.129192\n",
       "min     0.177927\n",
       "25%     0.450319\n",
       "50%     0.531072\n",
       "75%     0.623683\n",
       "max     0.706323"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(distortion_dict.values())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>74.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.367271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.159916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.062415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.270007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.371043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.466095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.776284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "count  74.000000\n",
       "mean    0.367271\n",
       "std     0.159916\n",
       "min     0.062415\n",
       "25%     0.270007\n",
       "50%     0.371043\n",
       "75%     0.466095\n",
       "max     0.776284"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(silhouette_dict.values())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hog', 0.17792687763780435),\n",
       " ('heat', 0.24897209150898678),\n",
       " ('income', 0.25270915632234053),\n",
       " ('dmk', 0.25882163770657574),\n",
       " ('instal-debt', 0.2808990712154988),\n",
       " ('potato', 0.28145799043823966),\n",
       " ('sunseed', 0.3052471127967343),\n",
       " ('retail', 0.3071615758168687),\n",
       " ('lei', 0.33203596770389765),\n",
       " ('soy-oil', 0.33729714390474336),\n",
       " ('rapeseed', 0.3432886106757653),\n",
       " ('jet', 0.3628153169808612),\n",
       " ('tea', 0.3735848572570473),\n",
       " ('oat', 0.4088288411781308),\n",
       " ('housing', 0.4329018344079031),\n",
       " ('ipi', 0.4400821126423167),\n",
       " ('coconut', 0.4417627897843391),\n",
       " ('propane', 0.4424060872205235),\n",
       " ('fuel', 0.44945781299699783),\n",
       " ('silver', 0.452902322883995),\n",
       " ('lead', 0.4533011524621149),\n",
       " ('rape-oil', 0.4605892522961108),\n",
       " ('orange', 0.4701828488713825),\n",
       " ('sorghum', 0.4710236745820891),\n",
       " ('palm-oil', 0.47189036535614787),\n",
       " ('alum', 0.4789406619646696),\n",
       " ('naphtha', 0.47978793438321365),\n",
       " ('platinum', 0.481967996811276),\n",
       " ('lumber', 0.48667637530275143),\n",
       " ('zinc', 0.4947527034770406),\n",
       " ('coconut-oil', 0.49964365218667417),\n",
       " ('rice', 0.5041319948059023),\n",
       " ('barley', 0.5063236993165681),\n",
       " ('wpi', 0.5084347299601101),\n",
       " ('strategic-metal', 0.5194760814377557),\n",
       " ('rubber', 0.5223123451036189),\n",
       " ('jobs', 0.5303583490473126),\n",
       " ('nickel', 0.5317855193199753),\n",
       " ('tin', 0.5323630144566348),\n",
       " ('gas', 0.5362594707922037),\n",
       " ('money-supply', 0.5368380801801524),\n",
       " ('sun-oil', 0.5392540582158831),\n",
       " ('soy-meal', 0.5496465572319951),\n",
       " ('l-cattle', 0.5747281737172821),\n",
       " ('reserves', 0.5790098873065145),\n",
       " ('soybean', 0.5811817559828669),\n",
       " ('dlr', 0.5822518499441002),\n",
       " ('livestock', 0.6063532481212222),\n",
       " ('copper', 0.6111808341076047),\n",
       " ('cocoa', 0.6119876886839427),\n",
       " ('carcass', 0.6150779679900638),\n",
       " ('yen', 0.6152425395227002),\n",
       " ('sugar', 0.6164251984004187),\n",
       " ('money-fx', 0.6188488042571552),\n",
       " ('interest', 0.6215862082234209),\n",
       " ('groundnut', 0.6243814722762353),\n",
       " ('pet-chem', 0.6372757310281584),\n",
       " ('veg-oil', 0.6374457354134633),\n",
       " ('meal-feed', 0.6417263830490062),\n",
       " ('wheat', 0.644694825258655),\n",
       " ('iron-steel', 0.648151462343479),\n",
       " ('gnp', 0.6494747411167416),\n",
       " ('cpi', 0.6516555847105472),\n",
       " ('corn', 0.6557754882108392),\n",
       " ('bop', 0.6570665584417147),\n",
       " ('gold', 0.6578500229239416),\n",
       " ('ship', 0.6591695239417111),\n",
       " ('cotton', 0.6607692987334355),\n",
       " ('crude', 0.6619655636513172),\n",
       " ('oilseed', 0.6721295412211733),\n",
       " ('coffee', 0.6730155183601333),\n",
       " ('trade', 0.6929800738790229),\n",
       " ('nat-gas', 0.7005898554180725),\n",
       " ('grain', 0.7063234930186212)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(distortion_dict.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('naphtha', 0.062414707405202025),\n",
       " ('sun-oil', 0.06576980745592892),\n",
       " ('cpi', 0.10092874581465683),\n",
       " ('trade', 0.111975195811744),\n",
       " ('gnp', 0.11256804561594994),\n",
       " ('propane', 0.11597138956373176),\n",
       " ('l-cattle', 0.11744798582507145),\n",
       " ('groundnut', 0.12168392776677675),\n",
       " ('crude', 0.1413425826985756),\n",
       " ('nat-gas', 0.14211305518807846),\n",
       " ('grain', 0.1827783202342523),\n",
       " ('carcass', 0.18611797200796704),\n",
       " ('ship', 0.19960308733158424),\n",
       " ('veg-oil', 0.20094007521821838),\n",
       " ('money-fx', 0.21327899077363202),\n",
       " ('gold', 0.21340759658894526),\n",
       " ('wpi', 0.24013855377377671),\n",
       " ('dlr', 0.24350234581666197),\n",
       " ('sugar', 0.2684972692807997),\n",
       " ('jobs', 0.2745378102567951),\n",
       " ('tin', 0.27455293792208085),\n",
       " ('copper', 0.2760744277334159),\n",
       " ('iron-steel', 0.2808845555022474),\n",
       " ('wheat', 0.2830587148020233),\n",
       " ('soy-meal', 0.28570830704294464),\n",
       " ('corn', 0.29641167249146105),\n",
       " ('soybean', 0.3188206374627352),\n",
       " ('reserves', 0.33391305947807676),\n",
       " ('rubber', 0.34217657606262675),\n",
       " ('barley', 0.34692366554075194),\n",
       " ('oilseed', 0.34802039979810484),\n",
       " ('coffee', 0.34915906403941466),\n",
       " ('hog', 0.35072311768229447),\n",
       " ('bop', 0.354050141732347),\n",
       " ('sunseed', 0.3578751047040432),\n",
       " ('lei', 0.36676287845910943),\n",
       " ('money-supply', 0.3677574590726903),\n",
       " ('interest', 0.3743287634685716),\n",
       " ('yen', 0.380821844827052),\n",
       " ('cotton', 0.3852209922407728),\n",
       " ('platinum', 0.3853807680015611),\n",
       " ('pet-chem', 0.38685237350270757),\n",
       " ('dmk', 0.3910607167305955),\n",
       " ('oat', 0.4013645202785869),\n",
       " ('orange', 0.4187288252287969),\n",
       " ('retail', 0.4190715923055044),\n",
       " ('coconut-oil', 0.4193046235872785),\n",
       " ('cocoa', 0.42792578646106794),\n",
       " ('sorghum', 0.42899606004814006),\n",
       " ('fuel', 0.43348776585028337),\n",
       " ('gas', 0.43937102787765986),\n",
       " ('rape-oil', 0.44701286622664405),\n",
       " ('soy-oil', 0.45317957652462315),\n",
       " ('housing', 0.45689456641892007),\n",
       " ('rapeseed', 0.4637697062778433),\n",
       " ('jet', 0.4668702577414288),\n",
       " ('potato', 0.5015651537354291),\n",
       " ('palm-oil', 0.5028824210707701),\n",
       " ('lumber', 0.5077021654480504),\n",
       " ('heat', 0.518425057250227),\n",
       " ('alum', 0.5303909469538086),\n",
       " ('nickel', 0.5346906117519622),\n",
       " ('tea', 0.5437357362323951),\n",
       " ('rice', 0.5523855350489014),\n",
       " ('livestock', 0.5660120111335987),\n",
       " ('coconut', 0.5660369975873375),\n",
       " ('income', 0.5665241650299737),\n",
       " ('ipi', 0.5786849113715355),\n",
       " ('zinc', 0.5869456767691981),\n",
       " ('meal-feed', 0.593981118104353),\n",
       " ('lead', 0.6091059557026174),\n",
       " ('strategic-metal', 0.6507437392479123),\n",
       " ('instal-debt', 0.6644602917562962),\n",
       " ('silver', 0.7762837213949222)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(silhouette_dict.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO: hierarchies "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
