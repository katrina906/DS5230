{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO in general:\n",
    "- Visualization with tsne or umap rather than mds -- tsne works. can't get umap to install.    \n",
    "- If time figure out proximity matrix and do correlation analysis\n",
    "- once do a full run, look at statistic distributions and look at outliers \n",
    "\n",
    "Linkage matrix: ward  \n",
    "because in general clusters are not well separated even once PCA reduced. single, complete, average linkage tend to create one big cluster and then a bunch of clusters with one or two points. Ward gives us more well sized clusters. \n",
    "\n",
    "High run time, but argue ok because small subset of documents clustering within   \n",
    "    \n",
    "Not sure silhouette is right still. sklearn gives something else (lower)    \n",
    "   \n",
    "Have subcluster process available. Give an example, but not doing in depth analysis of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gimli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "import sklearn\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, ward, fcluster\n",
    "import networkx as nx\n",
    "import collections\n",
    "import math\n",
    "import operator\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from kneed import KneeLocator\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.spatial.distance\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchial Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_tokenizer(str_input):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(df):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    stop_lem = [stemming_tokenizer(t) for t in stopwords.words('english')]\n",
    "    stop_lem = [item for sublist in stop_lem for item in sublist]\n",
    "    \n",
    "    # tfidf. stop word removal. word tokenizer. \n",
    "    tfidf = TfidfVectorizer(stop_words = stop_lem, tokenizer = stemming_tokenizer, max_features = 5000)\n",
    "    m = tfidf.fit_transform(df['text'])\n",
    "    \n",
    "    feature_names = tfidf.get_feature_names() # words \n",
    "\n",
    "    return m, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Search from TF-IDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_search(tfidf, feature_names, search):\n",
    "    try: # sometimes search already removed (stop word)\n",
    "        # remove search from the tfidf matrix: do not want as a label or clustering factor\n",
    "        search_index = feature_names.index(search)\n",
    "        cols = list(range(0,len(feature_names)))\n",
    "        del cols[cols.index(search_index)]\n",
    "        tfidf = tfidf[:,cols]\n",
    "        del feature_names[search_index]\n",
    "    except ValueError: \n",
    "        pass\n",
    "    except:\n",
    "        raise 'unknown error'\n",
    "    \n",
    "    return tfidf, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_calculate(m, reduce):\n",
    "    dist = euclidean_distances(m)  ## I think its ok to use euclidean because tf-idf normalizes\n",
    "    if reduce:\n",
    "        flat_dist = scipy.spatial.distance.pdist(m, 'euclidean') # needed for linkage function\n",
    "    else:\n",
    "        flat_dist = scipy.spatial.distance.pdist(m.toarray(), 'euclidean') # needed for linkage function\n",
    "\n",
    "    # euclidean can be innaccurate if documents are different lengths such that vectors are different lengths \n",
    "    # I would prefer to use euclidean because then more sensicl to calculate centroids\n",
    "    # ask professor?? \n",
    "    return dist, flat_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_reduce(m):\n",
    "    pca = PCA(n_components = 0.8) # keep % of variance \n",
    "    pcam = pca.fit_transform(m.toarray())\n",
    "\n",
    "    return pcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linkage Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def linkage_calculate(dist):\n",
    "    linkage_matrix = linkage(dist, method = 'ward') \n",
    "    return linkage_matrix\n",
    "    \n",
    "# plot dendogram\n",
    "#fig, ax = plt.subplots(figsize=(15, 20))\n",
    "#ax = dendrogram(linkage_matrix, orientation=\"right\", labels = df_retail.ids.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose K and Create Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_merge(df, f):\n",
    "    # merge in with original data via pandas\n",
    "    frameh = pd.DataFrame(df.index, index = [f], columns = ['index_search'])\n",
    "    frameh = pd.merge(frameh, df, right_index = True, left_on = 'index_search')\n",
    "    frameh['cluster'] = frameh.index.str[0]\n",
    "    frameh = frameh.reset_index()\n",
    "    return frameh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find Cluster Centroids and Cluster Labels__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_label(frameh, m_pca, m, feature_names, search):\n",
    "    # most common words in clusters (based on tf-idf not just frequency)\n",
    "    centroid = dict()\n",
    "    labels = dict()\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        ## centroid ## \n",
    "        cluster1 = list(frameh[frameh.cluster == c].index.unique())\n",
    "        # find documents cluster\n",
    "        m1_pca = m_pca[cluster1,:]\n",
    "        # take mean vector among all documents\n",
    "        m1_pca = m1_pca.mean(axis = 0)\n",
    "        # record mean vector: centroids of each sub cluster\n",
    "        centroid[c] = m1_pca\n",
    "\n",
    "        ## labels ##\n",
    "        # redo mean vector with non-reduced tfidf matrix \n",
    "        m1 = m[cluster1,:]\n",
    "        # take mean vector among all documents\n",
    "        m1 = m1.mean(axis = 0)\n",
    "        \n",
    "        # max values in mean vector \n",
    "        lst = []\n",
    "\n",
    "        for i in np.argsort(np.asarray(m1)[0])[::-1][:4]:  # 3 words as label - take an extra in case one is the search term\n",
    "            if feature_names[i] == search: # don't record as label if it is the search\n",
    "                continue\n",
    "            lst.append(feature_names[i])\n",
    "            \n",
    "        labels[c] = lst[:3] # 3 word labels\n",
    "        \n",
    "    return labels, centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate Silhouette__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_individ(frameh, dist):\n",
    "    # average distance to points in your cluster\n",
    "    sil_a = dict()\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        sil_a[c] = dict()\n",
    "        docs_i = list(frameh[frameh.cluster == c].index.unique())\n",
    "        if len(docs_i) == 1:\n",
    "            sil_a[c][docs_i[0]] = 0\n",
    "        else:\n",
    "            for i in docs_i:\n",
    "                docs_i.remove(i)\n",
    "                sil_a[c][i] = np.nanmean(dist[i,docs_i].tolist())\n",
    "\n",
    "    # minimum average distance to points in other clusters \n",
    "    sil_b = dict()\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        sil_b[c] = dict()\n",
    "        docs_in = list(frameh[frameh.cluster == c].index.unique())\n",
    "        for i in docs_in:\n",
    "            # loop through other clusters and find average distance \n",
    "            lst = []\n",
    "            for c2 in list(frameh.cluster.unique()):\n",
    "                if c2 != c:\n",
    "                    docs_out = list(frameh[frameh.cluster == c2].index.unique())\n",
    "                    if i in docs_out: # can be in multiple clustesr\n",
    "                        docs_out.remove(i)\n",
    "                    lst.append(np.nanmean(dist[i,docs_out].tolist()))\n",
    "                \n",
    "            # take minimum of average distance to other clusters\n",
    "            sil_b[c][i] = np.min(lst)\n",
    "            \n",
    "    return sil_a, sil_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_avg_calculate(frameh, dist):\n",
    "    sil_a, sil_b = silhouette_individ(frameh, dist)\n",
    "    \n",
    "    # find silhouette score of each point in each cluster and take average -> cluster score\n",
    "    sil_scores = dict()\n",
    "    for k,v in sil_a.items():\n",
    "        lst = []\n",
    "        for i in range(len(v.values())):\n",
    "            max_ab = max(list(sil_b[k].values())[i], list(sil_a[k].values())[i])\n",
    "            min_ab = min(list(sil_b[k].values())[i], list(sil_a[k].values())[i])\n",
    "            lst.append(1 - min_ab/max_ab)\n",
    "        sil_scores[k] = np.nanmean(lst) # ignore nans: ex point in all of the clusters, so no b to calculate \n",
    "        \n",
    "    # return clusters and overall average\n",
    "    return sil_scores, np.nanmean(list(sil_scores.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Clustering__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clusters(k, linkage_matrix, m_pca, m, df, feature_names, search):\n",
    "    f = fcluster(linkage_matrix, k, criterion = 'maxclust')\n",
    "    frameh = frame_merge(df, f)\n",
    "    labels, centroid = centroid_label(frameh, m_pca, m, feature_names, search)\n",
    "\n",
    "    return frameh, labels, centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate Distortion__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortion_calculate(m, centroid, frameh):\n",
    "    sumd = 0\n",
    "    for i in list(frameh.index.unique()):\n",
    "        c = int(frameh[frameh.index == i].cluster)\n",
    "        sumd += np.linalg.norm(m[i]-centroid[c])\n",
    "        \n",
    "    return sumd, sumd / len(frameh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate Distortion, Silhouette at various k values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def distortion_silhouette(linkage_matrix, m_pca, m, dist, df, feature_names, search):\n",
    "    # distortion - sum of squared errors between points and its centroid \n",
    "    # barely varies with different cluster numbers\n",
    "    distortion = dict()\n",
    "    silhouette = dict()\n",
    "\n",
    "    for k in range(2, min(math.ceil(len(df) / 3), 10)+1): \n",
    "        # max # clusters: 1/3 of documents as long as get on average 10 docs per. Else limit to 1/2 of documents. \n",
    "        # min # clusters: 2 \n",
    "        frameh, labels, centroid = find_clusters(k, linkage_matrix, m_pca, m, df, feature_names, search)\n",
    "\n",
    "        # calculate silhouette \n",
    "        x, silhouette[k] = silhouette_avg_calculate(frameh, dist)\n",
    "\n",
    "        # calculate distortion\n",
    "        sumd, sumd_avg = distortion_calculate(m_pca, centroid, frameh)\n",
    "        # take average \n",
    "        distortion[k] = sumd\n",
    "        \n",
    "    return distortion, silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortion_roc(distortion):\n",
    "    # relative rate of change \n",
    "    roc = []\n",
    "    for k,v in distortion.items(): \n",
    "        if k+1 in distortion:\n",
    "            roc.append(abs(distortion[k+1] - distortion[k]) / distortion[k])\n",
    "            \n",
    "    return roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find K Based on Distortion ROC Elbow__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots()\n",
    "#ax.plot(roc)\n",
    "#ax.axvline(k-1, color = 'black')\n",
    "#print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k(roc):\n",
    "    if len(roc) == 1:\n",
    "        return 2 # k = 2 to k = 3 --> k = 2 \n",
    "    \n",
    "    # find k using knee method \n",
    "    kn = KneeLocator(range(len(roc)), roc, curve='convex', direction='decreasing')\n",
    "    if kn.knee == None: # sometimes there is no knee, just take the max k in that case\n",
    "        return len(roc) + 1 # starts at 2. If length = 1, then that's k = 2 to k = 3 --> k = 2 \n",
    "    k = kn.knee + 2 # index started at 0 and k starts at 2 \n",
    "                    # if choose index 1 roc that is the second value and going from k = 3 to k = 4 --> k = 3 \n",
    "    \n",
    "    return k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distortion\n",
    "#fig, ax = plt.subplots()\n",
    "\n",
    "#distortion = sorted(distortion.items()) # sorted by key, return a list of tuples\n",
    "#x, y = zip(*distortion) # unpack a list of pairs into two tuples\n",
    "#ax.plot(x,y)\n",
    "#ax.axvline(k, color = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot distortion\n",
    "#fig, ax = plt.subplots()\n",
    "\n",
    "#distortion = sorted(distortion.items()) # sorted by key, return a list of tuples\n",
    "#x, y = zip(*distortion) # unpack a list of pairs into two tuples\n",
    "#ax.plot(x,y)\n",
    "#ax.axvline(k, color = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Un-Lemmatize Labels__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_unlem_create(df, labels, t):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # tfidf without lemitization\n",
    "    tfidf = TfidfVectorizer(stop_words = stopwords.words('english'))\n",
    "    m_norm = tfidf.fit_transform(df['text'])\n",
    "    words = tfidf.get_feature_names()\n",
    "\n",
    "    # dataframe that records words and their lemitized versions\n",
    "    aux = pd.DataFrame(words, columns =['word'] )\n",
    "    aux['word_stemmed'] = aux['word'].apply(lambda x : stemmer.stem(x))\n",
    "    \n",
    "    # count the number of words in the corpus \n",
    "    vec = sklearn.feature_extraction.text.CountVectorizer().fit(df['text'])\n",
    "    bag_of_words = vec.transform(df['text'])\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = pd.DataFrame(words_freq)\n",
    "    words_freq.columns = ['word', 'num']\n",
    "    \n",
    "    # merge with aux and sort such that when take first value, will be most frequent word in corpus\n",
    "    aux = pd.merge(aux, words_freq, on = 'word', how = 'left')\n",
    "    aux = aux.sort_values(['word_stemmed', 'num'], ascending = False)\n",
    "    \n",
    "    if t == 'flat':\n",
    "        # loop through returned labels and grab the first instance of the un-lemmatized word (just any version will do)\n",
    "        labels_unlem = dict()\n",
    "        for i in labels.keys():\n",
    "            labels_unlem[i] = []\n",
    "            for j in labels[i]:\n",
    "                if len(aux[aux.word_stemmed == j]) == 0:\n",
    "                    labels_unlem[i].append(j)\n",
    "                    continue\n",
    "                labels_unlem[i].append(aux[aux.word_stemmed == j].word.values[0])\n",
    "            \n",
    "    if t == 'sub': \n",
    "        labels_unlem = dict()\n",
    "        for i in labels.keys():\n",
    "            labels_unlem[i] = []\n",
    "            ct = 0\n",
    "            for j in labels[i]:\n",
    "                labels_unlem[i].append([])\n",
    "                for k in j:\n",
    "                    if len(aux[aux.word_stemmed == k]) == 0:\n",
    "                        labels_unlem[i][ct].append(k)\n",
    "                        continue\n",
    "                    labels_unlem[i][ct].append(aux[aux.word_stemmed == k].word.values[0])\n",
    "                ct += 1\n",
    "            \n",
    "    return labels_unlem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sort Labels based on Silhouette Scores of Clusters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sort(labels, max_score):\n",
    "    max_score = sorted(max_score.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    \n",
    "    labels_sorted = dict()\n",
    "    for i in max_score:\n",
    "        labels_sorted[i[0]] = labels[i[0]]\n",
    "        \n",
    "    return labels_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchy: Sub-Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linkage Matrix: Understand Node Linkages__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkage_df(linkage_matrix, frameh):\n",
    "    # in linkage matrix, indicate the aggregated node for each node pair\n",
    "    links = pd.DataFrame(linkage_matrix) # using euclidean \n",
    "    links.columns = ['source1', 'source2', 'd', 'n']\n",
    "\n",
    "    links['target'] = 0\n",
    "    n = len(frameh)-1\n",
    "    for i, row in links.iterrows():\n",
    "        n += 1\n",
    "        links.at[i,'target'] = n\n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten irregular nested lists\n",
    "def flatten(l):\n",
    "    for el in l:\n",
    "        if isinstance(el, collections.Iterable) and not isinstance(el, (str, bytes)):\n",
    "            yield from flatten(el)\n",
    "        else:\n",
    "            yield el"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find Documents at Various Sub-Clusters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_docs(merge, source):\n",
    "    merge = pd.merge(merge, merge[['target', 'docs']], left_on = source, right_on = 'target',  how = 'left')\n",
    "    merge.docs_x = np.where(merge.docs_x.isnull(), '', merge.docs_x)\n",
    "    merge.docs_y = np.where(merge.docs_y.isnull(), '', merge.docs_y)\n",
    "    merge['docs'] = merge[['docs_x', 'docs_y']].values.tolist()\n",
    "    merge = merge.drop(columns = ['docs_x', 'docs_y'])\n",
    "    #merge.docs = merge.docs.apply(np.ravel)\n",
    "    merge = merge.rename(columns = {'target_x':'target'})\n",
    "    merge.docs = list(merge.docs.apply(lambda row: flatten(row)))\n",
    "    merge.docs = merge.docs.apply(lambda row: [i for i in row if i != ''])\n",
    "\n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def assign_docs(frameh, links):\n",
    "    # initial merge between frame ids and source1/source2\n",
    "    merge = pd.merge(links, frameh[['ids']], left_on = 'source1', right_index = True, how = 'left')\n",
    "    merge = pd.merge(merge, frameh[['ids']], left_on = 'source2', right_index = True, how = 'left')\n",
    "    # create single docs list column \n",
    "    merge = merge.rename(columns = {'ids_x':'docs1', 'ids_y':'docs2'})\n",
    "    merge.docs2 = np.where(merge.docs2.isnull(), '', merge.docs2)\n",
    "    merge.docs1 = np.where(merge.docs1.isnull(), '', merge.docs1)\n",
    "    merge['docs']= merge[['docs1', 'docs2']].values.tolist()\n",
    "    merge = merge.drop(columns = ['docs1', 'docs2'])\n",
    "    # flattern docs list column\n",
    "    merge.docs = merge.docs.apply(lambda row: [i for i in row if i != ''])\n",
    "    merge['len'] = merge.docs.apply(lambda row: len(set(row)))\n",
    "\n",
    "    # loop until have one id per document at node (n)\n",
    "    while int(merge[merge.target == merge.target.max()].len) != int(merge[merge.target == merge.target.max()].n): \n",
    "        print(merge[merge.target == merge.target.max()].len)\n",
    "        merge = merge_docs(merge, 'source1')\n",
    "        merge = merge_docs(merge, 'source2')\n",
    "        merge['len'] = merge.docs.apply(lambda row: len(set(row)))\n",
    "        \n",
    "        merge = merge.drop(columns = ['target_y'])\n",
    "\n",
    "    merge.docs = merge.docs.apply(lambda row: set(row))\n",
    "    \n",
    "    return merge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Assign Graph Attributes: Docs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def attributes(G, merge, frameh, links):\n",
    "    # add docs to each target node so know which docs exist at each target \n",
    "    for i in list(merge.target.unique()):\n",
    "        G.nodes[i]['docs'] = merge[merge.target == i].docs.values[0]\n",
    "\n",
    "    # add docs to origianl nodes as well \n",
    "    ogdocs = pd.merge(links, frameh[['ids']], left_on = 'source1', right_index = True, how = 'left')\n",
    "    ogdocs = pd.merge(ogdocs, frameh[['ids']], left_on = 'source2', right_index = True, how = 'left')\n",
    "    for i in range(len(frameh)):\n",
    "        if len(ogdocs[ogdocs.source1 == i].ids_x) != 0:\n",
    "            G.nodes[i]['docs'] = ogdocs[ogdocs.source1 == i].ids_x.values[0]\n",
    "        if len(ogdocs[ogdocs.source2 == i].ids_y) != 0:\n",
    "            G.nodes[i]['docs'] = ogdocs[ogdocs.source2 == i].ids_y.values[0]\n",
    "            \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mark Top Level Clusters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_cluster(G, frameh):\n",
    "    # find top level clusters and mark as such - determined by fcluster above \n",
    "    nx.set_node_attributes(G, 0, 'cluster')\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        try:\n",
    "            c_ids = set(frameh[frameh.cluster == c].ids.unique())\n",
    "            node = [x for x,y in G.nodes(data=True) if y['docs']==c_ids][0] # equals ALL of these ids \n",
    "        except:\n",
    "            c_ids = frameh[frameh.cluster == c].ids.unique()\n",
    "            node = [x for x,y in G.nodes(data=True) if y['docs']==c_ids][0]\n",
    "        G.nodes[node]['cluster'] = c\n",
    "        \n",
    "    return G "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Labels for all Sub Clusters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_labels(links, m, frameh):\n",
    "    labels_sub = []\n",
    "    for i in list(range(links.target.max())):\n",
    "        if i < len(frameh):\n",
    "            cluster1 = list(frameh[frameh.ids == list(G.nodes[i].values())[0]].index.unique())\n",
    "        else:\n",
    "            cluster1 = list(frameh[frameh.ids.isin(list(list(G.nodes[i].values())[0]))].index.unique())\n",
    "        # find documents cluster\n",
    "        m1 = m[cluster1,:]\n",
    "        # take mean vector among all documents\n",
    "        m1 = m1.mean(axis = 0)\n",
    "\n",
    "        # max values in mean vector: labels\n",
    "        lst = []\n",
    "        for i in np.argsort(np.asarray(m1)[0])[::-1][:3]:\n",
    "            lst.append(feature_names[i])\n",
    "\n",
    "        labels_sub.append(lst)\n",
    "    \n",
    "    return labels_sub        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create Hierarchy Graph__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(links, merge, frameh):\n",
    "    # add nodes\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(links.source1)\n",
    "    G.add_nodes_from(links.source2)\n",
    "    G.add_nodes_from(links.target)\n",
    "\n",
    "    # add edges\n",
    "    subset = links[['source1', 'target']]\n",
    "    G.add_edges_from([tuple(x) for x in subset.values])\n",
    "    subset = links[['source2', 'target']]\n",
    "    G.add_edges_from([tuple(x) for x in subset.values])\n",
    "    \n",
    "    # add attributes\n",
    "    G = attributes(G, merge, frameh, links)\n",
    "    \n",
    "    # mark top clusters: attributes\n",
    "    G = top_cluster(G, frameh)\n",
    "    \n",
    "    return G "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hierarchy(linkage_matrix, frameh):\n",
    "    \n",
    "    links = linkage_df(linkage_matrix, frameh)\n",
    "    merge = assign_docs(frameh, links)\n",
    "    G = create_graph(links, merge, frameh)\n",
    "                     \n",
    "    return G, links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Labels for all Sub Clusters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_labels(links, m, frameh, G, feature_names):\n",
    "    labels_sub = []\n",
    "    for i in list(range(links.target.max())):\n",
    "        if i < len(frameh):\n",
    "            cluster1 = list(frameh[frameh.ids == list(G.nodes[i].values())[0]].index.unique())\n",
    "        else:\n",
    "            cluster1 = list(frameh[frameh.ids.isin(list(list(G.nodes[i].values())[0]))].index.unique())\n",
    "        # find documents cluster\n",
    "        m1 = m[cluster1,:]\n",
    "        # take mean vector among all documents\n",
    "        m1 = m1.mean(axis = 0)\n",
    "\n",
    "        # max values in mean vector: labels\n",
    "        lst = []\n",
    "        for i in np.argsort(np.asarray(m1)[0])[::-1][:3]:\n",
    "            lst.append(feature_names[i])\n",
    "\n",
    "        labels_sub.append(lst)\n",
    "    \n",
    "    return labels_sub        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels for subclusters 1 layer down from top flat clusters\n",
    "def labels_layerdown(G, frameh, labels_sub):\n",
    "    # identify topnodes \n",
    "    topnodes = []\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        topnodes.append([x for x,y in G.nodes(data=True) if y['cluster']==c][0])\n",
    "        \n",
    "    # find subcluster labels: 1 level down from topnodes \n",
    "    labels1 = dict()\n",
    "    for i in range(len(topnodes)):\n",
    "        sub = list(G.predecessors(topnodes[i]))\n",
    "        labels1[i] = [labels_sub[int(j)] for j in sub]\n",
    "        \n",
    "    return labels1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Main Hiearchy Function: Get Labels of all sub-clusters 1 down from top__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hierarchy_main(linkage_matrix, frameh, tfidf_unreduced, df_subset, feature_names):\n",
    "    G, links = create_hierarchy(linkage_matrix, frameh)\n",
    "    labels_sub = sub_labels(links, tfidf_unreduced, frameh, G, feature_names)\n",
    "    labels1 = labels_layerdown(G, frameh, labels_sub)\n",
    "    labels1 = labels_unlem_create(df_subset, labels1, t = 'sub')\n",
    "    \n",
    "    return labels1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dendogram with manually drawn in cut line for tin  \n",
    "#fig, ax = plt.subplots(figsize=(15, 20))\n",
    "#ax = dendrogram(linkage_matrix, orientation=\"right\", labels = frameh.ids.unique())\n",
    "#plt.axvline(1.69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(reduce, hierarchy = False, hsearch = False):\n",
    "    \n",
    "    # read in data\n",
    "    df = pd.read_pickle('reuters_processed')\n",
    "    \n",
    "    # set of topics\n",
    "    topics = list(df.categories)\n",
    "    topics = [item for sublist in topics for item in sublist]\n",
    "    topics = list(set(topics))\n",
    "    \n",
    "    distortion_dict = dict()\n",
    "    silhouette_dict = dict()\n",
    "    k_dict = dict()\n",
    "    labels_dict = dict()\n",
    "    dist_dict = dict()\n",
    "    cluster1 = list()\n",
    "    \n",
    "    df_final = pd.DataFrame()\n",
    "    \n",
    "    if hierarchy: \n",
    "        topics = [hsearch]\n",
    "    \n",
    "    for search in topics:\n",
    "        \n",
    "        if search == 'earn' or search == 'acq': # too big to deal with\n",
    "            continue\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "        df_subset = df[df.categories.map(set([search]).issubset)] \n",
    "        df_subset = df_subset.reset_index()\n",
    "        \n",
    "        if len(df_subset) < 5:\n",
    "            cluster1.append(search)\n",
    "            continue\n",
    "            \n",
    "        print(search)\n",
    "        \n",
    "        # TF-IDF matrix\n",
    "        tfidf, feature_names = tf_idf(df_subset)\n",
    "\n",
    "        # remove search from tf-idf matrix\n",
    "        tfidf, feature_names = remove_search(tfidf, feature_names, search)\n",
    "\n",
    "        # PCA dimensionality reduction\n",
    "        if reduce:\n",
    "            tfidf_unreduced = tfidf.copy()\n",
    "            tfidf = pca_reduce(tfidf)\n",
    "                    \n",
    "        # distances \n",
    "        dist, dist_flat = dist_calculate(tfidf, reduce)\n",
    "                \n",
    "        # linkage matrix\n",
    "        linkage_matrix = linkage_calculate(dist_flat)\n",
    "        \n",
    "        # find K \n",
    "        if reduce: \n",
    "            #return linkage_matrix, tfidf, tfidf_unreduced, dist, df_subset, feature_names, search\n",
    "            if len(df_subset) < 7: # else searching for a k is useless. only reasonable number is 2. \n",
    "                k = 2\n",
    "            else:\n",
    "                # use non-reduced tfidf to find labels. reduced for everything else. \n",
    "                distortion_lst, silhouette_lst = distortion_silhouette(linkage_matrix, tfidf, tfidf_unreduced, dist, df_subset,\n",
    "                                                                       feature_names, search)\n",
    "                roc = distortion_roc(distortion_lst)\n",
    "                k = find_k(roc)\n",
    "            \n",
    "            # final flat clusters\n",
    "            frameh, labels, centroid = find_clusters(k, linkage_matrix, tfidf, tfidf_unreduced, \n",
    "                                                     df_subset, feature_names, search)\n",
    "            \n",
    "            if frameh.cluster.nunique() == 1:   \n",
    "                cluster1.append(search)\n",
    "                continue\n",
    "                \n",
    "            distortion, distortion_avg = distortion_calculate(tfidf, centroid, frameh)\n",
    "            silhouette_cluster, silhouette_avg = silhouette_avg_calculate(frameh, dist)\n",
    "            \n",
    "        else:\n",
    "            # pass tfidf in for both reduced and unreduced arguments\n",
    "            distortion_lst, silhouette_lst = distortion_silhouette(linkage_matrix, tfidf, tfidf, dist, df_subset, \n",
    "                                                                   feature_names, search)\n",
    "            roc = distortion_roc(distortion_lst)\n",
    "            k = find_k(roc)\n",
    "\n",
    "            # final flat clusters\n",
    "            frameh, labels, centroid = find_clusters(k, linkage_matrix, tfidf, tfidf, \n",
    "                                                     df_subset, feature_names, search)\n",
    "            if frameh.cluster.nunique() == 1:   \n",
    "                cluster1.append(search)\n",
    "                continue\n",
    "                \n",
    "            distortion, distortion_avg = distortion_calculate(tfidf, centroid, frameh)\n",
    "            silhouette_cluster, silhouette_avg = silhouette_avg_calculate(frameh, dist)\n",
    "            \n",
    "        # un-lemmatize labels\n",
    "        labels = labels_unlem_create(df_subset, labels, t = 'flat')\n",
    "        \n",
    "        # sort labels based on cluster silhouette score\n",
    "        labels = label_sort(labels, silhouette_cluster)\n",
    "\n",
    "        # dictionary record \n",
    "        distortion_dict[search] = distortion_avg\n",
    "        silhouette_dict[search] = silhouette_avg\n",
    "        k_dict[search] = k\n",
    "        labels_dict[search] = labels\n",
    "        dist_dict[search] = dist\n",
    "        \n",
    "        frameh['search'] = search\n",
    "        df_final = df_final.append(frameh)\n",
    "        \n",
    "        if hierarchy: \n",
    "            labels1 = hierarchy_main(linkage_matrix, frameh, tfidf_unreduced, df_subset, feature_names)\n",
    "            return labels1\n",
    "\n",
    "    return distortion_dict, silhouette_dict, k_dict, labels_dict, df_final, dist_dict, cluster1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tin\n",
      "28    0\n",
      "Name: len, dtype: int64\n",
      "28    0\n",
      "Name: len, dtype: int64\n",
      "28    6\n",
      "Name: len, dtype: int64\n",
      "28    16\n",
      "Name: len, dtype: int64\n",
      "28    20\n",
      "Name: len, dtype: int64\n",
      "28    21\n",
      "Name: len, dtype: int64\n",
      "28    24\n",
      "Name: len, dtype: int64\n",
      "28    26\n",
      "Name: len, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#labels1 = main(reduce = True, hierarchy = True, hsearch = 'tin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distortion_dict, silhouette_dict, k_dict, labels_dict, df_final, dist_dict, cluster1 = main(reduce = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('hierarchial', \"wb\") as f:\n",
    "#    pickle.dump(df_final, f)\n",
    "#    pickle.dump(labels_dict, f)\n",
    "#    pickle.dump(k_dict, f)\n",
    "#    pickle.dump(distortion_dict, f)\n",
    "#    pickle.dump(silhouette_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = 'income'\n",
    "# TSNE\n",
    "embed = TSNE(n_components=2).fit_transform(dist_dict[search], 'precomputed')\n",
    "xs, ys = embed[:, 0], embed[:, 1]\n",
    "\n",
    "# DataFrame to Plot \n",
    "clusters = df_final[df_final.search == search].cluster.tolist()\n",
    "df_vis = pd.DataFrame(dict(x = xs, y = ys, cluster = clusters))\n",
    "df_vis.cluster = df_vis.cluster  # want clusters to start at 0 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) \n",
    "\n",
    "groups = df_vis.groupby('cluster')\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.scatter(group.x, group.y, label = labels_dict[search][name])\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(distortion_dict.values())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(silhouette_dict.values())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(distortion_dict.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(silhouette_dict.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO: hierarchies "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
