{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gimli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "import sklearn\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, ward, fcluster\n",
    "import networkx as nx\n",
    "import collections\n",
    "import math\n",
    "import operator\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from kneed import KneeLocator\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.spatial.distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchial Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(df):\n",
    "    # tfidf. stop word removal. word tokenizer. \n",
    "    tfidf = TfidfVectorizer(stop_words = 'english', analyzer = 'word')\n",
    "    m = tfidf.fit_transform(df['text'])\n",
    "    \n",
    "    feature_names = tfidf.get_feature_names() # words \n",
    "\n",
    "    return m, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_calculate(m):\n",
    "    dist = euclidean_distances(m)  ## I think its ok to use euclidean because tf-idf normalizes\n",
    "    flat_dist = scipy.spatial.distance.pdist(m, 'euclidean') # needed for linkage function\n",
    "    # euclidean can be innaccurate if documents are different lengths such that vectors are different lengths \n",
    "    # I would prefer to use euclidean because then more sensicl to calculate centroids\n",
    "    # ask professor?? \n",
    "    return dist, flat_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_reduce(m):\n",
    "    pca = PCA(n_components = 0.8) # keep 95% of variance \n",
    "    pcam = pca.fit_transform(m.toarray())\n",
    "\n",
    "    return pcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linkage Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def linkage_calculate(dist):\n",
    "    linkage_matrix = linkage(dist, method = 'ward') \n",
    "    return linkage_matrix\n",
    "    \n",
    "# plot dendogram\n",
    "#fig, ax = plt.subplots(figsize=(15, 20))\n",
    "#ax = dendrogram(linkage_matrix, orientation=\"right\", labels = df_retail.ids.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_merge(df, f):\n",
    "    # merge in with original data via pandas\n",
    "    frameh = pd.DataFrame(df.index, index = [f], columns = ['index_search'])\n",
    "    frameh = pd.merge(frameh, df, right_index = True, left_on = 'index_search')\n",
    "    frameh['cluster'] = frameh.index.str[0]\n",
    "    frameh = frameh.reset_index()\n",
    "    return frameh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_label(frameh, m_pca, m, feature_names, search):\n",
    "    # most common words in clusters (based on tf-idf not just frequency)\n",
    "    centroid = dict()\n",
    "    labels = []\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        ## centroid ## \n",
    "        cluster1 = list(frameh[frameh.cluster == c].index.unique())\n",
    "        # find documents cluster\n",
    "        m1_pca = m_pca[cluster1,:]\n",
    "        # take mean vector among all documents\n",
    "        m1_pca = m1_pca.mean(axis = 0)\n",
    "        # record mean vector: centroids of each sub cluster\n",
    "        centroid[c] = m1_pca\n",
    "\n",
    "        ## labels ##\n",
    "        # redo mean vector with non-reduced tfidf matrix \n",
    "        m1 = m[cluster1,:]\n",
    "        # take mean vector among all documents\n",
    "        m1 = m1.mean(axis = 0)\n",
    "        \n",
    "        # max values in mean vector \n",
    "        lst = []\n",
    "\n",
    "        for i in np.argsort(np.asarray(m1)[0])[::-1][:6]:\n",
    "            if feature_names[i] == search: # don't record as label if it is the search\n",
    "                continue\n",
    "            lst.append(feature_names[i])\n",
    "            \n",
    "\n",
    "        labels.append(lst)\n",
    "        \n",
    "    return labels, centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_individ(frameh, m):\n",
    "    sil_a = dict()\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        sil_a[c] = dict()\n",
    "        docs_i = list(frameh[frameh.cluster == c].index.unique())\n",
    "        for i in docs_i:\n",
    "            lst = []\n",
    "            for j in docs_i: \n",
    "                if i != j:\n",
    "                    if type(m) == np.ndarray: # if pca reduced, then ndarray instead of matrix\n",
    "                        lst.append(np.linalg.norm(m[i]-m[j]))\n",
    "                    else:\n",
    "                        lst.append(np.linalg.norm(m[i].toarray()-m[j].toarray()))\n",
    "            sil_a[c][i] = np.mean(lst)\n",
    "\n",
    "    sil_b = dict()\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        sil_b[c] = dict()\n",
    "        docs_in = list(frameh[frameh.cluster == c].index.unique())\n",
    "        docs_out = list(frameh[frameh.cluster != c].index.unique())\n",
    "        for i in docs_in:\n",
    "            lst = []\n",
    "            for j in docs_out: \n",
    "                if type(m) == np.ndarray:\n",
    "                    lst.append(np.linalg.norm(m[i]-m[j]))\n",
    "                else:\n",
    "                    lst.append(np.linalg.norm(m[i].toarray()-m[j].toarray()))\n",
    "            sil_b[c][i] = np.mean(lst)\n",
    "            \n",
    "    return sil_a, sil_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_take_avg(sil):\n",
    "    avg = []\n",
    "    for v in sil.values():\n",
    "        avg.append(list(v.values()))\n",
    "    avg = [item for sublist in avg for item in sublist]\n",
    "    avg = [0 if math.isnan(i) else i for i in avg]\n",
    "    avg = np.mean(avg)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_avg(frameh, m):\n",
    "    sil_a, sil_b = silhouette_individ(frameh, m)\n",
    "    avga = silhouette_take_avg(sil_a)\n",
    "    avgb = silhouette_take_avg(sil_b)\n",
    "\n",
    "    return (avgb - avga) / max(avgb, avga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters(k, linkage_matrix, m_pca, m, df, feature_names, search):\n",
    "    f = fcluster(linkage_matrix, k, criterion = 'maxclust')\n",
    "    frameh = frame_merge(df, f)\n",
    "    labels, centroid = centroid_label(frameh, m_pca, m, feature_names, search)\n",
    "\n",
    "    return frameh, labels, centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortion_calculate(m, centroid, frameh):\n",
    "    sumd = 0\n",
    "    for i in list(frameh.index.unique()):\n",
    "        c = int(frameh[frameh.index == i].cluster)\n",
    "        sumd += np.linalg.norm(m[i]-centroid[c])\n",
    "        \n",
    "    return sumd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def distortion_silhouette(linkage_matrix, m_pca, m, df, feature_names, search):\n",
    "    # distortion - sum of squared errors between points and its centroid \n",
    "    # barely varies with different cluster numbers\n",
    "    distortion = dict()\n",
    "    silhouette = dict()\n",
    "\n",
    "    for k in range(2, min(math.floor(len(df) / 3), 10)): \n",
    "        # max # clusters: 1/3 of documents as long as get on average 10 docs per. Else limit to 1/2 of documents. \n",
    "        # min # clusters: 2 \n",
    "        frameh, labels, centroid = clusters(k, linkage_matrix, m_pca, m, df, feature_names, search)\n",
    "\n",
    "        # calculate silhouette \n",
    "        silhouette[k] = silhouette_avg(frameh, m_pca)\n",
    "\n",
    "        # calculate distortion\n",
    "        sumd = distortion_calculate(m_pca, centroid, frameh)\n",
    "        # take average \n",
    "        distortion[k] = sumd\n",
    "        \n",
    "    return distortion, silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortion_roc(distortion):\n",
    "    # relative rate of change \n",
    "    roc = []\n",
    "    for k,v in distortion.items(): \n",
    "        if k+1 in distortion:\n",
    "            roc.append(abs(distortion[k+1] - distortion[k]) / distortion[k])\n",
    "            \n",
    "    return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k(roc):\n",
    "    # find k using knee method \n",
    "    from kneed import KneeLocator\n",
    "    kn = KneeLocator(range(len(roc)), roc, curve='convex', direction='decreasing')\n",
    "    k = kn.knee + 1 # index started at 0 \n",
    "    \n",
    "    return k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distortion\n",
    "#fig, ax = plt.subplots()\n",
    "\n",
    "#distortion = sorted(distortion.items()) # sorted by key, return a list of tuples\n",
    "#x, y = zip(*distortion) # unpack a list of pairs into two tuples\n",
    "#ax.plot(x,y)\n",
    "#ax.axvline(k, color = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(reduce):\n",
    "    \n",
    "    # read in data\n",
    "    df = pd.read_pickle('reuters_processed')\n",
    "    \n",
    "    distortion_dict = dict()\n",
    "    silhouette_dict = dict()\n",
    "    k_dict = dict()\n",
    "    labels_dict = dict()\n",
    "    \n",
    "    df_final = pd.DataFrame()\n",
    "    \n",
    "    for search in ['sugar', 'gold']: #'sugar', 'interest', 'gold'\n",
    "        df_subset = df[df.categories.map(set([search]).issubset)] \n",
    "        df_subset = df_subset.reset_index()\n",
    "        \n",
    "        print(search)\n",
    "        \n",
    "        # TF-IDF matrix\n",
    "        tfidf, feature_names = tf_idf(df_subset)\n",
    "        \n",
    "        # PCA dimensionality reduction\n",
    "        if reduce:\n",
    "            tfidf_unreduced = tfidf.copy()\n",
    "            tfidf = pca_reduce(tfidf)\n",
    "                    \n",
    "        # distances \n",
    "        dist, dist_flat = dist_calculate(tfidf)\n",
    "                \n",
    "        # linkage matrix\n",
    "        linkage_matrix = linkage_calculate(dist_flat)\n",
    "        \n",
    "        # find K \n",
    "        if reduce: \n",
    "            # use non-reduced tfidf to find labels. reduced for everything else. \n",
    "            distortion_lst, silhouette_lst = distortion_silhouette(linkage_matrix, tfidf, tfidf_unreduced, df_subset,\n",
    "                                                                   feature_names, search)\n",
    "            roc = distortion_roc(distortion_lst)\n",
    "            k = find_k(roc)\n",
    "            # final flat clusters\n",
    "            frameh, labels, centroid = clusters(k, linkage_matrix, tfidf, tfidf_unreduced, df_subset, feature_names, search)\n",
    "            distortion = distortion_calculate(tfidf, centroid, frameh)\n",
    "            silhouette = silhouette_avg(frameh, tfidf)\n",
    "            \n",
    "        else:\n",
    "            # pass tfidf in for both reduced and unreduced arguments\n",
    "            distortion_lst, silhouette_lst = distortion_silhouette(linkage_matrix, tfidf, tfidf, df_subset, \n",
    "                                                                   feature_names, search)\n",
    "            roc = distortion_roc(distortion_lst)\n",
    "            k = find_k(roc)\n",
    "\n",
    "            # final flat clusters\n",
    "            frameh, labels, centroid = clusters(k, linkage_matrix, tfidf, tfidf, df_subset, feature_names, search)\n",
    "            distortion = distortion_calculate(tfidf, centroid, frameh)\n",
    "            silhouette = silhouette_avg(frameh, tfidf)\n",
    "            \n",
    "        distortion_dict[search] = distortion\n",
    "        silhouette_dict[search] = silhouette\n",
    "        k_dict[search] = k\n",
    "        labels_dict[search] = labels\n",
    "        \n",
    "        frameh['search'] = search\n",
    "        df_final = df_final.append(frameh)\n",
    "\n",
    "\n",
    "    return distortion_dict, silhouette_dict, k_dict, labels_dict, df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sugar\n",
      "gold\n"
     ]
    }
   ],
   "source": [
    "distortion_dict, silhouette_dict, k_dict, labels_dict, df_final = main(reduce = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO: visualization, hierarchies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sugar': [['production', 'beet', 'imports', 'output', 'cane'],\n",
       "  ['ecus', 'traders', 'export', 'licences', 'ec', 'rebate'],\n",
       "  ['cargoes', 'white', 'traders', 'tender', 'india'],\n",
       "  ['intervention', 'ec', 'rebate', 'ecus', 'bd']],\n",
       " 'gold': [['reserves', 'ore', 'production', 'dollars', 'company'],\n",
       "  ['coins', 'warrants', 'price', 'market', 'coin']]}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
