{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- Analyze sub clusters and look at their performance \n",
    "- Change metric when calculating clusters (currently ward) \n",
    "- Change linkage metric when calculating \n",
    "- Check with professor that euclidean makes sense because tf-idf is normalized    \n",
    "    \n",
    "TO DO in general:\n",
    "- score and rank clusters \n",
    "- metrics to compare clusters for two methods\n",
    "- Visualization with tsne or umap rather than mds -- tsne works. can't get umap to install.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gimli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "import sklearn\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, ward, fcluster\n",
    "import networkx as nx\n",
    "import collections\n",
    "import math\n",
    "import operator\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from kneed import KneeLocator\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.spatial.distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchial Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(df):\n",
    "    # tfidf. stop word removal. word tokenizer. \n",
    "    tfidf = TfidfVectorizer(stop_words = 'english', analyzer = 'word')\n",
    "    m = tfidf.fit_transform(df['text'])\n",
    "    \n",
    "    feature_names = tfidf.get_feature_names() # words \n",
    "\n",
    "    return m, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_calculate(m):\n",
    "    dist = euclidean_distances(m)  ## I think its ok to use euclidean because tf-idf normalizes\n",
    "    flat_dist = scipy.spatial.distance.pdist(m, 'euclidean') # needed for linkage function\n",
    "    # euclidean can be innaccurate if documents are different lengths such that vectors are different lengths \n",
    "    # I would prefer to use euclidean because then more sensicl to calculate centroids\n",
    "    # ask professor?? \n",
    "    return dist, flat_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_reduce(m):\n",
    "    pca = PCA(n_components = 0.8) # keep 95% of variance \n",
    "    pcam = pca.fit_transform(m.toarray())\n",
    "\n",
    "    return pcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linkage Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def linkage_calculate(dist):\n",
    "    linkage_matrix = linkage(dist, method = 'ward') \n",
    "    return linkage_matrix\n",
    "    \n",
    "# plot dendogram\n",
    "#fig, ax = plt.subplots(figsize=(15, 20))\n",
    "#ax = dendrogram(linkage_matrix, orientation=\"right\", labels = df_retail.ids.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose K and Create Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_merge(df, f):\n",
    "    # merge in with original data via pandas\n",
    "    frameh = pd.DataFrame(df.index, index = [f], columns = ['index_search'])\n",
    "    frameh = pd.merge(frameh, df, right_index = True, left_on = 'index_search')\n",
    "    frameh['cluster'] = frameh.index.str[0]\n",
    "    frameh = frameh.reset_index()\n",
    "    return frameh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find Cluster Centroids and Cluster Labels__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_label(frameh, m_pca, m, feature_names, search):\n",
    "    # most common words in clusters (based on tf-idf not just frequency)\n",
    "    centroid = dict()\n",
    "    labels = []\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        ## centroid ## \n",
    "        cluster1 = list(frameh[frameh.cluster == c].index.unique())\n",
    "        # find documents cluster\n",
    "        m1_pca = m_pca[cluster1,:]\n",
    "        # take mean vector among all documents\n",
    "        m1_pca = m1_pca.mean(axis = 0)\n",
    "        # record mean vector: centroids of each sub cluster\n",
    "        centroid[c] = m1_pca\n",
    "\n",
    "        ## labels ##\n",
    "        # redo mean vector with non-reduced tfidf matrix \n",
    "        m1 = m[cluster1,:]\n",
    "        # take mean vector among all documents\n",
    "        m1 = m1.mean(axis = 0)\n",
    "        \n",
    "        # max values in mean vector \n",
    "        lst = []\n",
    "\n",
    "        for i in np.argsort(np.asarray(m1)[0])[::-1][:6]:\n",
    "            if feature_names[i] == search: # don't record as label if it is the search\n",
    "                continue\n",
    "            lst.append(feature_names[i])\n",
    "            \n",
    "        labels.append(lst)\n",
    "        \n",
    "    return labels, centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate Silhouette__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_individ(frameh, m):\n",
    "    sil_a = dict()\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        sil_a[c] = dict()\n",
    "        docs_i = list(frameh[frameh.cluster == c].index.unique())\n",
    "        for i in docs_i:\n",
    "            lst = []\n",
    "            for j in docs_i: \n",
    "                if i != j:\n",
    "                    if type(m) == np.ndarray: # if pca reduced, then ndarray instead of matrix\n",
    "                        lst.append(np.linalg.norm(m[i]-m[j]))\n",
    "                    else:\n",
    "                        lst.append(np.linalg.norm(m[i].toarray()-m[j].toarray()))\n",
    "            sil_a[c][i] = np.mean(lst)\n",
    "\n",
    "    sil_b = dict()\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        sil_b[c] = dict()\n",
    "        docs_in = list(frameh[frameh.cluster == c].index.unique())\n",
    "        docs_out = list(frameh[frameh.cluster != c].index.unique())\n",
    "        for i in docs_in:\n",
    "            lst = []\n",
    "            for j in docs_out: \n",
    "                if type(m) == np.ndarray:\n",
    "                    lst.append(np.linalg.norm(m[i]-m[j]))\n",
    "                else:\n",
    "                    lst.append(np.linalg.norm(m[i].toarray()-m[j].toarray()))\n",
    "            sil_b[c][i] = np.mean(lst)\n",
    "            \n",
    "    return sil_a, sil_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_take_avg(sil):\n",
    "    avg = []\n",
    "    for v in sil.values():\n",
    "        avg.append(list(v.values()))\n",
    "    avg = [item for sublist in avg for item in sublist]\n",
    "    avg = [0 if math.isnan(i) else i for i in avg]\n",
    "    avg = np.mean(avg)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_avg(frameh, m):\n",
    "    sil_a, sil_b = silhouette_individ(frameh, m)\n",
    "    avga = silhouette_take_avg(sil_a)\n",
    "    avgb = silhouette_take_avg(sil_b)\n",
    "\n",
    "    return (avgb - avga) / max(avgb, avga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Clustering__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clusters(k, linkage_matrix, m_pca, m, df, feature_names, search):\n",
    "    f = fcluster(linkage_matrix, k, criterion = 'maxclust')\n",
    "    frameh = frame_merge(df, f)\n",
    "    labels, centroid = centroid_label(frameh, m_pca, m, feature_names, search)\n",
    "\n",
    "    return frameh, labels, centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate Distortion__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortion_calculate(m, centroid, frameh):\n",
    "    sumd = 0\n",
    "    for i in list(frameh.index.unique()):\n",
    "        c = int(frameh[frameh.index == i].cluster)\n",
    "        sumd += np.linalg.norm(m[i]-centroid[c])\n",
    "        \n",
    "    return sumd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate Distortion, Silhouette at various k values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def distortion_silhouette(linkage_matrix, m_pca, m, df, feature_names, search):\n",
    "    # distortion - sum of squared errors between points and its centroid \n",
    "    # barely varies with different cluster numbers\n",
    "    distortion = dict()\n",
    "    silhouette = dict()\n",
    "\n",
    "    for k in range(2, min(math.floor(len(df) / 3), 10)): \n",
    "        # max # clusters: 1/3 of documents as long as get on average 10 docs per. Else limit to 1/2 of documents. \n",
    "        # min # clusters: 2 \n",
    "        frameh, labels, centroid = find_clusters(k, linkage_matrix, m_pca, m, df, feature_names, search)\n",
    "\n",
    "        # calculate silhouette \n",
    "        silhouette[k] = silhouette_avg(frameh, m_pca)\n",
    "\n",
    "        # calculate distortion\n",
    "        sumd = distortion_calculate(m_pca, centroid, frameh)\n",
    "        # take average \n",
    "        distortion[k] = sumd\n",
    "        \n",
    "    return distortion, silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortion_roc(distortion):\n",
    "    # relative rate of change \n",
    "    roc = []\n",
    "    for k,v in distortion.items(): \n",
    "        if k+1 in distortion:\n",
    "            roc.append(abs(distortion[k+1] - distortion[k]) / distortion[k])\n",
    "            \n",
    "    return roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find K Based on Distortion ROC Elbow__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k(roc):\n",
    "    # find k using knee method \n",
    "    from kneed import KneeLocator\n",
    "    kn = KneeLocator(range(len(roc)), roc, curve='convex', direction='decreasing')\n",
    "    k = kn.knee + 1 # index started at 0 \n",
    "    \n",
    "    return k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distortion\n",
    "#fig, ax = plt.subplots()\n",
    "\n",
    "#distortion = sorted(distortion.items()) # sorted by key, return a list of tuples\n",
    "#x, y = zip(*distortion) # unpack a list of pairs into two tuples\n",
    "#ax.plot(x,y)\n",
    "#ax.axvline(k, color = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchy: Sub-Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linkage Matrix: Understand Node Linkages__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkage_df(linkage_matrix):\n",
    "    # in linkage matrix, indicate the aggregated node for each node pair\n",
    "    links = pd.DataFrame(linkage_matrix) # using euclidean \n",
    "    links.columns = ['source1', 'source2', 'd', 'n']\n",
    "\n",
    "    links['target'] = 0\n",
    "    n = 24 \n",
    "    for i, row in links.iterrows():\n",
    "        n += 1\n",
    "        links.at[i,'target'] = n\n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten irregular nested lists\n",
    "def flatten(l):\n",
    "    for el in l:\n",
    "        if isinstance(el, collections.Iterable) and not isinstance(el, (str, bytes)):\n",
    "            yield from flatten(el)\n",
    "        else:\n",
    "            yield el"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find Documents at Various Sub-Clusters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_docs(merge, source):\n",
    "    merge = pd.merge(merge, merge[['target', 'docs']], left_on = source, right_on = 'target',  how = 'left')\n",
    "    merge.docs_x = np.where(merge.docs_x.isnull(), '', merge.docs_x)\n",
    "    merge.docs_y = np.where(merge.docs_y.isnull(), '', merge.docs_y)\n",
    "    merge['docs'] = merge[['docs_x', 'docs_y']].values.tolist()\n",
    "    merge = merge.drop(columns = ['docs_x', 'docs_y'])\n",
    "    #merge.docs = merge.docs.apply(np.ravel)\n",
    "    merge = merge.rename(columns = {'target_x':'target'})\n",
    "    merge.docs = list(merge.docs.apply(lambda row: flatten(row)))\n",
    "    merge.docs = merge.docs.apply(lambda row: [i for i in row if i != ''])\n",
    "\n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def assign_docs(frameh, links):\n",
    "    # initial merge between frame ids and source1/source2\n",
    "    merge = pd.merge(links, frameh[['ids']], left_on = 'source1', right_index = True, how = 'left')\n",
    "    merge = pd.merge(merge, frameh[['ids']], left_on = 'source2', right_index = True, how = 'left')\n",
    "    # create single docs list column \n",
    "    merge = merge.rename(columns = {'ids_x':'docs1', 'ids_y':'docs2'})\n",
    "    merge.docs2 = np.where(merge.docs2.isnull(), '', merge.docs2)\n",
    "    merge.docs1 = np.where(merge.docs1.isnull(), '', merge.docs1)\n",
    "    merge['docs']= merge[['docs1', 'docs2']].values.tolist()\n",
    "    merge = merge.drop(columns = ['docs1', 'docs2'])\n",
    "    # flattern docs list column\n",
    "    merge.docs = merge.docs.apply(lambda row: [i for i in row if i != ''])\n",
    "    merge['len'] = merge.docs.apply(lambda row: len(set(row)))\n",
    "\n",
    "    # loop until have one id per document at node (n)\n",
    "    while int(merge[merge.target == merge.target.max()].len) != int(merge[merge.target == merge.target.max()].n): \n",
    "        print(merge[merge.target == merge.target.max()].len)\n",
    "        merge = merge_docs(merge, 'source1')\n",
    "        merge = merge_docs(merge, 'source2')\n",
    "        merge['len'] = merge.docs.apply(lambda row: len(set(row)))\n",
    "        \n",
    "        merge = merge.drop(columns = ['target_y'])\n",
    "\n",
    "    merge.docs = merge.docs.apply(lambda row: set(row))\n",
    "    \n",
    "    return merge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Assign Graph Attributes: Docs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def attributes(G, merge):\n",
    "    # add docs to each target node so know which docs exist at each target \n",
    "    for i in list(merge.target.unique()):\n",
    "        G.nodes[i]['docs'] = merge[merge.target == i].docs.values[0]\n",
    "\n",
    "    # add docs to origianl nodes as well \n",
    "    ogdocs = pd.merge(links, frameh[['ids']], left_on = 'source1', right_index = True, how = 'left')\n",
    "    ogdocs = pd.merge(ogdocs, frameh[['ids']], left_on = 'source2', right_index = True, how = 'left')\n",
    "    for i in range(25):\n",
    "        if len(ogdocs[ogdocs.source1 == i].ids_x) != 0:\n",
    "            G.nodes[i]['docs'] = ogdocs[ogdocs.source1 == i].ids_x.values[0]\n",
    "        if len(ogdocs[ogdocs.source2 == i].ids_y) != 0:\n",
    "            G.nodes[i]['docs'] = ogdocs[ogdocs.source2 == i].ids_y.values[0]\n",
    "            \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mark Top Level Clusters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_cluster(G, frameh):\n",
    "    # find top level clusters and mark as such - determined by fcluster above \n",
    "    nx.set_node_attributes(G, 0, 'cluster')\n",
    "    for c in list(frameh.cluster.unique()):\n",
    "        try:\n",
    "            c_ids = set(frameh[frameh.cluster == c].ids.unique())\n",
    "            node = [x for x,y in G.nodes(data=True) if y['docs']==c_ids][0]\n",
    "        except:\n",
    "            c_ids = frameh[frameh.cluster == c].ids.unique()\n",
    "            node = [x for x,y in G.nodes(data=True) if y['docs']==c_ids][0]\n",
    "        G.nodes[node]['cluster'] = c\n",
    "        \n",
    "    return G "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create Hierarchy Graph__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(links, merge, frameh):\n",
    "    # add nodes\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(links.source1)\n",
    "    G.add_nodes_from(links.source2)\n",
    "    G.add_nodes_from(links.target)\n",
    "\n",
    "    # add edges\n",
    "    subset = links[['source1', 'target']]\n",
    "    G.add_edges_from([tuple(x) for x in subset.values])\n",
    "    subset = links[['source2', 'target']]\n",
    "    G.add_edges_from([tuple(x) for x in subset.values])\n",
    "    \n",
    "    # add attributes\n",
    "    G = attributes(G, merge)\n",
    "    \n",
    "    # mark top clusters: attributes\n",
    "    G = top_cluster(G, frameh)\n",
    "    \n",
    "    return G "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hierarchy(linkage_matrix, frameh):\n",
    "    \n",
    "    links = linkage_df(linkage_matrix)\n",
    "    merge = assign_docs(frameh, links)\n",
    "    G = create_graph(links, merge, frameh)\n",
    "                     \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(reduce):\n",
    "    \n",
    "    # read in data\n",
    "    df = pd.read_pickle('reuters_processed')\n",
    "    \n",
    "    distortion_dict = dict()\n",
    "    silhouette_dict = dict()\n",
    "    k_dict = dict()\n",
    "    labels_dict = dict()\n",
    "    dist_dict = dict()\n",
    "    graph_dict = dict()\n",
    "    \n",
    "    df_final = pd.DataFrame()\n",
    "    \n",
    "    for search in ['tin']: #'sugar', 'interest', 'gold'\n",
    "        df_subset = df[df.categories.map(set([search]).issubset)] \n",
    "        df_subset = df_subset.reset_index()\n",
    "        \n",
    "        print(search)\n",
    "        \n",
    "        # TF-IDF matrix\n",
    "        tfidf, feature_names = tf_idf(df_subset)\n",
    "        \n",
    "        # PCA dimensionality reduction\n",
    "        if reduce:\n",
    "            tfidf_unreduced = tfidf.copy()\n",
    "            tfidf = pca_reduce(tfidf)\n",
    "                    \n",
    "        # distances \n",
    "        dist, dist_flat = dist_calculate(tfidf)\n",
    "                \n",
    "        # linkage matrix\n",
    "        linkage_matrix = linkage_calculate(dist_flat)\n",
    "        \n",
    "        # find K \n",
    "        if reduce: \n",
    "            # use non-reduced tfidf to find labels. reduced for everything else. \n",
    "            distortion_lst, silhouette_lst = distortion_silhouette(linkage_matrix, tfidf, tfidf_unreduced, df_subset,\n",
    "                                                                   feature_names, search)\n",
    "            roc = distortion_roc(distortion_lst)\n",
    "            k = find_k(roc)\n",
    "            # final flat clusters\n",
    "            frameh, labels, centroid = find_clusters(k, linkage_matrix, tfidf, tfidf_unreduced, df_subset, feature_names, search)\n",
    "            distortion = distortion_calculate(tfidf, centroid, frameh)\n",
    "            silhouette = silhouette_avg(frameh, tfidf)\n",
    "            \n",
    "        else:\n",
    "            # pass tfidf in for both reduced and unreduced arguments\n",
    "            distortion_lst, silhouette_lst = distortion_silhouette(linkage_matrix, tfidf, tfidf, df_subset, \n",
    "                                                                   feature_names, search)\n",
    "            roc = distortion_roc(distortion_lst)\n",
    "            k = find_k(roc)\n",
    "\n",
    "            # final flat clusters\n",
    "            frameh, labels, centroid = find_clusters(k, linkage_matrix, tfidf, tfidf, df_subset, feature_names, search)\n",
    "            distortion = distortion_calculate(tfidf, centroid, frameh)\n",
    "            silhouette = silhouette_avg(frameh, tfidf)\n",
    "                    \n",
    "        distortion_dict[search] = distortion\n",
    "        silhouette_dict[search] = silhouette\n",
    "        k_dict[search] = k\n",
    "        labels_dict[search] = labels\n",
    "        dist_dict[search] = dist\n",
    "        \n",
    "        frameh['search'] = search\n",
    "        df_final = df_final.append(frameh)\n",
    "\n",
    "    return distortion_dict, silhouette_dict, k_dict, labels_dict, df_final, dist_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hierarchy graph \n",
    "# don't do this in the big loop. far too much processing power\n",
    "# only do for select searches \n",
    "     # but then how will I evaluate?? Maybe don't really and just present as an added bonus...\n",
    "#G = create_hierarchy(linkage_matrix, frameh)\n",
    "## need to optimize this. very slow. memory errors. Even for small terms (ex zinc, tin)\n",
    "## TO DO: modify centroid_label function to get labels for all levels of hierarchy. example in experimention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE\n",
    "embed = TSNE(n_components=2).fit_transform(dist_dict['sugar'], 'precomputed')\n",
    "xs, ys = embed[:, 0], embed[:, 1]\n",
    "\n",
    "# DataFrame to Plot \n",
    "clusters = df_final[df_final.search == 'sugar'].cluster.tolist()\n",
    "df_vis = pd.DataFrame(dict(x = xs, y = ys, cluster = clusters))\n",
    "df_vis.cluster = df_vis.cluster - 1 # want clusters to start at 0 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) \n",
    "\n",
    "groups = df_vis.groupby('cluster')\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.scatter(group.x, group.y, label = labels_dict['sugar'][name])\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO: hierarchies "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
